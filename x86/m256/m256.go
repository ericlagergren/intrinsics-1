package m256

import . "github.com/klauspost/intrinsics/x86"

// Abs16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := ABS(a[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm256_abs_epi16'.
// Requires AVX2.
func Abs16(a M256i) M256i {
	return M256i(abs16([32]byte(a)))
}

func abs16(a [32]byte) [32]byte


// MaskAbs16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm256_mask_abs_epi16'.
// Requires AVX512BW.
func MaskAbs16(src M256i, k Mmask16, a M256i) M256i {
	return M256i(maskAbs16([32]byte(src), uint16(k), [32]byte(a)))
}

func maskAbs16(src [32]byte, k uint16, a [32]byte) [32]byte


// MaskzAbs16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm256_maskz_abs_epi16'.
// Requires AVX512BW.
func MaskzAbs16(k Mmask16, a M256i) M256i {
	return M256i(maskzAbs16(uint16(k), [32]byte(a)))
}

func maskzAbs16(k uint16, a [32]byte) [32]byte


// Abs32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ABS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_abs_epi32'.
// Requires AVX2.
func Abs32(a M256i) M256i {
	return M256i(abs32([32]byte(a)))
}

func abs32(a [32]byte) [32]byte


// MaskAbs32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbs32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskAbs32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskAbs32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzAbs32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbs32(k Mmask8, a M256i) M256i {
	return M256i(maskzAbs32(uint8(k), [32]byte(a)))
}

func maskzAbs32(k uint8, a [32]byte) [32]byte


// Abs64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_abs_epi64'.
// Requires AVX512F.
func Abs64(a M256i) M256i {
	return M256i(abs64([32]byte(a)))
}

func abs64(a [32]byte) [32]byte


// MaskAbs64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbs64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskAbs64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskAbs64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzAbs64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbs64(k Mmask8, a M256i) M256i {
	return M256i(maskzAbs64(uint8(k), [32]byte(a)))
}

func maskzAbs64(k uint8, a [32]byte) [32]byte


// Abs8: Compute the absolute value of packed 8-bit integers in 'a', and store
// the unsigned results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := ABS(a[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm256_abs_epi8'.
// Requires AVX2.
func Abs8(a M256i) M256i {
	return M256i(abs8([32]byte(a)))
}

func abs8(a [32]byte) [32]byte


// MaskAbs8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm256_mask_abs_epi8'.
// Requires AVX512BW.
func MaskAbs8(src M256i, k Mmask32, a M256i) M256i {
	return M256i(maskAbs8([32]byte(src), uint32(k), [32]byte(a)))
}

func maskAbs8(src [32]byte, k uint32, a [32]byte) [32]byte


// MaskzAbs8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm256_maskz_abs_epi8'.
// Requires AVX512BW.
func MaskzAbs8(k Mmask32, a M256i) M256i {
	return M256i(maskzAbs8(uint32(k), [32]byte(a)))
}

func maskzAbs8(k uint32, a [32]byte) [32]byte


// AcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ACOS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acos_pd'.
// Requires AVX.
func AcosPd(a M256d) M256d {
	return M256d(acosPd([4]float64(a)))
}

func acosPd(a [4]float64) [4]float64


// AcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ACOS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acos_ps'.
// Requires AVX.
func AcosPs(a M256) M256 {
	return M256(acosPs([8]float32(a)))
}

func acosPs(a [8]float32) [8]float32


// AcoshPd: Compute the inverse hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ACOSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acosh_pd'.
// Requires AVX.
func AcoshPd(a M256d) M256d {
	return M256d(acoshPd([4]float64(a)))
}

func acoshPd(a [4]float64) [4]float64


// AcoshPs: Compute the inverse hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ACOSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acosh_ps'.
// Requires AVX.
func AcoshPs(a M256) M256 {
	return M256(acoshPs([8]float32(a)))
}

func acoshPs(a [8]float32) [8]float32


// Add16: Add packed 16-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[i+15:i] + b[i+15:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm256_add_epi16'.
// Requires AVX2.
func Add16(a M256i, b M256i) M256i {
	return M256i(add16([32]byte(a), [32]byte(b)))
}

func add16(a [32]byte, b [32]byte) [32]byte


// MaskAdd16: Add packed 16-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm256_mask_add_epi16'.
// Requires AVX512BW.
func MaskAdd16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAdd16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAdd16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAdd16: Add packed 16-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm256_maskz_add_epi16'.
// Requires AVX512BW.
func MaskzAdd16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAdd16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAdd16(k uint16, a [32]byte, b [32]byte) [32]byte


// Add32: Add packed 32-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_add_epi32'.
// Requires AVX2.
func Add32(a M256i, b M256i) M256i {
	return M256i(add32([32]byte(a), [32]byte(b)))
}

func add32(a [32]byte, b [32]byte) [32]byte


// MaskAdd32: Add packed 32-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_mask_add_epi32'.
// Requires AVX512F.
func MaskAdd32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAdd32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAdd32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAdd32: Add packed 32-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAdd32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAdd32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAdd32(k uint8, a [32]byte, b [32]byte) [32]byte


// Add64: Add packed 64-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_add_epi64'.
// Requires AVX2.
func Add64(a M256i, b M256i) M256i {
	return M256i(add64([32]byte(a), [32]byte(b)))
}

func add64(a [32]byte, b [32]byte) [32]byte


// MaskAdd64: Add packed 64-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_mask_add_epi64'.
// Requires AVX512F.
func MaskAdd64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAdd64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAdd64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAdd64: Add packed 64-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] :=0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAdd64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAdd64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAdd64(k uint8, a [32]byte, b [32]byte) [32]byte


// Add8: Add packed 8-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[i+7:i] + b[i+7:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm256_add_epi8'.
// Requires AVX2.
func Add8(a M256i, b M256i) M256i {
	return M256i(add8([32]byte(a), [32]byte(b)))
}

func add8(a [32]byte, b [32]byte) [32]byte


// MaskAdd8: Add packed 8-bit integers in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm256_mask_add_epi8'.
// Requires AVX512BW.
func MaskAdd8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAdd8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAdd8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAdd8: Add packed 8-bit integers in 'a' and 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm256_maskz_add_epi8'.
// Requires AVX512BW.
func MaskzAdd8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAdd8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAdd8(k uint32, a [32]byte, b [32]byte) [32]byte


// AddPd: Add packed double-precision (64-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm256_add_pd'.
// Requires AVX.
func AddPd(a M256d, b M256d) M256d {
	return M256d(addPd([4]float64(a), [4]float64(b)))
}

func addPd(a [4]float64, b [4]float64) [4]float64


// MaskAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm256_mask_add_pd'.
// Requires AVX512VL.
func MaskAddPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskAddPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskAddPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm256_maskz_add_pd'.
// Requires AVX512VL.
func MaskzAddPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzAddPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzAddPd(k uint8, a [4]float64, b [4]float64) [4]float64


// AddPs: Add packed single-precision (32-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm256_add_ps'.
// Requires AVX.
func AddPs(a M256, b M256) M256 {
	return M256(addPs([8]float32(a), [8]float32(b)))
}

func addPs(a [8]float32, b [8]float32) [8]float32


// MaskAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm256_mask_add_ps'.
// Requires AVX512VL.
func MaskAddPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskAddPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskAddPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm256_maskz_add_ps'.
// Requires AVX512VL.
func MaskzAddPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzAddPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzAddPs(k uint8, a [8]float32, b [8]float32) [8]float32


// Adds16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm256_adds_epi16'.
// Requires AVX2.
func Adds16(a M256i, b M256i) M256i {
	return M256i(adds16([32]byte(a), [32]byte(b)))
}

func adds16(a [32]byte, b [32]byte) [32]byte


// MaskAdds16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm256_mask_adds_epi16'.
// Requires AVX512BW.
func MaskAdds16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAdds16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAdds16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAdds16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm256_maskz_adds_epi16'.
// Requires AVX512BW.
func MaskzAdds16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAdds16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAdds16(k uint16, a [32]byte, b [32]byte) [32]byte


// Adds8: Add packed 8-bit integers in 'a' and 'b' using saturation, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm256_adds_epi8'.
// Requires AVX2.
func Adds8(a M256i, b M256i) M256i {
	return M256i(adds8([32]byte(a), [32]byte(b)))
}

func adds8(a [32]byte, b [32]byte) [32]byte


// MaskAdds8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm256_mask_adds_epi8'.
// Requires AVX512BW.
func MaskAdds8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAdds8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAdds8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAdds8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm256_maskz_adds_epi8'.
// Requires AVX512BW.
func MaskzAdds8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAdds8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAdds8(k uint32, a [32]byte, b [32]byte) [32]byte


// AddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm256_adds_epu16'.
// Requires AVX2.
func AddsEpu16(a M256i, b M256i) M256i {
	return M256i(addsEpu16([32]byte(a), [32]byte(b)))
}

func addsEpu16(a [32]byte, b [32]byte) [32]byte


// MaskAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm256_mask_adds_epu16'.
// Requires AVX512BW.
func MaskAddsEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAddsEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAddsEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm256_maskz_adds_epu16'.
// Requires AVX512BW.
func MaskzAddsEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAddsEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAddsEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// AddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm256_adds_epu8'.
// Requires AVX2.
func AddsEpu8(a M256i, b M256i) M256i {
	return M256i(addsEpu8([32]byte(a), [32]byte(b)))
}

func addsEpu8(a [32]byte, b [32]byte) [32]byte


// MaskAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm256_mask_adds_epu8'.
// Requires AVX512BW.
func MaskAddsEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAddsEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAddsEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm256_maskz_adds_epu8'.
// Requires AVX512BW.
func MaskzAddsEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAddsEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAddsEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// AddsubPd: Alternatively add and subtract packed double-precision (64-bit)
// floating-point elements in 'a' to/from packed elements in 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDSUBPD'. Intrinsic: '_mm256_addsub_pd'.
// Requires AVX.
func AddsubPd(a M256d, b M256d) M256d {
	return M256d(addsubPd([4]float64(a), [4]float64(b)))
}

func addsubPd(a [4]float64, b [4]float64) [4]float64


// AddsubPs: Alternatively add and subtract packed single-precision (32-bit)
// floating-point elements in 'a' to/from packed elements in 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDSUBPS'. Intrinsic: '_mm256_addsub_ps'.
// Requires AVX.
func AddsubPs(a M256, b M256) M256 {
	return M256(addsubPs([8]float32(a), [8]float32(b)))
}

func addsubPs(a [8]float32, b [8]float32) [8]float32


// Alignr32: Concatenate 'a' and 'b' into a 64-byte immediate result, shift the
// result right by 'count' 32-bit elements, and store the low 32 bytes (8
// elements) in 'dst'. 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (32*count)
//		dst[255:0] := temp[255:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm256_alignr_epi32'.
// Requires AVX512VL.
func Alignr32(a M256i, b M256i, count int) M256i {
	return M256i(alignr32([32]byte(a), [32]byte(b), count))
}

func alignr32(a [32]byte, b [32]byte, count int) [32]byte


// MaskAlignr32: Concatenate 'a' and 'b' into a 64-byte immediate result, shift
// the result right by 'count' 32-bit elements, and store the low 32 bytes (8
// elements) in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (32*count)
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm256_mask_alignr_epi32'.
// Requires AVX512VL.
func MaskAlignr32(src M256i, k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskAlignr32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskAlignr32(src [32]byte, k uint8, a [32]byte, b [32]byte, count int) [32]byte


// MaskzAlignr32: Concatenate 'a' and 'b' into a 64-byte immediate result,
// shift the result right by 'count' 32-bit elements, and store the low 32
// bytes (8 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (32*count)
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm256_maskz_alignr_epi32'.
// Requires AVX512VL.
func MaskzAlignr32(k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskzAlignr32(uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskzAlignr32(k uint8, a [32]byte, b [32]byte, count int) [32]byte


// Alignr64: Concatenate 'a' and 'b' into a 64-byte immediate result, shift the
// result right by 'count' 64-bit elements, and store the low 32 bytes (4
// elements) in 'dst'. 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (64*count)
//		dst[255:0] := temp[255:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm256_alignr_epi64'.
// Requires AVX512VL.
func Alignr64(a M256i, b M256i, count int) M256i {
	return M256i(alignr64([32]byte(a), [32]byte(b), count))
}

func alignr64(a [32]byte, b [32]byte, count int) [32]byte


// MaskAlignr64: Concatenate 'a' and 'b' into a 64-byte immediate result, shift
// the result right by 'count' 64-bit elements, and store the low 32 bytes (4
// elements) in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (64*count)
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm256_mask_alignr_epi64'.
// Requires AVX512VL.
func MaskAlignr64(src M256i, k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskAlignr64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskAlignr64(src [32]byte, k uint8, a [32]byte, b [32]byte, count int) [32]byte


// MaskzAlignr64: Concatenate 'a' and 'b' into a 64-byte immediate result,
// shift the result right by 'count' 64-bit elements, and store the low 32
// bytes (4 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (64*count)
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm256_maskz_alignr_epi64'.
// Requires AVX512VL.
func MaskzAlignr64(k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskzAlignr64(uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskzAlignr64(k uint8, a [32]byte, b [32]byte, count int) [32]byte


// Alignr8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a 32-byte
// temporary result, shift the result right by 'count' bytes, and store the low
// 16 bytes in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm256_alignr_epi8'.
// Requires AVX2.
func Alignr8(a M256i, b M256i, count int) M256i {
	return M256i(alignr8([32]byte(a), [32]byte(b), count))
}

func alignr8(a [32]byte, b [32]byte, count int) [32]byte


// MaskAlignr8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			tmp_dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm256_mask_alignr_epi8'.
// Requires AVX512BW.
func MaskAlignr8(src M256i, k Mmask32, a M256i, b M256i, count int) M256i {
	return M256i(maskAlignr8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b), count))
}

func maskAlignr8(src [32]byte, k uint32, a [32]byte, b [32]byte, count int) [32]byte


// MaskzAlignr8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			tmp_dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm256_maskz_alignr_epi8'.
// Requires AVX512BW.
func MaskzAlignr8(k Mmask32, a M256i, b M256i, count int) M256i {
	return M256i(maskzAlignr8(uint32(k), [32]byte(a), [32]byte(b), count))
}

func maskzAlignr8(k uint32, a [32]byte, b [32]byte, count int) [32]byte


// MaskAnd32: Compute the bitwise AND of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_mask_and_epi32'.
// Requires AVX512F.
func MaskAnd32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAnd32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAnd32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAnd32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAnd32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAnd32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAnd32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskAnd64: Compute the bitwise AND of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_mask_and_epi64'.
// Requires AVX512F.
func MaskAnd64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAnd64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAnd64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAnd64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAnd64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAnd64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAnd64(k uint8, a [32]byte, b [32]byte) [32]byte


// AndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm256_and_pd'.
// Requires AVX.
func AndPd(a M256d, b M256d) M256d {
	return M256d(andPd([4]float64(a), [4]float64(b)))
}

func andPd(a [4]float64, b [4]float64) [4]float64


// MaskAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm256_mask_and_pd'.
// Requires AVX512DQ.
func MaskAndPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskAndPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskAndPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0 
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm256_maskz_and_pd'.
// Requires AVX512DQ.
func MaskzAndPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzAndPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzAndPd(k uint8, a [4]float64, b [4]float64) [4]float64


// AndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm256_and_ps'.
// Requires AVX.
func AndPs(a M256, b M256) M256 {
	return M256(andPs([8]float32(a), [8]float32(b)))
}

func andPs(a [8]float32, b [8]float32) [8]float32


// MaskAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm256_mask_and_ps'.
// Requires AVX512DQ.
func MaskAndPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskAndPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskAndPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm256_maskz_and_ps'.
// Requires AVX512DQ.
func MaskzAndPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzAndPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzAndPs(k uint8, a [8]float32, b [8]float32) [8]float32


// AndSi256: Compute the bitwise AND of 256 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := (a[255:0] AND b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPAND'. Intrinsic: '_mm256_and_si256'.
// Requires AVX2.
func AndSi256(a M256i, b M256i) M256i {
	return M256i(andSi256([32]byte(a), [32]byte(b)))
}

func andSi256(a [32]byte, b [32]byte) [32]byte


// MaskAndnot32: Compute the bitwise AND NOT of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_mask_andnot_epi32'.
// Requires AVX512F.
func MaskAndnot32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndnot32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndnot32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnot32: Compute the bitwise AND NOT of packed 32-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnot32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndnot32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndnot32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskAndnot64: Compute the bitwise AND NOT of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_mask_andnot_epi64'.
// Requires AVX512F.
func MaskAndnot64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndnot64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndnot64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnot64: Compute the bitwise AND NOT of packed 64-bit integers in 'a'
// and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnot64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndnot64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndnot64(k uint8, a [32]byte, b [32]byte) [32]byte


// AndnotPd: Compute the bitwise AND NOT of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm256_andnot_pd'.
// Requires AVX.
func AndnotPd(a M256d, b M256d) M256d {
	return M256d(andnotPd([4]float64(a), [4]float64(b)))
}

func andnotPd(a [4]float64, b [4]float64) [4]float64


// MaskAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm256_mask_andnot_pd'.
// Requires AVX512DQ.
func MaskAndnotPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskAndnotPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskAndnotPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm256_maskz_andnot_pd'.
// Requires AVX512DQ.
func MaskzAndnotPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzAndnotPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzAndnotPd(k uint8, a [4]float64, b [4]float64) [4]float64


// AndnotPs: Compute the bitwise AND NOT of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm256_andnot_ps'.
// Requires AVX.
func AndnotPs(a M256, b M256) M256 {
	return M256(andnotPs([8]float32(a), [8]float32(b)))
}

func andnotPs(a [8]float32, b [8]float32) [8]float32


// MaskAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm256_mask_andnot_ps'.
// Requires AVX512DQ.
func MaskAndnotPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskAndnotPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskAndnotPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm256_maskz_andnot_ps'.
// Requires AVX512DQ.
func MaskzAndnotPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzAndnotPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzAndnotPs(k uint8, a [8]float32, b [8]float32) [8]float32


// AndnotSi256: Compute the bitwise AND NOT of 256 bits (representing integer
// data) in 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := ((NOT a[255:0]) AND b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDN'. Intrinsic: '_mm256_andnot_si256'.
// Requires AVX2.
func AndnotSi256(a M256i, b M256i) M256i {
	return M256i(andnotSi256([32]byte(a), [32]byte(b)))
}

func andnotSi256(a [32]byte, b [32]byte) [32]byte


// AsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ASIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asin_pd'.
// Requires AVX.
func AsinPd(a M256d) M256d {
	return M256d(asinPd([4]float64(a)))
}

func asinPd(a [4]float64) [4]float64


// AsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ASIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asin_ps'.
// Requires AVX.
func AsinPs(a M256) M256 {
	return M256(asinPs([8]float32(a)))
}

func asinPs(a [8]float32) [8]float32


// AsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ASINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asinh_pd'.
// Requires AVX.
func AsinhPd(a M256d) M256d {
	return M256d(asinhPd([4]float64(a)))
}

func asinhPd(a [4]float64) [4]float64


// AsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ASINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asinh_ps'.
// Requires AVX.
func AsinhPs(a M256) M256 {
	return M256(asinhPs([8]float32(a)))
}

func asinhPs(a [8]float32) [8]float32


// AtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan_pd'.
// Requires AVX.
func AtanPd(a M256d) M256d {
	return M256d(atanPd([4]float64(a)))
}

func atanPd(a [4]float64) [4]float64


// AtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan_ps'.
// Requires AVX.
func AtanPs(a M256) M256 {
	return M256(atanPs([8]float32(a)))
}

func atanPs(a [8]float32) [8]float32


// Atan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan2_pd'.
// Requires AVX.
func Atan2Pd(a M256d, b M256d) M256d {
	return M256d(atan2Pd([4]float64(a), [4]float64(b)))
}

func atan2Pd(a [4]float64, b [4]float64) [4]float64


// Atan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan2_ps'.
// Requires AVX.
func Atan2Ps(a M256, b M256) M256 {
	return M256(atan2Ps([8]float32(a), [8]float32(b)))
}

func atan2Ps(a [8]float32, b [8]float32) [8]float32


// AtanhPd: Compute the inverse hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ATANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atanh_pd'.
// Requires AVX.
func AtanhPd(a M256d) M256d {
	return M256d(atanhPd([4]float64(a)))
}

func atanhPd(a [4]float64) [4]float64


// AtanhPs: Compute the inverse hyperbolic tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ATANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atanh_ps'.
// Requires AVX.
func AtanhPs(a M256) M256 {
	return M256(atanhPs([8]float32(a)))
}

func atanhPs(a [8]float32) [8]float32


// AvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm256_avg_epu16'.
// Requires AVX2.
func AvgEpu16(a M256i, b M256i) M256i {
	return M256i(avgEpu16([32]byte(a), [32]byte(b)))
}

func avgEpu16(a [32]byte, b [32]byte) [32]byte


// MaskAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm256_mask_avg_epu16'.
// Requires AVX512BW.
func MaskAvgEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAvgEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAvgEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm256_maskz_avg_epu16'.
// Requires AVX512BW.
func MaskzAvgEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAvgEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAvgEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// AvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm256_avg_epu8'.
// Requires AVX2.
func AvgEpu8(a M256i, b M256i) M256i {
	return M256i(avgEpu8([32]byte(a), [32]byte(b)))
}

func avgEpu8(a [32]byte, b [32]byte) [32]byte


// MaskAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm256_mask_avg_epu8'.
// Requires AVX512BW.
func MaskAvgEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAvgEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAvgEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm256_maskz_avg_epu8'.
// Requires AVX512BW.
func MaskzAvgEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAvgEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAvgEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// Blend16: Blend packed 16-bit integers from 'a' and 'b' using control mask
// 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[j%8]
//				dst[i+15:i] := b[i+15:i]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDW'. Intrinsic: '_mm256_blend_epi16'.
// Requires AVX2.
func Blend16(a M256i, b M256i, imm8 int) M256i {
	return M256i(blend16([32]byte(a), [32]byte(b), imm8))
}

func blend16(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskBlend16: Blend packed 16-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := b[i+15:i]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMW'. Intrinsic: '_mm256_mask_blend_epi16'.
// Requires AVX512BW.
func MaskBlend16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskBlend16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskBlend16(k uint16, a [32]byte, b [32]byte) [32]byte


// Blend32: Blend packed 32-bit integers from 'a' and 'b' using control mask
// 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[j%8]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDD'. Intrinsic: '_mm256_blend_epi32'.
// Requires AVX2.
func Blend32(a M256i, b M256i, imm8 int) M256i {
	return M256i(blend32([32]byte(a), [32]byte(b), imm8))
}

func blend32(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskBlend32: Blend packed 32-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm256_mask_blend_epi32'.
// Requires AVX512F.
func MaskBlend32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskBlend32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskBlend32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlend64: Blend packed 64-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm256_mask_blend_epi64'.
// Requires AVX512F.
func MaskBlend64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskBlend64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskBlend64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlend8: Blend packed 8-bit integers from 'a' and 'b' using control mask
// 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := b[i+7:i]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMB'. Intrinsic: '_mm256_mask_blend_epi8'.
// Requires AVX512BW.
func MaskBlend8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskBlend8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskBlend8(k uint32, a [32]byte, b [32]byte) [32]byte


// BlendPd: Blend packed double-precision (64-bit) floating-point elements from
// 'a' and 'b' using control mask 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[j%8]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDPD'. Intrinsic: '_mm256_blend_pd'.
// Requires AVX.
func BlendPd(a M256d, b M256d, imm8 int) M256d {
	return M256d(blendPd([4]float64(a), [4]float64(b), imm8))
}

func blendPd(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskBlendPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm256_mask_blend_pd'.
// Requires AVX512F.
func MaskBlendPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskBlendPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskBlendPd(k uint8, a [4]float64, b [4]float64) [4]float64


// BlendPs: Blend packed single-precision (32-bit) floating-point elements from
// 'a' and 'b' using control mask 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[j%8]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDPS'. Intrinsic: '_mm256_blend_ps'.
// Requires AVX.
func BlendPs(a M256, b M256, imm8 int) M256 {
	return M256(blendPs([8]float32(a), [8]float32(b), imm8))
}

func blendPs(a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskBlendPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm256_mask_blend_ps'.
// Requires AVX512F.
func MaskBlendPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskBlendPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskBlendPs(k uint8, a [8]float32, b [8]float32) [8]float32


// Blendv8: Blend packed 8-bit integers from 'a' and 'b' using 'mask', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF mask[i+7]
//				dst[i+7:i] := b[i+7:i]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDVB'. Intrinsic: '_mm256_blendv_epi8'.
// Requires AVX2.
func Blendv8(a M256i, b M256i, mask M256i) M256i {
	return M256i(blendv8([32]byte(a), [32]byte(b), [32]byte(mask)))
}

func blendv8(a [32]byte, b [32]byte, mask [32]byte) [32]byte


// BlendvPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using 'mask', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDVPD'. Intrinsic: '_mm256_blendv_pd'.
// Requires AVX.
func BlendvPd(a M256d, b M256d, mask M256d) M256d {
	return M256d(blendvPd([4]float64(a), [4]float64(b), [4]float64(mask)))
}

func blendvPd(a [4]float64, b [4]float64, mask [4]float64) [4]float64


// BlendvPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using 'mask', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDVPS'. Intrinsic: '_mm256_blendv_ps'.
// Requires AVX.
func BlendvPs(a M256, b M256, mask M256) M256 {
	return M256(blendvPs([8]float32(a), [8]float32(b), [8]float32(mask)))
}

func blendvPs(a [8]float32, b [8]float32, mask [8]float32) [8]float32


// BroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm256_broadcast_f32x2'.
// Requires AVX512DQ.
func BroadcastF32x2(a M128) M256 {
	return M256(broadcastF32x2([4]float32(a)))
}

func broadcastF32x2(a [4]float32) [8]float32


// MaskBroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm256_mask_broadcast_f32x2'.
// Requires AVX512DQ.
func MaskBroadcastF32x2(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastF32x2([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastF32x2(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm256_maskz_broadcast_f32x2'.
// Requires AVX512DQ.
func MaskzBroadcastF32x2(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastF32x2(uint8(k), [4]float32(a)))
}

func maskzBroadcastF32x2(k uint8, a [4]float32) [8]float32


// BroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_broadcast_f32x4'.
// Requires AVX512F.
func BroadcastF32x4(a M128) M256 {
	return M256(broadcastF32x4([4]float32(a)))
}

func broadcastF32x4(a [4]float32) [8]float32


// MaskBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_mask_broadcast_f32x4'.
// Requires AVX512F.
func MaskBroadcastF32x4(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastF32x4([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastF32x4(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_maskz_broadcast_f32x4'.
// Requires AVX512F.
func MaskzBroadcastF32x4(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastF32x4(uint8(k), [4]float32(a)))
}

func maskzBroadcastF32x4(k uint8, a [4]float32) [8]float32


// BroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm256_broadcast_f64x2'.
// Requires AVX512DQ.
func BroadcastF64x2(a M128d) M256d {
	return M256d(broadcastF64x2([2]float64(a)))
}

func broadcastF64x2(a [2]float64) [4]float64


// MaskBroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm256_mask_broadcast_f64x2'.
// Requires AVX512DQ.
func MaskBroadcastF64x2(src M256d, k Mmask8, a M128d) M256d {
	return M256d(maskBroadcastF64x2([4]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastF64x2(src [4]float64, k uint8, a [2]float64) [4]float64


// MaskzBroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm256_maskz_broadcast_f64x2'.
// Requires AVX512DQ.
func MaskzBroadcastF64x2(k Mmask8, a M128d) M256d {
	return M256d(maskzBroadcastF64x2(uint8(k), [2]float64(a)))
}

func maskzBroadcastF64x2(k uint8, a [2]float64) [4]float64


// BroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a' to all
// elements of "dst. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm256_broadcast_i32x2'.
// Requires AVX512DQ.
func BroadcastI32x2(a M128i) M256i {
	return M256i(broadcastI32x2([16]byte(a)))
}

func broadcastI32x2(a [16]byte) [32]byte


// MaskBroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm256_mask_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskBroadcastI32x2(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastI32x2([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI32x2(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm256_maskz_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskzBroadcastI32x2(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastI32x2(uint8(k), [16]byte(a)))
}

func maskzBroadcastI32x2(k uint8, a [16]byte) [32]byte


// BroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_broadcast_i32x4'.
// Requires AVX512F.
func BroadcastI32x4(a M128i) M256i {
	return M256i(broadcastI32x4([16]byte(a)))
}

func broadcastI32x4(a [16]byte) [32]byte


// MaskBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_mask_broadcast_i32x4'.
// Requires AVX512F.
func MaskBroadcastI32x4(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastI32x4([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI32x4(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_maskz_broadcast_i32x4'.
// Requires AVX512F.
func MaskzBroadcastI32x4(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastI32x4(uint8(k), [16]byte(a)))
}

func maskzBroadcastI32x4(k uint8, a [16]byte) [32]byte


// BroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm256_broadcast_i64x2'.
// Requires AVX512DQ.
func BroadcastI64x2(a M128i) M256i {
	return M256i(broadcastI64x2([16]byte(a)))
}

func broadcastI64x2(a [16]byte) [32]byte


// MaskBroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm256_mask_broadcast_i64x2'.
// Requires AVX512DQ.
func MaskBroadcastI64x2(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastI64x2([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI64x2(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm256_maskz_broadcast_i64x2'.
// Requires AVX512DQ.
func MaskzBroadcastI64x2(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastI64x2(uint8(k), [16]byte(a)))
}

func maskzBroadcastI64x2(k uint8, a [16]byte) [32]byte


// BroadcastPd: Broadcast 128 bits from memory (composed of 2 packed
// double-precision (64-bit) floating-point elements) to all elements of 'dst'. 
//
//		tmp[127:0] = MEM[mem_addr+127:mem_addr]
//		dst[127:0] := tmp[127:0]
//		dst[255:128] := tmp[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF128'. Intrinsic: '_mm256_broadcast_pd'.
// Requires AVX.
func BroadcastPd(mem_addr M128dConst) M256d {
	return M256d(broadcastPd(mem_addr))
}

func broadcastPd(mem_addr M128dConst) [4]float64


// BroadcastPs: Broadcast 128 bits from memory (composed of 4 packed
// single-precision (32-bit) floating-point elements) to all elements of 'dst'. 
//
//		tmp[127:0] = MEM[mem_addr+127:mem_addr]
//		dst[127:0] := tmp[127:0]
//		dst[255:128] := tmp[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF128'. Intrinsic: '_mm256_broadcast_ps'.
// Requires AVX.
func BroadcastPs(mem_addr M128Const) M256 {
	return M256(broadcastPs(mem_addr))
}

func broadcastPs(mem_addr M128Const) [8]float32


// BroadcastSd: Broadcast a double-precision (64-bit) floating-point element
// from memory to all elements of 'dst'. 
//
//		tmp[63:0] = MEM[mem_addr+63:mem_addr]
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := tmp[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_broadcast_sd'.
// Requires AVX.
func BroadcastSd(mem_addr float64) M256d {
	return M256d(broadcastSd(mem_addr))
}

func broadcastSd(mem_addr float64) [4]float64


// BroadcastSs: Broadcast a single-precision (32-bit) floating-point element
// from memory to all elements of 'dst'. 
//
//		tmp[31:0] = MEM[mem_addr+31:mem_addr]
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := tmp[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_broadcast_ss'.
// Requires AVX.
func BroadcastSs(mem_addr float32) M256 {
	return M256(broadcastSs(mem_addr))
}

func broadcastSs(mem_addr float32) [8]float32


// Broadcastb8: Broadcast the low packed 8-bit integer from 'a' to all elements
// of 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_broadcastb_epi8'.
// Requires AVX2.
func Broadcastb8(a M128i) M256i {
	return M256i(broadcastb8([16]byte(a)))
}

func broadcastb8(a [16]byte) [32]byte


// MaskBroadcastb8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_mask_broadcastb_epi8'.
// Requires AVX512BW.
func MaskBroadcastb8(src M256i, k Mmask32, a M128i) M256i {
	return M256i(maskBroadcastb8([32]byte(src), uint32(k), [16]byte(a)))
}

func maskBroadcastb8(src [32]byte, k uint32, a [16]byte) [32]byte


// MaskzBroadcastb8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_maskz_broadcastb_epi8'.
// Requires AVX512BW.
func MaskzBroadcastb8(k Mmask32, a M128i) M256i {
	return M256i(maskzBroadcastb8(uint32(k), [16]byte(a)))
}

func maskzBroadcastb8(k uint32, a [16]byte) [32]byte


// Broadcastd32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_broadcastd_epi32'.
// Requires AVX2.
func Broadcastd32(a M128i) M256i {
	return M256i(broadcastd32([16]byte(a)))
}

func broadcastd32(a [16]byte) [32]byte


// MaskBroadcastd32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastd32(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastd32([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastd32(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastd32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastd32(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastd32(uint8(k), [16]byte(a)))
}

func maskzBroadcastd32(k uint8, a [16]byte) [32]byte


// Broadcastmb64: Broadcast the low 8-bits from input mask 'k' to all 64-bit
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ZeroExtend(k[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTMB2Q'. Intrinsic: '_mm256_broadcastmb_epi64'.
// Requires AVX512CD.
func Broadcastmb64(k Mmask8) M256i {
	return M256i(broadcastmb64(uint8(k)))
}

func broadcastmb64(k uint8) [32]byte


// Broadcastmw32: Broadcast the low 16-bits from input mask 'k' to all 32-bit
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ZeroExtend(k[15:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTMW2D'. Intrinsic: '_mm256_broadcastmw_epi32'.
// Requires AVX512CD.
func Broadcastmw32(k Mmask16) M256i {
	return M256i(broadcastmw32(uint16(k)))
}

func broadcastmw32(k uint16) [32]byte


// Broadcastq64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_broadcastq_epi64'.
// Requires AVX2.
func Broadcastq64(a M128i) M256i {
	return M256i(broadcastq64([16]byte(a)))
}

func broadcastq64(a [16]byte) [32]byte


// MaskBroadcastq64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastq64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastq64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastq64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastq64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastq64(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastq64(uint8(k), [16]byte(a)))
}

func maskzBroadcastq64(k uint8, a [16]byte) [32]byte


// BroadcastsdPd: Broadcast the low double-precision (64-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_broadcastsd_pd'.
// Requires AVX2.
func BroadcastsdPd(a M128d) M256d {
	return M256d(broadcastsdPd([2]float64(a)))
}

func broadcastsdPd(a [2]float64) [4]float64


// MaskBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_mask_broadcastsd_pd'.
// Requires AVX512F.
func MaskBroadcastsdPd(src M256d, k Mmask8, a M128d) M256d {
	return M256d(maskBroadcastsdPd([4]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastsdPd(src [4]float64, k uint8, a [2]float64) [4]float64


// MaskzBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_maskz_broadcastsd_pd'.
// Requires AVX512F.
func MaskzBroadcastsdPd(k Mmask8, a M128d) M256d {
	return M256d(maskzBroadcastsdPd(uint8(k), [2]float64(a)))
}

func maskzBroadcastsdPd(k uint8, a [2]float64) [4]float64


// Broadcastsi128Si256: Broadcast 128 bits of integer data from 'a' to all
// 128-bit lanes in 'dst'. 
//
//		dst[127:0] := a[127:0]
//		dst[255:128] := a[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI128'. Intrinsic: '_mm256_broadcastsi128_si256'.
// Requires AVX2.
func Broadcastsi128Si256(a M128i) M256i {
	return M256i(broadcastsi128Si256([16]byte(a)))
}

func broadcastsi128Si256(a [16]byte) [32]byte


// BroadcastssPs: Broadcast the low single-precision (32-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_broadcastss_ps'.
// Requires AVX2.
func BroadcastssPs(a M128) M256 {
	return M256(broadcastssPs([4]float32(a)))
}

func broadcastssPs(a [4]float32) [8]float32


// MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastssPs([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastssPs(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastssPs(uint8(k), [4]float32(a)))
}

func maskzBroadcastssPs(k uint8, a [4]float32) [8]float32


// Broadcastw16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_broadcastw_epi16'.
// Requires AVX2.
func Broadcastw16(a M128i) M256i {
	return M256i(broadcastw16([16]byte(a)))
}

func broadcastw16(a [16]byte) [32]byte


// MaskBroadcastw16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_mask_broadcastw_epi16'.
// Requires AVX512BW.
func MaskBroadcastw16(src M256i, k Mmask16, a M128i) M256i {
	return M256i(maskBroadcastw16([32]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastw16(src [32]byte, k uint16, a [16]byte) [32]byte


// MaskzBroadcastw16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_maskz_broadcastw_epi16'.
// Requires AVX512BW.
func MaskzBroadcastw16(k Mmask16, a M128i) M256i {
	return M256i(maskzBroadcastw16(uint16(k), [16]byte(a)))
}

func maskzBroadcastw16(k uint16, a [16]byte) [32]byte


// Bslli128: Shift 128-bit lanes in 'a' left by 'imm8' bytes while shifting in
// zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] << (tmp*8)
//		dst[255:128] := a[255:128] << (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLDQ'. Intrinsic: '_mm256_bslli_epi128'.
// Requires AVX2.
func Bslli128(a M256i, imm8 int) M256i {
	return M256i(bslli128([32]byte(a), imm8))
}

func bslli128(a [32]byte, imm8 int) [32]byte


// Bsrli128: Shift 128-bit lanes in 'a' right by 'imm8' bytes while shifting in
// zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] >> (tmp*8)
//		dst[255:128] := a[255:128] >> (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLDQ'. Intrinsic: '_mm256_bsrli_epi128'.
// Requires AVX2.
func Bsrli128(a M256i, imm8 int) M256i {
	return M256i(bsrli128([32]byte(a), imm8))
}

func bsrli128(a [32]byte, imm8 int) [32]byte


// CastpdPs: Cast vector of type __m256d to type __m256.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd_ps'.
// Requires AVX.
func CastpdPs(a M256d) M256 {
	return M256(castpdPs([4]float64(a)))
}

func castpdPs(a [4]float64) [8]float32


// CastpdSi256: Casts vector of type __m256d to type __m256i. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd_si256'.
// Requires AVX.
func CastpdSi256(a M256d) M256i {
	return M256i(castpdSi256([4]float64(a)))
}

func castpdSi256(a [4]float64) [32]byte


// Castpd128Pd256: Casts vector of type __m128d to type __m256d; the upper 128
// bits of the result are undefined. This intrinsic is only used for
// compilation and does not generate any instructions, thus it has zero
// latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd128_pd256'.
// Requires AVX.
func Castpd128Pd256(a M128d) M256d {
	return M256d(castpd128Pd256([2]float64(a)))
}

func castpd128Pd256(a [2]float64) [4]float64


// Castpd256Pd128: Casts vector of type __m256d to type __m128d. This intrinsic
// is only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd256_pd128'.
// Requires AVX.
func Castpd256Pd128(a M256d) M128d {
	return M128d(castpd256Pd128([4]float64(a)))
}

func castpd256Pd128(a [4]float64) [2]float64


// CastpsPd: Cast vector of type __m256 to type __m256d.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps_pd'.
// Requires AVX.
func CastpsPd(a M256) M256d {
	return M256d(castpsPd([8]float32(a)))
}

func castpsPd(a [8]float32) [4]float64


// CastpsSi256: Casts vector of type __m256 to type __m256i. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps_si256'.
// Requires AVX.
func CastpsSi256(a M256) M256i {
	return M256i(castpsSi256([8]float32(a)))
}

func castpsSi256(a [8]float32) [32]byte


// Castps128Ps256: Casts vector of type __m128 to type __m256; the upper 128
// bits of the result are undefined. This intrinsic is only used for
// compilation and does not generate any instructions, thus it has zero
// latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps128_ps256'.
// Requires AVX.
func Castps128Ps256(a M128) M256 {
	return M256(castps128Ps256([4]float32(a)))
}

func castps128Ps256(a [4]float32) [8]float32


// Castps256Ps128: Casts vector of type __m256 to type __m128. This intrinsic
// is only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps256_ps128'.
// Requires AVX.
func Castps256Ps128(a M256) M128 {
	return M128(castps256Ps128([8]float32(a)))
}

func castps256Ps128(a [8]float32) [4]float32


// Castsi128Si256: Casts vector of type __m128i to type __m256i; the upper 128
// bits of the result are undefined. This intrinsic is only used for
// compilation and does not generate any instructions, thus it has zero
// latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi128_si256'.
// Requires AVX.
func Castsi128Si256(a M128i) M256i {
	return M256i(castsi128Si256([16]byte(a)))
}

func castsi128Si256(a [16]byte) [32]byte


// Castsi256Pd: Casts vector of type __m256i to type __m256d. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi256_pd'.
// Requires AVX.
func Castsi256Pd(a M256i) M256d {
	return M256d(castsi256Pd([32]byte(a)))
}

func castsi256Pd(a [32]byte) [4]float64


// Castsi256Ps: Casts vector of type __m256i to type __m256. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi256_ps'.
// Requires AVX.
func Castsi256Ps(a M256i) M256 {
	return M256(castsi256Ps([32]byte(a)))
}

func castsi256Ps(a [32]byte) [8]float32


// Castsi256Si128: Casts vector of type __m256i to type __m128i. This intrinsic
// is only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi256_si128'.
// Requires AVX.
func Castsi256Si128(a M256i) M128i {
	return M128i(castsi256Si128([32]byte(a)))
}

func castsi256Si128(a [32]byte) [16]byte


// CbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cbrt_pd'.
// Requires AVX.
func CbrtPd(a M256d) M256d {
	return M256d(cbrtPd([4]float64(a)))
}

func cbrtPd(a [4]float64) [4]float64


// CbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cbrt_ps'.
// Requires AVX.
func CbrtPs(a M256) M256 {
	return M256(cbrtPs([8]float32(a)))
}

func cbrtPs(a [8]float32) [8]float32


// CdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorm_pd'.
// Requires AVX.
func CdfnormPd(a M256d) M256d {
	return M256d(cdfnormPd([4]float64(a)))
}

func cdfnormPd(a [4]float64) [4]float64


// CdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorm_ps'.
// Requires AVX.
func CdfnormPs(a M256) M256 {
	return M256(cdfnormPs([8]float32(a)))
}

func cdfnormPs(a [8]float32) [8]float32


// CdfnorminvPd: Compute the inverse cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorminv_pd'.
// Requires AVX.
func CdfnorminvPd(a M256d) M256d {
	return M256d(cdfnorminvPd([4]float64(a)))
}

func cdfnorminvPd(a [4]float64) [4]float64


// CdfnorminvPs: Compute the inverse cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorminv_ps'.
// Requires AVX.
func CdfnorminvPs(a M256) M256 {
	return M256(cdfnorminvPs([8]float32(a)))
}

func cdfnorminvPs(a [8]float32) [8]float32


// CeilPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPD'. Intrinsic: '_mm256_ceil_pd'.
// Requires AVX.
func CeilPd(a M256d) M256d {
	return M256d(ceilPd([4]float64(a)))
}

func ceilPd(a [4]float64) [4]float64


// CeilPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm256_ceil_ps'.
// Requires AVX.
func CeilPs(a M256) M256 {
	return M256(ceilPs([8]float32(a)))
}

func ceilPs(a [8]float32) [8]float32


// CexpPs: Compute the exponential value of 'e' raised to the power of packed
// complex single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cexp_ps'.
// Requires AVX.
func CexpPs(a M256) M256 {
	return M256(cexpPs([8]float32(a)))
}

func cexpPs(a [8]float32) [8]float32


// ClogPs: Compute the natural logarithm of packed complex single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_clog_ps'.
// Requires AVX.
func ClogPs(a M256) M256 {
	return M256(clogPs([8]float32(a)))
}

func clogPs(a [8]float32) [8]float32


// Cmp16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmp_epi16_mask'.
// Requires AVX512BW.
func Cmp16Mask(a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(cmp16Mask([32]byte(a), [32]byte(b), imm8))
}

func cmp16Mask(a [32]byte, b [32]byte, imm8 int) uint16


// MaskCmp16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmp_epi16_mask'.
// Requires AVX512BW.
func MaskCmp16Mask(k1 Mmask16, a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(maskCmp16Mask(uint16(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmp16Mask(k1 uint16, a [32]byte, b [32]byte, imm8 int) uint16


// Cmp32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmp_epi32_mask'.
// Requires AVX512F.
func Cmp32Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmp32Mask([32]byte(a), [32]byte(b), imm8))
}

func cmp32Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmp32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmp_epi32_mask'.
// Requires AVX512F.
func MaskCmp32Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmp32Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmp32Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// Cmp64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmp_epi64_mask'.
// Requires AVX512F.
func Cmp64Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmp64Mask([32]byte(a), [32]byte(b), imm8))
}

func cmp64Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmp64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmp64Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmp64Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmp64Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// Cmp8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmp_epi8_mask'.
// Requires AVX512BW.
func Cmp8Mask(a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(cmp8Mask([32]byte(a), [32]byte(b), imm8))
}

func cmp8Mask(a [32]byte, b [32]byte, imm8 int) uint32


// MaskCmp8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmp_epi8_mask'.
// Requires AVX512BW.
func MaskCmp8Mask(k1 Mmask32, a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(maskCmp8Mask(uint32(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmp8Mask(k1 uint32, a [32]byte, b [32]byte, imm8 int) uint32


// CmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmp_epu16_mask'.
// Requires AVX512BW.
func CmpEpu16Mask(a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(cmpEpu16Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu16Mask(a [32]byte, b [32]byte, imm8 int) uint16


// MaskCmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmp_epu16_mask'.
// Requires AVX512BW.
func MaskCmpEpu16Mask(k1 Mmask16, a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(maskCmpEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu16Mask(k1 uint16, a [32]byte, b [32]byte, imm8 int) uint16


// CmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmp_epu32_mask'.
// Requires AVX512F.
func CmpEpu32Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu32Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu32Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmp_epu32_mask'.
// Requires AVX512F.
func MaskCmpEpu32Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu32Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu64Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu64Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu64Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmp_epu8_mask'.
// Requires AVX512BW.
func CmpEpu8Mask(a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(cmpEpu8Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu8Mask(a [32]byte, b [32]byte, imm8 int) uint32


// MaskCmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmp_epu8_mask'.
// Requires AVX512BW.
func MaskCmpEpu8Mask(k1 Mmask32, a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(maskCmpEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu8Mask(k1 uint32, a [32]byte, b [32]byte, imm8 int) uint32


// CmpPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' based on the comparison operand specified by 'imm8', and store
// the results in 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] OP b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_cmp_pd'.
// Requires AVX.
func CmpPd(a M256d, b M256d, imm8 int) M256d {
	return M256d(cmpPd([4]float64(a), [4]float64(b), imm8))
}

func cmpPd(a [4]float64, b [4]float64, imm8 int) [4]float64


// CmpPdMask: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_cmp_pd_mask'.
// Requires AVX512F.
func CmpPdMask(a M256d, b M256d, imm8 int) Mmask8 {
	return Mmask8(cmpPdMask([4]float64(a), [4]float64(b), imm8))
}

func cmpPdMask(a [4]float64, b [4]float64, imm8 int) uint8


// MaskCmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_mask_cmp_pd_mask'.
// Requires AVX512F.
func MaskCmpPdMask(k1 Mmask8, a M256d, b M256d, imm8 int) Mmask8 {
	return Mmask8(maskCmpPdMask(uint8(k1), [4]float64(a), [4]float64(b), imm8))
}

func maskCmpPdMask(k1 uint8, a [4]float64, b [4]float64, imm8 int) uint8


// CmpPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' based on the comparison operand specified by 'imm8', and store
// the results in 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] OP b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_cmp_ps'.
// Requires AVX.
func CmpPs(a M256, b M256, imm8 int) M256 {
	return M256(cmpPs([8]float32(a), [8]float32(b), imm8))
}

func cmpPs(a [8]float32, b [8]float32, imm8 int) [8]float32


// CmpPsMask: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_cmp_ps_mask'.
// Requires AVX512F.
func CmpPsMask(a M256, b M256, imm8 int) Mmask8 {
	return Mmask8(cmpPsMask([8]float32(a), [8]float32(b), imm8))
}

func cmpPsMask(a [8]float32, b [8]float32, imm8 int) uint8


// MaskCmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_mask_cmp_ps_mask'.
// Requires AVX512F.
func MaskCmpPsMask(k1 Mmask8, a M256, b M256, imm8 int) Mmask8 {
	return Mmask8(maskCmpPsMask(uint8(k1), [8]float32(a), [8]float32(b), imm8))
}

func maskCmpPsMask(k1 uint8, a [8]float32, b [8]float32, imm8 int) uint8


// Cmpeq16: Compare packed 16-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := ( a[i+15:i] == b[i+15:i] ) ? 0xFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQW'. Intrinsic: '_mm256_cmpeq_epi16'.
// Requires AVX2.
func Cmpeq16(a M256i, b M256i) M256i {
	return M256i(cmpeq16([32]byte(a), [32]byte(b)))
}

func cmpeq16(a [32]byte, b [32]byte) [32]byte


// Cmpeq16Mask: Compare packed 16-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpeq_epi16_mask'.
// Requires AVX512BW.
func Cmpeq16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpeq16Mask([32]byte(a), [32]byte(b)))
}

func cmpeq16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpeq16Mask: Compare packed 16-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k1' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpeq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpeq16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpeq16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeq16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Cmpeq32: Compare packed 32-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQD'. Intrinsic: '_mm256_cmpeq_epi32'.
// Requires AVX2.
func Cmpeq32(a M256i, b M256i) M256i {
	return M256i(cmpeq32([32]byte(a), [32]byte(b)))
}

func cmpeq32(a [32]byte, b [32]byte) [32]byte


// Cmpeq32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpeq_epi32_mask'.
// Requires AVX512F.
func Cmpeq32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeq32Mask([32]byte(a), [32]byte(b)))
}

func cmpeq32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeq32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k1' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpeq_epi32_mask'.
// Requires AVX512F.
func MaskCmpeq32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeq32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeq32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpeq64: Compare packed 64-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] == b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm256_cmpeq_epi64'.
// Requires AVX2.
func Cmpeq64(a M256i, b M256i) M256i {
	return M256i(cmpeq64([32]byte(a), [32]byte(b)))
}

func cmpeq64(a [32]byte, b [32]byte) [32]byte


// Cmpeq64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpeq_epi64_mask'.
// Requires AVX512F.
func Cmpeq64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeq64Mask([32]byte(a), [32]byte(b)))
}

func cmpeq64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeq64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k1' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeq64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeq64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeq64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpeq8: Compare packed 8-bit integers in 'a' and 'b' for equality, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := ( a[i+7:i] == b[i+7:i] ) ? 0xFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQB'. Intrinsic: '_mm256_cmpeq_epi8'.
// Requires AVX2.
func Cmpeq8(a M256i, b M256i) M256i {
	return M256i(cmpeq8([32]byte(a), [32]byte(b)))
}

func cmpeq8(a [32]byte, b [32]byte) [32]byte


// Cmpeq8Mask: Compare packed 8-bit integers in 'a' and 'b' for equality, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpeq_epi8_mask'.
// Requires AVX512BW.
func Cmpeq8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpeq8Mask([32]byte(a), [32]byte(b)))
}

func cmpeq8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpeq8Mask: Compare packed 8-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpeq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpeq8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpeq8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeq8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpeq_epu16_mask'.
// Requires AVX512BW.
func CmpeqEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpeqEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpeq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpeqEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpeq_epu32_mask'.
// Requires AVX512F.
func CmpeqEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpeq_epu32_mask'.
// Requires AVX512F.
func MaskCmpeqEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpeq_epu8_mask'.
// Requires AVX512BW.
func CmpeqEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpeqEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpeq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpeqEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// Cmpge16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpge_epi16_mask'.
// Requires AVX512BW.
func Cmpge16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpge16Mask([32]byte(a), [32]byte(b)))
}

func cmpge16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpge16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpge_epi16_mask'.
// Requires AVX512BW.
func MaskCmpge16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpge16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpge16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Cmpge32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpge_epi32_mask'.
// Requires AVX512F.
func Cmpge32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpge32Mask([32]byte(a), [32]byte(b)))
}

func cmpge32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpge32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpge_epi32_mask'.
// Requires AVX512F.
func MaskCmpge32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpge32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpge32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpge64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpge_epi64_mask'.
// Requires AVX512F.
func Cmpge64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpge64Mask([32]byte(a), [32]byte(b)))
}

func cmpge64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpge64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpge64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpge64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpge64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpge8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpge_epi8_mask'.
// Requires AVX512BW.
func Cmpge8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpge8Mask([32]byte(a), [32]byte(b)))
}

func cmpge8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpge8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpge_epi8_mask'.
// Requires AVX512BW.
func MaskCmpge8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpge8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpge8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpge_epu16_mask'.
// Requires AVX512BW.
func CmpgeEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpgeEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpge_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpgeEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpge_epu32_mask'.
// Requires AVX512F.
func CmpgeEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpge_epu32_mask'.
// Requires AVX512F.
func MaskCmpgeEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpge_epu8_mask'.
// Requires AVX512BW.
func CmpgeEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpgeEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpge_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpgeEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// Cmpgt16: Compare packed 16-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := ( a[i+15:i] > b[i+15:i] ) ? 0xFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTW'. Intrinsic: '_mm256_cmpgt_epi16'.
// Requires AVX2.
func Cmpgt16(a M256i, b M256i) M256i {
	return M256i(cmpgt16([32]byte(a), [32]byte(b)))
}

func cmpgt16(a [32]byte, b [32]byte) [32]byte


// Cmpgt16Mask: Compare packed 16-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpgt_epi16_mask'.
// Requires AVX512BW.
func Cmpgt16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpgt16Mask([32]byte(a), [32]byte(b)))
}

func cmpgt16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpgt16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpgt_epi16_mask'.
// Requires AVX512BW.
func MaskCmpgt16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpgt16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgt16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Cmpgt32: Compare packed 32-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] > b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTD'. Intrinsic: '_mm256_cmpgt_epi32'.
// Requires AVX2.
func Cmpgt32(a M256i, b M256i) M256i {
	return M256i(cmpgt32([32]byte(a), [32]byte(b)))
}

func cmpgt32(a [32]byte, b [32]byte) [32]byte


// Cmpgt32Mask: Compare packed 32-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpgt_epi32_mask'.
// Requires AVX512F.
func Cmpgt32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgt32Mask([32]byte(a), [32]byte(b)))
}

func cmpgt32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgt32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpgt_epi32_mask'.
// Requires AVX512F.
func MaskCmpgt32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgt32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgt32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpgt64: Compare packed 64-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] > b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm256_cmpgt_epi64'.
// Requires AVX2.
func Cmpgt64(a M256i, b M256i) M256i {
	return M256i(cmpgt64([32]byte(a), [32]byte(b)))
}

func cmpgt64(a [32]byte, b [32]byte) [32]byte


// Cmpgt64Mask: Compare packed 64-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpgt_epi64_mask'.
// Requires AVX512F.
func Cmpgt64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgt64Mask([32]byte(a), [32]byte(b)))
}

func cmpgt64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgt64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgt64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgt64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgt64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpgt8: Compare packed 8-bit integers in 'a' and 'b' for greater-than, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := ( a[i+7:i] > b[i+7:i] ) ? 0xFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTB'. Intrinsic: '_mm256_cmpgt_epi8'.
// Requires AVX2.
func Cmpgt8(a M256i, b M256i) M256i {
	return M256i(cmpgt8([32]byte(a), [32]byte(b)))
}

func cmpgt8(a [32]byte, b [32]byte) [32]byte


// Cmpgt8Mask: Compare packed 8-bit integers in 'a' and 'b' for greater-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpgt_epi8_mask'.
// Requires AVX512BW.
func Cmpgt8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpgt8Mask([32]byte(a), [32]byte(b)))
}

func cmpgt8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpgt8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpgt_epi8_mask'.
// Requires AVX512BW.
func MaskCmpgt8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpgt8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgt8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpgt_epu16_mask'.
// Requires AVX512BW.
func CmpgtEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpgtEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpgt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpgtEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpgt_epu32_mask'.
// Requires AVX512F.
func CmpgtEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpgt_epu32_mask'.
// Requires AVX512F.
func MaskCmpgtEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpgt_epu8_mask'.
// Requires AVX512BW.
func CmpgtEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpgtEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpgt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpgtEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// Cmple16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmple_epi16_mask'.
// Requires AVX512BW.
func Cmple16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmple16Mask([32]byte(a), [32]byte(b)))
}

func cmple16Mask(a [32]byte, b [32]byte) uint16


// MaskCmple16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmple_epi16_mask'.
// Requires AVX512BW.
func MaskCmple16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmple16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmple16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Cmple32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmple_epi32_mask'.
// Requires AVX512F.
func Cmple32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmple32Mask([32]byte(a), [32]byte(b)))
}

func cmple32Mask(a [32]byte, b [32]byte) uint8


// MaskCmple32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmple_epi32_mask'.
// Requires AVX512F.
func MaskCmple32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmple32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmple32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmple64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmple_epi64_mask'.
// Requires AVX512F.
func Cmple64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmple64Mask([32]byte(a), [32]byte(b)))
}

func cmple64Mask(a [32]byte, b [32]byte) uint8


// MaskCmple64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmple64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmple64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmple64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmple8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmple_epi8_mask'.
// Requires AVX512BW.
func Cmple8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmple8Mask([32]byte(a), [32]byte(b)))
}

func cmple8Mask(a [32]byte, b [32]byte) uint32


// MaskCmple8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmple_epi8_mask'.
// Requires AVX512BW.
func MaskCmple8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmple8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmple8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmple_epu16_mask'.
// Requires AVX512BW.
func CmpleEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpleEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//					k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmple_epu16_mask'.
// Requires AVX512BW.
func MaskCmpleEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpleEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmple_epu32_mask'.
// Requires AVX512F.
func CmpleEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmple_epu32_mask'.
// Requires AVX512F.
func MaskCmpleEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmple_epu8_mask'.
// Requires AVX512BW.
func CmpleEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpleEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmple_epu8_mask'.
// Requires AVX512BW.
func MaskCmpleEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpleEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// Cmplt16Mask: Compare packed 16-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmplt_epi16_mask'.
// Requires AVX512BW.
func Cmplt16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmplt16Mask([32]byte(a), [32]byte(b)))
}

func cmplt16Mask(a [32]byte, b [32]byte) uint16


// MaskCmplt16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmplt_epi16_mask'.
// Requires AVX512BW.
func MaskCmplt16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmplt16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmplt16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Cmplt32Mask: Compare packed 32-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmplt_epi32_mask'.
// Requires AVX512F.
func Cmplt32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmplt32Mask([32]byte(a), [32]byte(b)))
}

func cmplt32Mask(a [32]byte, b [32]byte) uint8


// MaskCmplt32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmplt32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmplt32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmplt32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmplt64Mask: Compare packed 64-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmplt_epi64_mask'.
// Requires AVX512F.
func Cmplt64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmplt64Mask([32]byte(a), [32]byte(b)))
}

func cmplt64Mask(a [32]byte, b [32]byte) uint8


// MaskCmplt64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmplt64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmplt64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmplt64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmplt8Mask: Compare packed 8-bit integers in 'a' and 'b' for less-than, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmplt_epi8_mask'.
// Requires AVX512BW.
func Cmplt8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmplt8Mask([32]byte(a), [32]byte(b)))
}

func cmplt8Mask(a [32]byte, b [32]byte) uint32


// MaskCmplt8Mask: Compare packed 8-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmplt_epi8_mask'.
// Requires AVX512BW.
func MaskCmplt8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmplt8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmplt8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmplt_epu16_mask'.
// Requires AVX512BW.
func CmpltEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpltEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmplt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpltEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpltEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmplt_epu32_mask'.
// Requires AVX512F.
func CmpltEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmplt_epu32_mask'.
// Requires AVX512F.
func MaskCmpltEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmplt_epu8_mask'.
// Requires AVX512BW.
func CmpltEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpltEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmplt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpltEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpltEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// Cmpneq16Mask: Compare packed 16-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpneq_epi16_mask'.
// Requires AVX512BW.
func Cmpneq16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpneq16Mask([32]byte(a), [32]byte(b)))
}

func cmpneq16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpneq16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpneq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpneq16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpneq16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneq16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Cmpneq32Mask: Compare packed 32-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpneq_epi32_mask'.
// Requires AVX512F.
func Cmpneq32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneq32Mask([32]byte(a), [32]byte(b)))
}

func cmpneq32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneq32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpneq_epi32_mask'.
// Requires AVX512F.
func MaskCmpneq32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneq32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneq32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpneq64Mask: Compare packed 64-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpneq_epi64_mask'.
// Requires AVX512F.
func Cmpneq64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneq64Mask([32]byte(a), [32]byte(b)))
}

func cmpneq64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneq64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneq64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneq64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneq64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Cmpneq8Mask: Compare packed 8-bit integers in 'a' and 'b' for not-equal, and
// store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpneq_epi8_mask'.
// Requires AVX512BW.
func Cmpneq8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpneq8Mask([32]byte(a), [32]byte(b)))
}

func cmpneq8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpneq8Mask: Compare packed 8-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpneq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpneq8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpneq8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneq8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpneq_epu16_mask'.
// Requires AVX512BW.
func CmpneqEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpneqEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpneq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpneqEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpneq_epu32_mask'.
// Requires AVX512F.
func CmpneqEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpneq_epu32_mask'.
// Requires AVX512F.
func MaskCmpneqEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpneq_epu8_mask'.
// Requires AVX512BW.
func CmpneqEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpneqEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpneq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpneqEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// MaskCompress32: Contiguously store the active 32-bit integers in 'a' (those
// with their respective bit set in writemask 'k') to 'dst', and pass through
// the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompress32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskCompress32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskCompress32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzCompress32: Contiguously store the active 32-bit integers in 'a' (those
// with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompress32(k Mmask8, a M256i) M256i {
	return M256i(maskzCompress32(uint8(k), [32]byte(a)))
}

func maskzCompress32(k uint8, a [32]byte) [32]byte


// MaskCompress64: Contiguously store the active 64-bit integers in 'a' (those
// with their respective bit set in writemask 'k') to 'dst', and pass through
// the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompress64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskCompress64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskCompress64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzCompress64: Contiguously store the active 64-bit integers in 'a' (those
// with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompress64(k Mmask8, a M256i) M256i {
	return M256i(maskzCompress64(uint8(k), [32]byte(a)))
}

func maskzCompress64(k uint8, a [32]byte) [32]byte


// MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskCompressPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskCompressPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd(k Mmask8, a M256d) M256d {
	return M256d(maskzCompressPd(uint8(k), [4]float64(a)))
}

func maskzCompressPd(k uint8, a [4]float64) [4]float64


// MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskCompressPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskCompressPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs(k Mmask8, a M256) M256 {
	return M256(maskzCompressPs(uint8(k), [8]float32(a)))
}

func maskzCompressPs(k uint8, a [8]float32) [8]float32


// MaskCompressstoreu32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_mask_compressstoreu_epi32'.
// Requires AVX512F.
func MaskCompressstoreu32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCompressstoreu32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCompressstoreu32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCompressstoreu64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to unaligned memory
// at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_mask_compressstoreu_epi64'.
// Requires AVX512F.
func MaskCompressstoreu64(base_addr uintptr, k Mmask8, a M256i)  {
	maskCompressstoreu64(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCompressstoreu64(base_addr uintptr, k uint8, a [32]byte) 


// MaskCompressstoreuPd: Contiguously store the active double-precision
// (64-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_mask_compressstoreu_pd'.
// Requires AVX512F.
func MaskCompressstoreuPd(base_addr uintptr, k Mmask8, a M256d)  {
	maskCompressstoreuPd(uintptr(base_addr), uint8(k), [4]float64(a))
}

func maskCompressstoreuPd(base_addr uintptr, k uint8, a [4]float64) 


// MaskCompressstoreuPs: Contiguously store the active single-precision
// (32-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_mask_compressstoreu_ps'.
// Requires AVX512F.
func MaskCompressstoreuPs(base_addr uintptr, k Mmask8, a M256)  {
	maskCompressstoreuPs(uintptr(base_addr), uint8(k), [8]float32(a))
}

func maskCompressstoreuPs(base_addr uintptr, k uint8, a [8]float32) 


// Conflict32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			FOR k := 0 to j-1
//				m := k*32
//				dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//			ENDFOR
//			dst[i+31:i+j] := 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm256_conflict_epi32'.
// Requires AVX512CD.
func Conflict32(a M256i) M256i {
	return M256i(conflict32([32]byte(a)))
}

func conflict32(a [32]byte) [32]byte


// MaskConflict32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm256_mask_conflict_epi32'.
// Requires AVX512CD.
func MaskConflict32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskConflict32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskConflict32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzConflict32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm256_maskz_conflict_epi32'.
// Requires AVX512CD.
func MaskzConflict32(k Mmask8, a M256i) M256i {
	return M256i(maskzConflict32(uint8(k), [32]byte(a)))
}

func maskzConflict32(k uint8, a [32]byte) [32]byte


// Conflict64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			FOR k := 0 to j-1
//				m := k*64
//				dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//			ENDFOR
//			dst[i+63:i+j] := 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm256_conflict_epi64'.
// Requires AVX512CD.
func Conflict64(a M256i) M256i {
	return M256i(conflict64([32]byte(a)))
}

func conflict64(a [32]byte) [32]byte


// MaskConflict64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm256_mask_conflict_epi64'.
// Requires AVX512CD.
func MaskConflict64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskConflict64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskConflict64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzConflict64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm256_maskz_conflict_epi64'.
// Requires AVX512CD.
func MaskzConflict64(k Mmask8, a M256i) M256i {
	return M256i(maskzConflict64(uint8(k), [32]byte(a)))
}

func maskzConflict64(k uint8, a [32]byte) [32]byte


// CosPd: Compute the cosine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cos_pd'.
// Requires AVX.
func CosPd(a M256d) M256d {
	return M256d(cosPd([4]float64(a)))
}

func cosPd(a [4]float64) [4]float64


// CosPs: Compute the cosine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cos_ps'.
// Requires AVX.
func CosPs(a M256) M256 {
	return M256(cosPs([8]float32(a)))
}

func cosPs(a [8]float32) [8]float32


// CosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := COSD(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosd_pd'.
// Requires AVX.
func CosdPd(a M256d) M256d {
	return M256d(cosdPd([4]float64(a)))
}

func cosdPd(a [4]float64) [4]float64


// CosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := COSD(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosd_ps'.
// Requires AVX.
func CosdPs(a M256) M256 {
	return M256(cosdPs([8]float32(a)))
}

func cosdPs(a [8]float32) [8]float32


// CoshPd: Compute the hyperbolic cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := COSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosh_pd'.
// Requires AVX.
func CoshPd(a M256d) M256d {
	return M256d(coshPd([4]float64(a)))
}

func coshPd(a [4]float64) [4]float64


// CoshPs: Compute the hyperbolic cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := COSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosh_ps'.
// Requires AVX.
func CoshPs(a M256) M256 {
	return M256(coshPs([8]float32(a)))
}

func coshPs(a [8]float32) [8]float32


// CsqrtPs: Compute the square root of packed complex single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_csqrt_ps'.
// Requires AVX.
func CsqrtPs(a M256) M256 {
	return M256(csqrtPs([8]float32(a)))
}

func csqrtPs(a [8]float32) [8]float32


// MaskCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh(src M128i, k Mmask8, a M256, rounding int) M128i {
	return M128i(maskCvtRoundpsPh([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtRoundpsPh(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// MaskzCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh(k Mmask8, a M256, rounding int) M128i {
	return M128i(maskzCvtRoundpsPh(uint8(k), [8]float32(a), rounding))
}

func maskzCvtRoundpsPh(k uint8, a [8]float32, rounding int) [16]byte


// Cvtepi1632: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_cvtepi16_epi32'.
// Requires AVX2.
func Cvtepi1632(a M128i) M256i {
	return M256i(cvtepi1632([16]byte(a)))
}

func cvtepi1632(a [16]byte) [32]byte


// MaskCvtepi1632: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi1632(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi1632([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi1632(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi1632: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi1632(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi1632(uint8(k), [16]byte(a)))
}

func maskzCvtepi1632(k uint8, a [16]byte) [32]byte


// Cvtepi1664: Sign extend packed 16-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_cvtepi16_epi64'.
// Requires AVX2.
func Cvtepi1664(a M128i) M256i {
	return M256i(cvtepi1664([16]byte(a)))
}

func cvtepi1664(a [16]byte) [32]byte


// MaskCvtepi1664: Sign extend packed 16-bit integers in the low 8 bytes of 'a'
// to packed 64-bit integers, and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi1664(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi1664([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi1664(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi1664: Sign extend packed 16-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi1664(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi1664(uint8(k), [16]byte(a)))
}

func maskzCvtepi1664(k uint8, a [16]byte) [32]byte


// Cvtepi168: Convert packed 16-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_cvtepi16_epi8'.
// Requires AVX512BW.
func Cvtepi168(a M256i) M128i {
	return M128i(cvtepi168([32]byte(a)))
}

func cvtepi168(a [32]byte) [16]byte


// MaskCvtepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_mask_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskCvtepi168(src M128i, k Mmask16, a M256i) M128i {
	return M128i(maskCvtepi168([16]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtepi168(src [16]byte, k uint16, a [32]byte) [16]byte


// MaskzCvtepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_maskz_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtepi168(k Mmask16, a M256i) M128i {
	return M128i(maskzCvtepi168(uint16(k), [32]byte(a)))
}

func maskzCvtepi168(k uint16, a [32]byte) [16]byte


// MaskCvtepi16Storeu8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_mask_cvtepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtepi16Storeu8(base_addr uintptr, k Mmask16, a M256i)  {
	maskCvtepi16Storeu8(uintptr(base_addr), uint16(k), [32]byte(a))
}

func maskCvtepi16Storeu8(base_addr uintptr, k uint16, a [32]byte) 


// Cvtepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi3216(a M256i) M128i {
	return M128i(cvtepi3216([32]byte(a)))
}

func cvtepi3216(a [32]byte) [16]byte


// MaskCvtepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi3216(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi3216([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi3216(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi3216(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi3216(uint8(k), [32]byte(a)))
}

func maskzCvtepi3216(k uint8, a [32]byte) [16]byte


// Cvtepi3264: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := SignExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_cvtepi32_epi64'.
// Requires AVX2.
func Cvtepi3264(a M128i) M256i {
	return M256i(cvtepi3264([16]byte(a)))
}

func cvtepi3264(a [16]byte) [32]byte


// MaskCvtepi3264: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi3264(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi3264([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi3264(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi3264: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi3264(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi3264(uint8(k), [16]byte(a)))
}

func maskzCvtepi3264(k uint8, a [16]byte) [32]byte


// Cvtepi328: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi328(a M256i) M128i {
	return M128i(cvtepi328([32]byte(a)))
}

func cvtepi328(a [32]byte) [16]byte


// MaskCvtepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi328(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi328([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi328(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi328(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi328(uint8(k), [32]byte(a)))
}

func maskzCvtepi328(k uint8, a [32]byte) [16]byte


// Cvtepi32Pd: Convert packed 32-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_cvtepi32_pd'.
// Requires AVX.
func Cvtepi32Pd(a M128i) M256d {
	return M256d(cvtepi32Pd([16]byte(a)))
}

func cvtepi32Pd(a [16]byte) [4]float64


// MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd(src M256d, k Mmask8, a M128i) M256d {
	return M256d(maskCvtepi32Pd([4]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Pd(src [4]float64, k uint8, a [16]byte) [4]float64


// MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd(k Mmask8, a M128i) M256d {
	return M256d(maskzCvtepi32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Pd(k uint8, a [16]byte) [4]float64


// Cvtepi32Ps: Convert packed 32-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_cvtepi32_ps'.
// Requires AVX.
func Cvtepi32Ps(a M256i) M256 {
	return M256(cvtepi32Ps([32]byte(a)))
}

func cvtepi32Ps(a [32]byte) [8]float32


// MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps(src M256, k Mmask8, a M256i) M256 {
	return M256(maskCvtepi32Ps([8]float32(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Ps(src [8]float32, k uint8, a [32]byte) [8]float32


// MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps(k Mmask8, a M256i) M256 {
	return M256(maskzCvtepi32Ps(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Ps(k uint8, a [32]byte) [8]float32


// MaskCvtepi32Storeu16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_mask_cvtepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi32Storeu16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi32Storeu16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi32Storeu16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi32Storeu8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_mask_cvtepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi32Storeu8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi32Storeu8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi32Storeu8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi6416(a M256i) M128i {
	return M128i(cvtepi6416([32]byte(a)))
}

func cvtepi6416(a [32]byte) [16]byte


// MaskCvtepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi6416(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi6416([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi6416(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi6416(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi6416(uint8(k), [32]byte(a)))
}

func maskzCvtepi6416(k uint8, a [32]byte) [16]byte


// Cvtepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi6432(a M256i) M128i {
	return M128i(cvtepi6432([32]byte(a)))
}

func cvtepi6432(a [32]byte) [16]byte


// MaskCvtepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi6432(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi6432([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi6432(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi6432(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi6432(uint8(k), [32]byte(a)))
}

func maskzCvtepi6432(k uint8, a [32]byte) [16]byte


// Cvtepi648: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi648(a M256i) M128i {
	return M128i(cvtepi648([32]byte(a)))
}

func cvtepi648(a [32]byte) [16]byte


// MaskCvtepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi648(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi648([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi648(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi648(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi648(uint8(k), [32]byte(a)))
}

func maskzCvtepi648(k uint8, a [32]byte) [16]byte


// Cvtepi64Pd: Convert packed 64-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm256_cvtepi64_pd'.
// Requires AVX512DQ.
func Cvtepi64Pd(a M256i) M256d {
	return M256d(cvtepi64Pd([32]byte(a)))
}

func cvtepi64Pd(a [32]byte) [4]float64


// MaskCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm256_mask_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskCvtepi64Pd(src M256d, k Mmask8, a M256i) M256d {
	return M256d(maskCvtepi64Pd([4]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Pd(src [4]float64, k uint8, a [32]byte) [4]float64


// MaskzCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm256_maskz_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskzCvtepi64Pd(k Mmask8, a M256i) M256d {
	return M256d(maskzCvtepi64Pd(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Pd(k uint8, a [32]byte) [4]float64


// Cvtepi64Ps: Convert packed 64-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm256_cvtepi64_ps'.
// Requires AVX512DQ.
func Cvtepi64Ps(a M256i) M128 {
	return M128(cvtepi64Ps([32]byte(a)))
}

func cvtepi64Ps(a [32]byte) [4]float32


// MaskCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm256_mask_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskCvtepi64Ps(src M128, k Mmask8, a M256i) M128 {
	return M128(maskCvtepi64Ps([4]float32(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Ps(src [4]float32, k uint8, a [32]byte) [4]float32


// MaskzCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm256_maskz_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskzCvtepi64Ps(k Mmask8, a M256i) M128 {
	return M128(maskzCvtepi64Ps(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Ps(k uint8, a [32]byte) [4]float32


// MaskCvtepi64Storeu16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi64Storeu16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64Storeu16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64Storeu16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi64Storeu32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtepi64Storeu32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64Storeu32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64Storeu32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi64Storeu8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the active results (those with their
// respective bit set in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi64Storeu8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64Storeu8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64Storeu8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtepi816: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			dst[l+15:l] := SignExtend(a[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm256_cvtepi8_epi16'.
// Requires AVX2.
func Cvtepi816(a M128i) M256i {
	return M256i(cvtepi816([16]byte(a)))
}

func cvtepi816(a [16]byte) [32]byte


// MaskCvtepi816: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm256_mask_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskCvtepi816(src M256i, k Mmask16, a M128i) M256i {
	return M256i(maskCvtepi816([32]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepi816(src [32]byte, k uint16, a [16]byte) [32]byte


// MaskzCvtepi816: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm256_maskz_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskzCvtepi816(k Mmask16, a M128i) M256i {
	return M256i(maskzCvtepi816(uint16(k), [16]byte(a)))
}

func maskzCvtepi816(k uint16, a [16]byte) [32]byte


// Cvtepi832: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_cvtepi8_epi32'.
// Requires AVX2.
func Cvtepi832(a M128i) M256i {
	return M256i(cvtepi832([16]byte(a)))
}

func cvtepi832(a [16]byte) [32]byte


// MaskCvtepi832: Sign extend packed 8-bit integers in the low 8 bytes of 'a'
// to packed 32-bit integers, and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi832(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi832([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi832(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi832: Sign extend packed 8-bit integers in the low 8 bytes of 'a'
// to packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi832(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi832(uint8(k), [16]byte(a)))
}

func maskzCvtepi832(k uint8, a [16]byte) [32]byte


// Cvtepi864: Sign extend packed 8-bit integers in the low 8 bytes of 'a' to
// packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_cvtepi8_epi64'.
// Requires AVX2.
func Cvtepi864(a M128i) M256i {
	return M256i(cvtepi864([16]byte(a)))
}

func cvtepi864(a [16]byte) [32]byte


// MaskCvtepi864: Sign extend packed 8-bit integers in the low 4 bytes of 'a'
// to packed 64-bit integers, and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi864(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi864([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi864(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi864: Sign extend packed 8-bit integers in the low 4 bytes of 'a'
// to packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi864(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi864(uint8(k), [16]byte(a)))
}

func maskzCvtepi864(k uint8, a [16]byte) [32]byte


// Cvtepu1632: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_cvtepu16_epi32'.
// Requires AVX2.
func Cvtepu1632(a M128i) M256i {
	return M256i(cvtepu1632([16]byte(a)))
}

func cvtepu1632(a [16]byte) [32]byte


// MaskCvtepu1632: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu1632(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu1632([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu1632(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu1632: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu1632(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu1632(uint8(k), [16]byte(a)))
}

func maskzCvtepu1632(k uint8, a [16]byte) [32]byte


// Cvtepu1664: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_cvtepu16_epi64'.
// Requires AVX2.
func Cvtepu1664(a M128i) M256i {
	return M256i(cvtepu1664([16]byte(a)))
}

func cvtepu1664(a [16]byte) [32]byte


// MaskCvtepu1664: Zero extend packed unsigned 16-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu1664(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu1664([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu1664(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu1664: Zero extend packed unsigned 16-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu1664(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu1664(uint8(k), [16]byte(a)))
}

func maskzCvtepu1664(k uint8, a [16]byte) [32]byte


// Cvtepu3264: Zero extend packed unsigned 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := ZeroExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_cvtepu32_epi64'.
// Requires AVX2.
func Cvtepu3264(a M128i) M256i {
	return M256i(cvtepu3264([16]byte(a)))
}

func cvtepu3264(a [16]byte) [32]byte


// MaskCvtepu3264: Zero extend packed unsigned 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu3264(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu3264([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu3264(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu3264: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu3264(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu3264(uint8(k), [16]byte(a)))
}

func maskzCvtepu3264(k uint8, a [16]byte) [32]byte


// Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd(a M128i) M256d {
	return M256d(cvtepu32Pd([16]byte(a)))
}

func cvtepu32Pd(a [16]byte) [4]float64


// MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd(src M256d, k Mmask8, a M128i) M256d {
	return M256d(maskCvtepu32Pd([4]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Pd(src [4]float64, k uint8, a [16]byte) [4]float64


// MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd(k Mmask8, a M128i) M256d {
	return M256d(maskzCvtepu32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Pd(k uint8, a [16]byte) [4]float64


// Cvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm256_cvtepu64_pd'.
// Requires AVX512DQ.
func Cvtepu64Pd(a M256i) M256d {
	return M256d(cvtepu64Pd([32]byte(a)))
}

func cvtepu64Pd(a [32]byte) [4]float64


// MaskCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm256_mask_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskCvtepu64Pd(src M256d, k Mmask8, a M256i) M256d {
	return M256d(maskCvtepu64Pd([4]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepu64Pd(src [4]float64, k uint8, a [32]byte) [4]float64


// MaskzCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm256_maskz_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskzCvtepu64Pd(k Mmask8, a M256i) M256d {
	return M256d(maskzCvtepu64Pd(uint8(k), [32]byte(a)))
}

func maskzCvtepu64Pd(k uint8, a [32]byte) [4]float64


// Cvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm256_cvtepu64_ps'.
// Requires AVX512DQ.
func Cvtepu64Ps(a M256i) M128 {
	return M128(cvtepu64Ps([32]byte(a)))
}

func cvtepu64Ps(a [32]byte) [4]float32


// MaskCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm256_mask_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskCvtepu64Ps(src M128, k Mmask8, a M256i) M128 {
	return M128(maskCvtepu64Ps([4]float32(src), uint8(k), [32]byte(a)))
}

func maskCvtepu64Ps(src [4]float32, k uint8, a [32]byte) [4]float32


// MaskzCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm256_maskz_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskzCvtepu64Ps(k Mmask8, a M256i) M128 {
	return M128(maskzCvtepu64Ps(uint8(k), [32]byte(a)))
}

func maskzCvtepu64Ps(k uint8, a [32]byte) [4]float32


// Cvtepu816: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			dst[l+15:l] := ZeroExtend(a[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm256_cvtepu8_epi16'.
// Requires AVX2.
func Cvtepu816(a M128i) M256i {
	return M256i(cvtepu816([16]byte(a)))
}

func cvtepu816(a [16]byte) [32]byte


// MaskCvtepu816: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm256_mask_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskCvtepu816(src M256i, k Mmask16, a M128i) M256i {
	return M256i(maskCvtepu816([32]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepu816(src [32]byte, k uint16, a [16]byte) [32]byte


// MaskzCvtepu816: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm256_maskz_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskzCvtepu816(k Mmask16, a M128i) M256i {
	return M256i(maskzCvtepu816(uint16(k), [16]byte(a)))
}

func maskzCvtepu816(k uint16, a [16]byte) [32]byte


// Cvtepu832: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_cvtepu8_epi32'.
// Requires AVX2.
func Cvtepu832(a M128i) M256i {
	return M256i(cvtepu832([16]byte(a)))
}

func cvtepu832(a [16]byte) [32]byte


// MaskCvtepu832: Zero extend packed unsigned 8-bit integers in the low 8 bytes
// of 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu832(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu832([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu832(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu832: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu832(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu832(uint8(k), [16]byte(a)))
}

func maskzCvtepu832(k uint8, a [16]byte) [32]byte


// Cvtepu864: Zero extend packed unsigned 8-bit integers in the low 8 byte sof
// 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_cvtepu8_epi64'.
// Requires AVX2.
func Cvtepu864(a M128i) M256i {
	return M256i(cvtepu864([16]byte(a)))
}

func cvtepu864(a [16]byte) [32]byte


// MaskCvtepu864: Zero extend packed unsigned 8-bit integers in the low 4 bytes
// of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu864(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu864([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu864(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu864: Zero extend packed unsigned 8-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu864(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu864(uint8(k), [16]byte(a)))
}

func maskzCvtepu864(k uint8, a [16]byte) [32]byte


// Cvtpd32: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_cvtpd_epi32'.
// Requires AVX.
func Cvtpd32(a M256d) M128i {
	return M128i(cvtpd32([4]float64(a)))
}

func cvtpd32(a [4]float64) [16]byte


// MaskCvtpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpd32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvtpd32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpd32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvtpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpd32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvtpd32(uint8(k), [4]float64(a)))
}

func maskzCvtpd32(k uint8, a [4]float64) [16]byte


// Cvtpd64: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm256_cvtpd_epi64'.
// Requires AVX512DQ.
func Cvtpd64(a M256d) M256i {
	return M256i(cvtpd64([4]float64(a)))
}

func cvtpd64(a [4]float64) [32]byte


// MaskCvtpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm256_mask_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskCvtpd64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvtpd64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpd64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvtpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm256_maskz_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskzCvtpd64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvtpd64(uint8(k), [4]float64(a)))
}

func maskzCvtpd64(k uint8, a [4]float64) [32]byte


// CvtpdEpu32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu32(a M256d) M128i {
	return M128i(cvtpdEpu32([4]float64(a)))
}

func cvtpdEpu32(a [4]float64) [16]byte


// MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvtpdEpu32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpu32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvtpdEpu32(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpu32(k uint8, a [4]float64) [16]byte


// CvtpdEpu64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm256_cvtpd_epu64'.
// Requires AVX512DQ.
func CvtpdEpu64(a M256d) M256i {
	return M256i(cvtpdEpu64([4]float64(a)))
}

func cvtpdEpu64(a [4]float64) [32]byte


// MaskCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm256_mask_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskCvtpdEpu64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvtpdEpu64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpu64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm256_maskz_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskzCvtpdEpu64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvtpdEpu64(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpu64(k uint8, a [4]float64) [32]byte


// CvtpdPs: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_cvtpd_ps'.
// Requires AVX.
func CvtpdPs(a M256d) M128 {
	return M128(cvtpdPs([4]float64(a)))
}

func cvtpdPs(a [4]float64) [4]float32


// MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs(src M128, k Mmask8, a M256d) M128 {
	return M128(maskCvtpdPs([4]float32(src), uint8(k), [4]float64(a)))
}

func maskCvtpdPs(src [4]float32, k uint8, a [4]float64) [4]float32


// MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs(k Mmask8, a M256d) M128 {
	return M128(maskzCvtpdPs(uint8(k), [4]float64(a)))
}

func maskzCvtpdPs(k uint8, a [4]float64) [4]float32


// CvtphPs: Convert packed half-precision (16-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_cvtph_ps'.
// Requires FP16C.
func CvtphPs(a M128i) M256 {
	return M256(cvtphPs([16]byte(a)))
}

func cvtphPs(a [16]byte) [8]float32


// MaskCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs(src M256, k Mmask8, a M128i) M256 {
	return M256(maskCvtphPs([8]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtphPs(src [8]float32, k uint8, a [16]byte) [8]float32


// MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs(k Mmask8, a M128i) M256 {
	return M256(maskzCvtphPs(uint8(k), [16]byte(a)))
}

func maskzCvtphPs(k uint8, a [16]byte) [8]float32


// Cvtps32: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_cvtps_epi32'.
// Requires AVX.
func Cvtps32(a M256) M256i {
	return M256i(cvtps32([8]float32(a)))
}

func cvtps32(a [8]float32) [32]byte


// MaskCvtps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtps32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvtps32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtps32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvtps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtps32(k Mmask8, a M256) M256i {
	return M256i(maskzCvtps32(uint8(k), [8]float32(a)))
}

func maskzCvtps32(k uint8, a [8]float32) [32]byte


// Cvtps64: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm256_cvtps_epi64'.
// Requires AVX512DQ.
func Cvtps64(a M128) M256i {
	return M256i(cvtps64([4]float32(a)))
}

func cvtps64(a [4]float32) [32]byte


// MaskCvtps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm256_mask_cvtps_epi64'.
// Requires AVX512DQ.
func MaskCvtps64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvtps64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtps64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvtps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm256_maskz_cvtps_epi64'.
// Requires AVX512DQ.
func MaskzCvtps64(k Mmask8, a M128) M256i {
	return M256i(maskzCvtps64(uint8(k), [4]float32(a)))
}

func maskzCvtps64(k uint8, a [4]float32) [32]byte


// CvtpsEpu32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu32(a M256) M256i {
	return M256i(cvtpsEpu32([8]float32(a)))
}

func cvtpsEpu32(a [8]float32) [32]byte


// MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvtpsEpu32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtpsEpu32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu32(k Mmask8, a M256) M256i {
	return M256i(maskzCvtpsEpu32(uint8(k), [8]float32(a)))
}

func maskzCvtpsEpu32(k uint8, a [8]float32) [32]byte


// CvtpsEpu64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm256_cvtps_epu64'.
// Requires AVX512DQ.
func CvtpsEpu64(a M128) M256i {
	return M256i(cvtpsEpu64([4]float32(a)))
}

func cvtpsEpu64(a [4]float32) [32]byte


// MaskCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm256_mask_cvtps_epu64'.
// Requires AVX512DQ.
func MaskCvtpsEpu64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvtpsEpu64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpu64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm256_maskz_cvtps_epu64'.
// Requires AVX512DQ.
func MaskzCvtpsEpu64(k Mmask8, a M128) M256i {
	return M256i(maskzCvtpsEpu64(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpu64(k uint8, a [4]float32) [32]byte


// CvtpsPd: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed double-precision (64-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm256_cvtps_pd'.
// Requires AVX.
func CvtpsPd(a M128) M256d {
	return M256d(cvtpsPd([4]float32(a)))
}

func cvtpsPd(a [4]float32) [4]float64


// CvtpsPh: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed half-precision (16-bit) floating-point elements, and store the
// results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_cvtps_ph'.
// Requires FP16C.
func CvtpsPh(a M256, rounding int) M128i {
	return M128i(cvtpsPh([8]float32(a), rounding))
}

func cvtpsPh(a [8]float32, rounding int) [16]byte


// MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh(src M128i, k Mmask8, a M256, rounding int) M128i {
	return M128i(maskCvtpsPh([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtpsPh(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh(k Mmask8, a M256, rounding int) M128i {
	return M128i(maskzCvtpsPh(uint8(k), [8]float32(a), rounding))
}

func maskzCvtpsPh(k uint8, a [8]float32, rounding int) [16]byte


// Cvtsepi168: Convert packed 16-bit integers in 'a' to packed 8-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_cvtsepi16_epi8'.
// Requires AVX512BW.
func Cvtsepi168(a M256i) M128i {
	return M128i(cvtsepi168([32]byte(a)))
}

func cvtsepi168(a [32]byte) [16]byte


// MaskCvtsepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_mask_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskCvtsepi168(src M128i, k Mmask16, a M256i) M128i {
	return M128i(maskCvtsepi168([16]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtsepi168(src [16]byte, k uint16, a [32]byte) [16]byte


// MaskzCvtsepi168: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_maskz_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtsepi168(k Mmask16, a M256i) M128i {
	return M128i(maskzCvtsepi168(uint16(k), [32]byte(a)))
}

func maskzCvtsepi168(k uint16, a [32]byte) [16]byte


// MaskCvtsepi16Storeu8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_mask_cvtsepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtsepi16Storeu8(base_addr uintptr, k Mmask16, a M256i)  {
	maskCvtsepi16Storeu8(uintptr(base_addr), uint16(k), [32]byte(a))
}

func maskCvtsepi16Storeu8(base_addr uintptr, k uint16, a [32]byte) 


// Cvtsepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi3216(a M256i) M128i {
	return M128i(cvtsepi3216([32]byte(a)))
}

func cvtsepi3216(a [32]byte) [16]byte


// MaskCvtsepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi3216(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi3216([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi3216(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi3216: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi3216(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi3216(uint8(k), [32]byte(a)))
}

func maskzCvtsepi3216(k uint8, a [32]byte) [16]byte


// Cvtsepi328: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi328(a M256i) M128i {
	return M128i(cvtsepi328([32]byte(a)))
}

func cvtsepi328(a [32]byte) [16]byte


// MaskCvtsepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi328(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi328([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi328(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi328: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi328(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi328(uint8(k), [32]byte(a)))
}

func maskzCvtsepi328(k uint8, a [32]byte) [16]byte


// MaskCvtsepi32Storeu16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_mask_cvtsepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Storeu16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi32Storeu16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi32Storeu16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi32Storeu8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_mask_cvtsepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Storeu8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi32Storeu8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi32Storeu8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtsepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi6416(a M256i) M128i {
	return M128i(cvtsepi6416([32]byte(a)))
}

func cvtsepi6416(a [32]byte) [16]byte


// MaskCvtsepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi6416(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi6416([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi6416(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi6416: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi6416(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi6416(uint8(k), [32]byte(a)))
}

func maskzCvtsepi6416(k uint8, a [32]byte) [16]byte


// Cvtsepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi6432(a M256i) M128i {
	return M128i(cvtsepi6432([32]byte(a)))
}

func cvtsepi6432(a [32]byte) [16]byte


// MaskCvtsepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi6432(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi6432([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi6432(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi6432: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi6432(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi6432(uint8(k), [32]byte(a)))
}

func maskzCvtsepi6432(k uint8, a [32]byte) [16]byte


// Cvtsepi648: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi648(a M256i) M128i {
	return M128i(cvtsepi648([32]byte(a)))
}

func cvtsepi648(a [32]byte) [16]byte


// MaskCvtsepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi648(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi648([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi648(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi648: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi648(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi648(uint8(k), [32]byte(a)))
}

func maskzCvtsepi648(k uint8, a [32]byte) [16]byte


// MaskCvtsepi64Storeu16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Storeu16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64Storeu16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64Storeu16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi64Storeu32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Storeu32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64Storeu32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64Storeu32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi64Storeu8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Storeu8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64Storeu8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64Storeu8(base_addr uintptr, k uint8, a [32]byte) 


// Cvttpd32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 32-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_cvttpd_epi32'.
// Requires AVX.
func Cvttpd32(a M256d) M128i {
	return M128i(cvttpd32([4]float64(a)))
}

func cvttpd32(a [4]float64) [16]byte


// MaskCvttpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpd32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvttpd32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpd32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvttpd32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpd32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvttpd32(uint8(k), [4]float64(a)))
}

func maskzCvttpd32(k uint8, a [4]float64) [16]byte


// Cvttpd64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 64-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm256_cvttpd_epi64'.
// Requires AVX512DQ.
func Cvttpd64(a M256d) M256i {
	return M256i(cvttpd64([4]float64(a)))
}

func cvttpd64(a [4]float64) [32]byte


// MaskCvttpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm256_mask_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskCvttpd64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvttpd64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpd64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvttpd64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm256_maskz_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskzCvttpd64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvttpd64(uint8(k), [4]float64(a)))
}

func maskzCvttpd64(k uint8, a [4]float64) [32]byte


// CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu32(a M256d) M128i {
	return M128i(cvttpdEpu32([4]float64(a)))
}

func cvttpdEpu32(a [4]float64) [16]byte


// MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvttpdEpu32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpu32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvttpdEpu32(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpu32(k uint8, a [4]float64) [16]byte


// CvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm256_cvttpd_epu64'.
// Requires AVX512DQ.
func CvttpdEpu64(a M256d) M256i {
	return M256i(cvttpdEpu64([4]float64(a)))
}

func cvttpdEpu64(a [4]float64) [32]byte


// MaskCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm256_mask_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskCvttpdEpu64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvttpdEpu64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpu64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm256_maskz_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskzCvttpdEpu64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvttpdEpu64(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpu64(k uint8, a [4]float64) [32]byte


// Cvttps32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 32-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_cvttps_epi32'.
// Requires AVX.
func Cvttps32(a M256) M256i {
	return M256i(cvttps32([8]float32(a)))
}

func cvttps32(a [8]float32) [32]byte


// MaskCvttps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttps32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvttps32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttps32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvttps32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttps32(k Mmask8, a M256) M256i {
	return M256i(maskzCvttps32(uint8(k), [8]float32(a)))
}

func maskzCvttps32(k uint8, a [8]float32) [32]byte


// Cvttps64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 64-bit integers with truncation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm256_cvttps_epi64'.
// Requires AVX512DQ.
func Cvttps64(a M128) M256i {
	return M256i(cvttps64([4]float32(a)))
}

func cvttps64(a [4]float32) [32]byte


// MaskCvttps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm256_mask_cvttps_epi64'.
// Requires AVX512DQ.
func MaskCvttps64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvttps64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttps64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvttps64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm256_maskz_cvttps_epi64'.
// Requires AVX512DQ.
func MaskzCvttps64(k Mmask8, a M128) M256i {
	return M256i(maskzCvttps64(uint8(k), [4]float32(a)))
}

func maskzCvttps64(k uint8, a [4]float32) [32]byte


// CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu32(a M256) M256i {
	return M256i(cvttpsEpu32([8]float32(a)))
}

func cvttpsEpu32(a [8]float32) [32]byte


// MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvttpsEpu32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttpsEpu32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu32(k Mmask8, a M256) M256i {
	return M256i(maskzCvttpsEpu32(uint8(k), [8]float32(a)))
}

func maskzCvttpsEpu32(k uint8, a [8]float32) [32]byte


// CvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm256_cvttps_epu64'.
// Requires AVX512DQ.
func CvttpsEpu64(a M128) M256i {
	return M256i(cvttpsEpu64([4]float32(a)))
}

func cvttpsEpu64(a [4]float32) [32]byte


// MaskCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm256_mask_cvttps_epu64'.
// Requires AVX512DQ.
func MaskCvttpsEpu64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvttpsEpu64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpu64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm256_maskz_cvttps_epu64'.
// Requires AVX512DQ.
func MaskzCvttpsEpu64(k Mmask8, a M128) M256i {
	return M256i(maskzCvttpsEpu64(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpu64(k uint8, a [4]float32) [32]byte


// Cvtusepi168: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_cvtusepi16_epi8'.
// Requires AVX512BW.
func Cvtusepi168(a M256i) M128i {
	return M128i(cvtusepi168([32]byte(a)))
}

func cvtusepi168(a [32]byte) [16]byte


// MaskCvtusepi168: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_mask_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskCvtusepi168(src M128i, k Mmask16, a M256i) M128i {
	return M128i(maskCvtusepi168([16]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtusepi168(src [16]byte, k uint16, a [32]byte) [16]byte


// MaskzCvtusepi168: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_maskz_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtusepi168(k Mmask16, a M256i) M128i {
	return M128i(maskzCvtusepi168(uint16(k), [32]byte(a)))
}

func maskzCvtusepi168(k uint16, a [32]byte) [16]byte


// MaskCvtusepi16Storeu8: Convert packed unsigned 16-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_mask_cvtusepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtusepi16Storeu8(base_addr uintptr, k Mmask16, a M256i)  {
	maskCvtusepi16Storeu8(uintptr(base_addr), uint16(k), [32]byte(a))
}

func maskCvtusepi16Storeu8(base_addr uintptr, k uint16, a [32]byte) 


// Cvtusepi3216: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi3216(a M256i) M128i {
	return M128i(cvtusepi3216([32]byte(a)))
}

func cvtusepi3216(a [32]byte) [16]byte


// MaskCvtusepi3216: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi3216(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi3216([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi3216(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi3216: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi3216(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi3216(uint8(k), [32]byte(a)))
}

func maskzCvtusepi3216(k uint8, a [32]byte) [16]byte


// Cvtusepi328: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi328(a M256i) M128i {
	return M128i(cvtusepi328([32]byte(a)))
}

func cvtusepi328(a [32]byte) [16]byte


// MaskCvtusepi328: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi328(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi328([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi328(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi328: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi328(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi328(uint8(k), [32]byte(a)))
}

func maskzCvtusepi328(k uint8, a [32]byte) [16]byte


// MaskCvtusepi32Storeu16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_mask_cvtusepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Storeu16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi32Storeu16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi32Storeu16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi32Storeu8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_mask_cvtusepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Storeu8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi32Storeu8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi32Storeu8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtusepi6416: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi6416(a M256i) M128i {
	return M128i(cvtusepi6416([32]byte(a)))
}

func cvtusepi6416(a [32]byte) [16]byte


// MaskCvtusepi6416: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi6416(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi6416([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi6416(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi6416: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi6416(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi6416(uint8(k), [32]byte(a)))
}

func maskzCvtusepi6416(k uint8, a [32]byte) [16]byte


// Cvtusepi6432: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi6432(a M256i) M128i {
	return M128i(cvtusepi6432([32]byte(a)))
}

func cvtusepi6432(a [32]byte) [16]byte


// MaskCvtusepi6432: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi6432(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi6432([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi6432(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi6432: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi6432(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi6432(uint8(k), [32]byte(a)))
}

func maskzCvtusepi6432(k uint8, a [32]byte) [16]byte


// Cvtusepi648: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi648(a M256i) M128i {
	return M128i(cvtusepi648([32]byte(a)))
}

func cvtusepi648(a [32]byte) [16]byte


// MaskCvtusepi648: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi648(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi648([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi648(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi648: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi648(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi648(uint8(k), [32]byte(a)))
}

func maskzCvtusepi648(k uint8, a [32]byte) [16]byte


// MaskCvtusepi64Storeu16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Storeu16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64Storeu16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64Storeu16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi64Storeu32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Storeu32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64Storeu32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64Storeu32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi64Storeu8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Storeu8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64Storeu8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64Storeu8(base_addr uintptr, k uint8, a [32]byte) 


// DbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets of
// unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst'.
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm256_dbsad_epu8'.
// Requires AVX512BW.
func DbsadEpu8(a M256i, b M256i, imm8 int) M256i {
	return M256i(dbsadEpu8([32]byte(a), [32]byte(b), imm8))
}

func dbsadEpu8(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskDbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets
// of unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 3
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm256_mask_dbsad_epu8'.
// Requires AVX512BW.
func MaskDbsadEpu8(src M256i, k Mmask16, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskDbsadEpu8([32]byte(src), uint16(k), [32]byte(a), [32]byte(b), imm8))
}

func maskDbsadEpu8(src [32]byte, k uint16, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzDbsadEpu8: Compute the sum of absolute differences (SADs) of
// quadruplets of unsigned 8-bit integers in 'a' compared to those in 'b', and
// store the 16-bit results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 3
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm256_maskz_dbsad_epu8'.
// Requires AVX512BW.
func MaskzDbsadEpu8(k Mmask16, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzDbsadEpu8(uint16(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzDbsadEpu8(k uint16, a [32]byte, b [32]byte, imm8 int) [32]byte


// Div16: Divide packed 16-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi16'.
// Requires AVX.
func Div16(a M256i, b M256i) M256i {
	return M256i(div16([32]byte(a), [32]byte(b)))
}

func div16(a [32]byte, b [32]byte) [32]byte


// Div32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi32'.
// Requires AVX.
func Div32(a M256i, b M256i) M256i {
	return M256i(div32([32]byte(a), [32]byte(b)))
}

func div32(a [32]byte, b [32]byte) [32]byte


// Div64: Divide packed 64-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi64'.
// Requires AVX.
func Div64(a M256i, b M256i) M256i {
	return M256i(div64([32]byte(a), [32]byte(b)))
}

func div64(a [32]byte, b [32]byte) [32]byte


// Div8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi8'.
// Requires AVX.
func Div8(a M256i, b M256i) M256i {
	return M256i(div8([32]byte(a), [32]byte(b)))
}

func div8(a [32]byte, b [32]byte) [32]byte


// DivEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu16'.
// Requires AVX.
func DivEpu16(a M256i, b M256i) M256i {
	return M256i(divEpu16([32]byte(a), [32]byte(b)))
}

func divEpu16(a [32]byte, b [32]byte) [32]byte


// DivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu32'.
// Requires AVX.
func DivEpu32(a M256i, b M256i) M256i {
	return M256i(divEpu32([32]byte(a), [32]byte(b)))
}

func divEpu32(a [32]byte, b [32]byte) [32]byte


// DivEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu64'.
// Requires AVX.
func DivEpu64(a M256i, b M256i) M256i {
	return M256i(divEpu64([32]byte(a), [32]byte(b)))
}

func divEpu64(a [32]byte, b [32]byte) [32]byte


// DivEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu8'.
// Requires AVX.
func DivEpu8(a M256i, b M256i) M256i {
	return M256i(divEpu8([32]byte(a), [32]byte(b)))
}

func divEpu8(a [32]byte, b [32]byte) [32]byte


// DivPd: Divide packed double-precision (64-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_div_pd'.
// Requires AVX.
func DivPd(a M256d, b M256d) M256d {
	return M256d(divPd([4]float64(a), [4]float64(b)))
}

func divPd(a [4]float64, b [4]float64) [4]float64


// MaskDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskDivPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskDivPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzDivPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzDivPd(k uint8, a [4]float64, b [4]float64) [4]float64


// DivPs: Divide packed single-precision (32-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_div_ps'.
// Requires AVX.
func DivPs(a M256, b M256) M256 {
	return M256(divPs([8]float32(a), [8]float32(b)))
}

func divPs(a [8]float32, b [8]float32) [8]float32


// MaskDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskDivPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskDivPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzDivPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzDivPs(k uint8, a [8]float32, b [8]float32) [8]float32


// DpPs: Conditionally multiply the packed single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the high 4 bits in 'imm8', sum
// the four products, and conditionally store the sum in 'dst' using the low 4
// bits of 'imm8'. 
//
//		DP(a[127:0], b[127:0], imm8[7:0]) {
//			FOR j := 0 to 3
//				i := j*32
//				IF imm8[(4+j)%8]
//					temp[i+31:i] := a[i+31:i] * b[i+31:i]
//				ELSE
//					temp[i+31:i] := 0
//				FI
//			ENDFOR
//			
//			sum[31:0] := (temp[127:96] + temp[95:64]) + (temp[63:32] + temp[31:0])
//			
//			FOR j := 0 to 3
//				i := j*32
//				IF imm8[j%8]
//					tmpdst[i+31:i] := sum[31:0]
//				ELSE
//					tmpdst[i+31:i] := 0
//				FI
//			ENDFOR
//			RETURN tmpdst[127:0]
//		}
//		
//		dst[127:0] := DP(a[127:0], b[127:0], imm8[7:0])
//		dst[255:128] := DP(a[255:128], b[255:128], imm8[7:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VDPPS'. Intrinsic: '_mm256_dp_ps'.
// Requires AVX.
func DpPs(a M256, b M256, imm8 int) M256 {
	return M256(dpPs([8]float32(a), [8]float32(b), imm8))
}

func dpPs(a [8]float32, b [8]float32, imm8 int) [8]float32


// ErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erf_pd'.
// Requires AVX.
func ErfPd(a M256d) M256d {
	return M256d(erfPd([4]float64(a)))
}

func erfPd(a [4]float64) [4]float64


// ErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erf_ps'.
// Requires AVX.
func ErfPs(a M256) M256 {
	return M256(erfPs([8]float32(a)))
}

func erfPs(a [8]float32) [8]float32


// ErfcPd: Compute the complementary error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfc_pd'.
// Requires AVX.
func ErfcPd(a M256d) M256d {
	return M256d(erfcPd([4]float64(a)))
}

func erfcPd(a [4]float64) [4]float64


// ErfcPs: Compute the complementary error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfc_ps'.
// Requires AVX.
func ErfcPs(a M256) M256 {
	return M256(erfcPs([8]float32(a)))
}

func erfcPs(a [8]float32) [8]float32


// ErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfcinv_pd'.
// Requires AVX.
func ErfcinvPd(a M256d) M256d {
	return M256d(erfcinvPd([4]float64(a)))
}

func erfcinvPd(a [4]float64) [4]float64


// ErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfcinv_ps'.
// Requires AVX.
func ErfcinvPs(a M256) M256 {
	return M256(erfcinvPs([8]float32(a)))
}

func erfcinvPs(a [8]float32) [8]float32


// ErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfinv_pd'.
// Requires AVX.
func ErfinvPd(a M256d) M256d {
	return M256d(erfinvPd([4]float64(a)))
}

func erfinvPd(a [4]float64) [4]float64


// ErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfinv_ps'.
// Requires AVX.
func ErfinvPs(a M256) M256 {
	return M256(erfinvPs([8]float32(a)))
}

func erfinvPs(a [8]float32) [8]float32


// ExpPd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp_pd'.
// Requires AVX.
func ExpPd(a M256d) M256d {
	return M256d(expPd([4]float64(a)))
}

func expPd(a [4]float64) [4]float64


// ExpPs: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp_ps'.
// Requires AVX.
func ExpPs(a M256) M256 {
	return M256(expPs([8]float32(a)))
}

func expPs(a [8]float32) [8]float32


// Exp10Pd: Compute the exponential value of 10 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 10^(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp10_pd'.
// Requires AVX.
func Exp10Pd(a M256d) M256d {
	return M256d(exp10Pd([4]float64(a)))
}

func exp10Pd(a [4]float64) [4]float64


// Exp10Ps: Compute the exponential value of 10 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 10^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp10_ps'.
// Requires AVX.
func Exp10Ps(a M256) M256 {
	return M256(exp10Ps([8]float32(a)))
}

func exp10Ps(a [8]float32) [8]float32


// Exp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 2^(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp2_pd'.
// Requires AVX.
func Exp2Pd(a M256d) M256d {
	return M256d(exp2Pd([4]float64(a)))
}

func exp2Pd(a [4]float64) [4]float64


// Exp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 2^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp2_ps'.
// Requires AVX.
func Exp2Ps(a M256) M256 {
	return M256(exp2Ps([8]float32(a)))
}

func exp2Ps(a [8]float32) [8]float32


// MaskExpand32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpand32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskExpand32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskExpand32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzExpand32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpand32(k Mmask8, a M256i) M256i {
	return M256i(maskzExpand32(uint8(k), [32]byte(a)))
}

func maskzExpand32(k uint8, a [32]byte) [32]byte


// MaskExpand64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpand64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskExpand64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskExpand64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzExpand64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpand64(k Mmask8, a M256i) M256i {
	return M256i(maskzExpand64(uint8(k), [32]byte(a)))
}

func maskzExpand64(k uint8, a [32]byte) [32]byte


// MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskExpandPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskExpandPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd(k Mmask8, a M256d) M256d {
	return M256d(maskzExpandPd(uint8(k), [4]float64(a)))
}

func maskzExpandPd(k uint8, a [4]float64) [4]float64


// MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskExpandPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskExpandPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs(k Mmask8, a M256) M256 {
	return M256(maskzExpandPs(uint8(k), [8]float32(a)))
}

func maskzExpandPs(k uint8, a [8]float32) [8]float32


// MaskExpandloadu32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_mask_expandloadu_epi32'.
// Requires AVX512F.
func MaskExpandloadu32(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskExpandloadu32([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloadu32(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzExpandloadu32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_maskz_expandloadu_epi32'.
// Requires AVX512F.
func MaskzExpandloadu32(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzExpandloadu32(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloadu32(k uint8, mem_addr uintptr) [32]byte


// MaskExpandloadu64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_mask_expandloadu_epi64'.
// Requires AVX512F.
func MaskExpandloadu64(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskExpandloadu64([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloadu64(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzExpandloadu64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_maskz_expandloadu_epi64'.
// Requires AVX512F.
func MaskzExpandloadu64(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzExpandloadu64(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloadu64(k uint8, mem_addr uintptr) [32]byte


// MaskExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_mask_expandloadu_pd'.
// Requires AVX512F.
func MaskExpandloaduPd(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskExpandloaduPd([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPd(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_maskz_expandloadu_pd'.
// Requires AVX512F.
func MaskzExpandloaduPd(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzExpandloaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPd(k uint8, mem_addr uintptr) [4]float64


// MaskExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_mask_expandloadu_ps'.
// Requires AVX512F.
func MaskExpandloaduPs(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskExpandloaduPs([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPs(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_maskz_expandloadu_ps'.
// Requires AVX512F.
func MaskzExpandloaduPs(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzExpandloaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPs(k uint8, mem_addr uintptr) [8]float32


// Expm1Pd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i]) - 1.0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_expm1_pd'.
// Requires AVX.
func Expm1Pd(a M256d) M256d {
	return M256d(expm1Pd([4]float64(a)))
}

func expm1Pd(a [4]float64) [4]float64


// Expm1Ps: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i]) - 1.0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_expm1_ps'.
// Requires AVX.
func Expm1Ps(a M256) M256 {
	return M256(expm1Ps([8]float32(a)))
}

func expm1Ps(a [8]float32) [8]float32


// Extract16: Extract a 16-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[15:0] := (a[255:0] >> (index * 16))[15:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi16'.
// Requires AVX.
func Extract16(a M256i, index int) int16 {
	return int16(extract16([32]byte(a), index))
}

func extract16(a [32]byte, index int) int16


// Extract32: Extract a 32-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[31:0] := (a[255:0] >> (index * 32))[31:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi32'.
// Requires AVX.
func Extract32(a M256i, index int) int32 {
	return int32(extract32([32]byte(a), index))
}

func extract32(a [32]byte, index int) int32


// Extract64: Extract a 64-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[63:0] := (a[255:0] >> (index * 64))[63:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi64'.
// Requires AVX.
func Extract64(a M256i, index int) int64 {
	return int64(extract64([32]byte(a), index))
}

func extract64(a [32]byte, index int) int64


// Extract8: Extract an 8-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[7:0] := (a[255:0] >> (index * 8))[7:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi8'.
// Requires AVX.
func Extract8(a M256i, index int) int8 {
	return int8(extract8([32]byte(a), index))
}

func extract8(a [32]byte, index int) int8


// Extractf128Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF128'. Intrinsic: '_mm256_extractf128_pd'.
// Requires AVX.
func Extractf128Pd(a M256d, imm8 int) M128d {
	return M128d(extractf128Pd([4]float64(a), imm8))
}

func extractf128Pd(a [4]float64, imm8 int) [2]float64


// Extractf128Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF128'. Intrinsic: '_mm256_extractf128_ps'.
// Requires AVX.
func Extractf128Ps(a M256, imm8 int) M128 {
	return M128(extractf128Ps([8]float32(a), imm8))
}

func extractf128Ps(a [8]float32, imm8 int) [4]float32


// Extractf128Si256: Extract 128 bits (composed of integer data) from 'a',
// selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF128'. Intrinsic: '_mm256_extractf128_si256'.
// Requires AVX.
func Extractf128Si256(a M256i, imm8 int) M128i {
	return M128i(extractf128Si256([32]byte(a), imm8))
}

func extractf128Si256(a [32]byte, imm8 int) [16]byte


// Extractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_extractf32x4_ps'.
// Requires AVX512F.
func Extractf32x4Ps(a M256, imm8 int) M128 {
	return M128(extractf32x4Ps([8]float32(a), imm8))
}

func extractf32x4Ps(a [8]float32, imm8 int) [4]float32


// MaskExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_mask_extractf32x4_ps'.
// Requires AVX512F.
func MaskExtractf32x4Ps(src M128, k Mmask8, a M256, imm8 int) M128 {
	return M128(maskExtractf32x4Ps([4]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskExtractf32x4Ps(src [4]float32, k uint8, a [8]float32, imm8 int) [4]float32


// MaskzExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_maskz_extractf32x4_ps'.
// Requires AVX512F.
func MaskzExtractf32x4Ps(k Mmask8, a M256, imm8 int) M128 {
	return M128(maskzExtractf32x4Ps(uint8(k), [8]float32(a), imm8))
}

func maskzExtractf32x4Ps(k uint8, a [8]float32, imm8 int) [4]float32


// Extractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm256_extractf64x2_pd'.
// Requires AVX512DQ.
func Extractf64x2Pd(a M256d, imm8 int) M128d {
	return M128d(extractf64x2Pd([4]float64(a), imm8))
}

func extractf64x2Pd(a [4]float64, imm8 int) [2]float64


// MaskExtractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm256_mask_extractf64x2_pd'.
// Requires AVX512DQ.
func MaskExtractf64x2Pd(src M128d, k Mmask8, a M256d, imm8 int) M128d {
	return M128d(maskExtractf64x2Pd([2]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskExtractf64x2Pd(src [2]float64, k uint8, a [4]float64, imm8 int) [2]float64


// MaskzExtractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm256_maskz_extractf64x2_pd'.
// Requires AVX512DQ.
func MaskzExtractf64x2Pd(k Mmask8, a M256d, imm8 int) M128d {
	return M128d(maskzExtractf64x2Pd(uint8(k), [4]float64(a), imm8))
}

func maskzExtractf64x2Pd(k uint8, a [4]float64, imm8 int) [2]float64


// Extracti128Si256: Extract 128 bits (composed of integer data) from 'a',
// selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI128'. Intrinsic: '_mm256_extracti128_si256'.
// Requires AVX2.
func Extracti128Si256(a M256i, imm8 int) M128i {
	return M128i(extracti128Si256([32]byte(a), imm8))
}

func extracti128Si256(a [32]byte, imm8 int) [16]byte


// Extracti32x432: Extract 128 bits (composed of 4 packed 32-bit integers) from
// 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_extracti32x4_epi32'.
// Requires AVX512F.
func Extracti32x432(a M256i, imm8 int) M128i {
	return M128i(extracti32x432([32]byte(a), imm8))
}

func extracti32x432(a [32]byte, imm8 int) [16]byte


// MaskExtracti32x432: Extract 128 bits (composed of 4 packed 32-bit integers)
// from 'a', selected with 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_mask_extracti32x4_epi32'.
// Requires AVX512F.
func MaskExtracti32x432(src M128i, k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskExtracti32x432([16]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskExtracti32x432(src [16]byte, k uint8, a [32]byte, imm8 int) [16]byte


// MaskzExtracti32x432: Extract 128 bits (composed of 4 packed 32-bit integers)
// from 'a', selected with 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_maskz_extracti32x4_epi32'.
// Requires AVX512F.
func MaskzExtracti32x432(k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskzExtracti32x432(uint8(k), [32]byte(a), imm8))
}

func maskzExtracti32x432(k uint8, a [32]byte, imm8 int) [16]byte


// Extracti64x264: Extract 128 bits (composed of 2 packed 64-bit integers) from
// 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm256_extracti64x2_epi64'.
// Requires AVX512DQ.
func Extracti64x264(a M256i, imm8 int) M128i {
	return M128i(extracti64x264([32]byte(a), imm8))
}

func extracti64x264(a [32]byte, imm8 int) [16]byte


// MaskExtracti64x264: Extract 128 bits (composed of 2 packed 64-bit integers)
// from 'a', selected with 'imm8', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm256_mask_extracti64x2_epi64'.
// Requires AVX512DQ.
func MaskExtracti64x264(src M128i, k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskExtracti64x264([16]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskExtracti64x264(src [16]byte, k uint8, a [32]byte, imm8 int) [16]byte


// MaskzExtracti64x264: Extract 128 bits (composed of 2 packed 64-bit integers)
// from 'a', selected with 'imm8', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm256_maskz_extracti64x2_epi64'.
// Requires AVX512DQ.
func MaskzExtracti64x264(k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskzExtracti64x264(uint8(k), [32]byte(a), imm8))
}

func maskzExtracti64x264(k uint8, a [32]byte, imm8 int) [16]byte


// FixupimmPd: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? –INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 1⁄2
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd(a M256d, b M256d, c M256i, imm8 int) M256d {
	return M256d(fixupimmPd([4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func fixupimmPd(a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? –INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 1⁄2
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd(a M256d, k Mmask8, b M256d, c M256i, imm8 int) M256d {
	return M256d(maskFixupimmPd([4]float64(a), uint8(k), [4]float64(b), [32]byte(c), imm8))
}

func maskFixupimmPd(a [4]float64, k uint8, b [4]float64, c [32]byte, imm8 int) [4]float64


// MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? –INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 1⁄2
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd(k Mmask8, a M256d, b M256d, c M256i, imm8 int) M256d {
	return M256d(maskzFixupimmPd(uint8(k), [4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func maskzFixupimmPd(k uint8, a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// FixupimmPs: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? –INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 1⁄2
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs(a M256, b M256, c M256i, imm8 int) M256 {
	return M256(fixupimmPs([8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func fixupimmPs(a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? –INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 1⁄2
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs(a M256, k Mmask8, b M256, c M256i, imm8 int) M256 {
	return M256(maskFixupimmPs([8]float32(a), uint8(k), [8]float32(b), [32]byte(c), imm8))
}

func maskFixupimmPs(a [8]float32, k uint8, b [8]float32, c [32]byte, imm8 int) [8]float32


// MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? –INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 1⁄2
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs(k Mmask8, a M256, b M256, c M256i, imm8 int) M256 {
	return M256(maskzFixupimmPs(uint8(k), [8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func maskzFixupimmPs(k uint8, a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// FloorPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPD'. Intrinsic: '_mm256_floor_pd'.
// Requires AVX.
func FloorPd(a M256d) M256d {
	return M256d(floorPd([4]float64(a)))
}

func floorPd(a [4]float64) [4]float64


// FloorPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm256_floor_ps'.
// Requires AVX.
func FloorPs(a M256) M256 {
	return M256(floorPs([8]float32(a)))
}

func floorPs(a [8]float32) [8]float32


// FmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_fmadd_pd'.
// Requires FMA.
func FmaddPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmaddPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmaddPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask_fmadd_pd'.
// Requires AVX512F.
func MaskFmaddPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask3_fmadd_pd'.
// Requires AVX512F.
func Mask3FmaddPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_fmadd_ps'.
// Requires FMA.
func FmaddPs(a M256, b M256, c M256) M256 {
	return M256(fmaddPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmaddPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask_fmadd_ps'.
// Requires AVX512F.
func MaskFmaddPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask3_fmadd_ps'.
// Requires AVX512F.
func Mask3FmaddPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_fmaddsub_pd'.
// Requires FMA.
func FmaddsubPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmaddsubPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmaddsubPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmaddsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmaddsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmaddsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmaddsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmaddsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmaddsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_fmaddsub_ps'.
// Requires FMA.
func FmaddsubPs(a M256, b M256, c M256) M256 {
	return M256(fmaddsubPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmaddsubPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmaddsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmaddsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmaddsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmaddsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmaddsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmaddsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_fmsub_pd'.
// Requires FMA.
func FmsubPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmsubPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmsubPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask_fmsub_pd'.
// Requires AVX512F.
func MaskFmsubPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask3_fmsub_pd'.
// Requires AVX512F.
func Mask3FmsubPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_fmsub_ps'.
// Requires FMA.
func FmsubPs(a M256, b M256, c M256) M256 {
	return M256(fmsubPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmsubPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask_fmsub_ps'.
// Requires AVX512F.
func MaskFmsubPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask3_fmsub_ps'.
// Requires AVX512F.
func Mask3FmsubPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_fmsubadd_pd'.
// Requires FMA.
func FmsubaddPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmsubaddPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmsubaddPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmsubaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmsubaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmsubaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmsubaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmsubaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmsubaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_fmsubadd_ps'.
// Requires FMA.
func FmsubaddPs(a M256, b M256, c M256) M256 {
	return M256(fmsubaddPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmsubaddPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmsubaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmsubaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmsubaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmsubaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmsubaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmsubaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FnmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_fnmadd_pd'.
// Requires FMA.
func FnmaddPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fnmaddPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fnmaddPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask_fnmadd_pd'.
// Requires AVX512F.
func MaskFnmaddPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFnmaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFnmaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask3_fnmadd_pd'.
// Requires AVX512F.
func Mask3FnmaddPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FnmaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FnmaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFnmaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFnmaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FnmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			a[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_fnmadd_ps'.
// Requires FMA.
func FnmaddPs(a M256, b M256, c M256) M256 {
	return M256(fnmaddPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fnmaddPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask_fnmadd_ps'.
// Requires AVX512F.
func MaskFnmaddPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFnmaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFnmaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask3_fnmadd_ps'.
// Requires AVX512F.
func Mask3FnmaddPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FnmaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FnmaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFnmaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFnmaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FnmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_fnmsub_pd'.
// Requires FMA.
func FnmsubPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fnmsubPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fnmsubPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask_fnmsub_pd'.
// Requires AVX512F.
func MaskFnmsubPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFnmsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFnmsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask3_fnmsub_pd'.
// Requires AVX512F.
func Mask3FnmsubPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FnmsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FnmsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFnmsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFnmsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FnmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_fnmsub_ps'.
// Requires FMA.
func FnmsubPs(a M256, b M256, c M256) M256 {
	return M256(fnmsubPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fnmsubPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask_fnmsub_ps'.
// Requires AVX512F.
func MaskFnmsubPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFnmsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFnmsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask3_fnmsub_ps'.
// Requires AVX512F.
func Mask3FnmsubPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FnmsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FnmsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFnmsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFnmsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FpclassPdMask: Test packed double-precision (64-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm256_fpclass_pd_mask'.
// Requires AVX512DQ.
func FpclassPdMask(a M256d, imm8 int) Mmask8 {
	return Mmask8(fpclassPdMask([4]float64(a), imm8))
}

func fpclassPdMask(a [4]float64, imm8 int) uint8


// MaskFpclassPdMask: Test packed double-precision (64-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm256_mask_fpclass_pd_mask'.
// Requires AVX512DQ.
func MaskFpclassPdMask(k1 Mmask8, a M256d, imm8 int) Mmask8 {
	return Mmask8(maskFpclassPdMask(uint8(k1), [4]float64(a), imm8))
}

func maskFpclassPdMask(k1 uint8, a [4]float64, imm8 int) uint8


// FpclassPsMask: Test packed single-precision (32-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm256_fpclass_ps_mask'.
// Requires AVX512DQ.
func FpclassPsMask(a M256, imm8 int) Mmask8 {
	return Mmask8(fpclassPsMask([8]float32(a), imm8))
}

func fpclassPsMask(a [8]float32, imm8 int) uint8


// MaskFpclassPsMask: Test packed single-precision (32-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm256_mask_fpclass_ps_mask'.
// Requires AVX512DQ.
func MaskFpclassPsMask(k1 Mmask8, a M256, imm8 int) Mmask8 {
	return Mmask8(maskFpclassPsMask(uint8(k1), [8]float32(a), imm8))
}

func maskFpclassPsMask(k1 uint8, a [8]float32, imm8 int) uint8


// GetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_getexp_pd'.
// Requires AVX512F.
func GetexpPd(a M256d) M256d {
	return M256d(getexpPd([4]float64(a)))
}

func getexpPd(a [4]float64) [4]float64


// MaskGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_mask_getexp_pd'.
// Requires AVX512F.
func MaskGetexpPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskGetexpPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskGetexpPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd(k Mmask8, a M256d) M256d {
	return M256d(maskzGetexpPd(uint8(k), [4]float64(a)))
}

func maskzGetexpPd(k uint8, a [4]float64) [4]float64


// GetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_getexp_ps'.
// Requires AVX512F.
func GetexpPs(a M256) M256 {
	return M256(getexpPs([8]float32(a)))
}

func getexpPs(a [8]float32) [8]float32


// MaskGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_mask_getexp_ps'.
// Requires AVX512F.
func MaskGetexpPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskGetexpPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskGetexpPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs(k Mmask8, a M256) M256 {
	return M256(maskzGetexpPs(uint8(k), [8]float32(a)))
}

func maskzGetexpPs(k uint8, a [8]float32) [8]float32


// GetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '±(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_getmant_pd'.
// Requires AVX512F.
func GetmantPd(a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(getmantPd([4]float64(a), interv, sc))
}

func getmantPd(a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// MaskGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '±(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_mask_getmant_pd'.
// Requires AVX512F.
func MaskGetmantPd(src M256d, k Mmask8, a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(maskGetmantPd([4]float64(src), uint8(k), [4]float64(a), interv, sc))
}

func maskGetmantPd(src [4]float64, k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// MaskzGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '±(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd(k Mmask8, a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(maskzGetmantPd(uint8(k), [4]float64(a), interv, sc))
}

func maskzGetmantPd(k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// GetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '±(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_getmant_ps'.
// Requires AVX512F.
func GetmantPs(a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(getmantPs([8]float32(a), interv, sc))
}

func getmantPs(a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// MaskGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '±(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_mask_getmant_ps'.
// Requires AVX512F.
func MaskGetmantPs(src M256, k Mmask8, a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(maskGetmantPs([8]float32(src), uint8(k), [8]float32(a), interv, sc))
}

func maskGetmantPs(src [8]float32, k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// MaskzGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '±(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs(k Mmask8, a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(maskzGetmantPs(uint8(k), [8]float32(a), interv, sc))
}

func maskzGetmantPs(k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// Hadd16: Horizontally add adjacent pairs of 16-bit integers in 'a' and 'b',
// and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0] := a[31:16] + a[15:0]
//		dst[31:16] := a[63:48] + a[47:32]
//		dst[47:32] := a[95:80] + a[79:64]
//		dst[63:48] := a[127:112] + a[111:96]
//		dst[79:64] := b[31:16] + b[15:0]
//		dst[95:80] := b[63:48] + b[47:32]
//		dst[111:96] := b[95:80] + b[79:64]
//		dst[127:112] := b[127:112] + b[111:96]
//		dst[143:128] := a[159:144] + a[143:128]
//		dst[159:144] := a[191:176] + a[175:160]
//		dst[175:160] := a[223:208] + a[207:192]
//		dst[191:176] := a[255:240] + a[239:224]
//		dst[207:192] := b[127:112] + b[143:128]
//		dst[223:208] := b[159:144] + b[175:160]
//		dst[239:224] := b[191:176] + b[207:192]
//		dst[255:240] := b[223:208] + b[239:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHADDW'. Intrinsic: '_mm256_hadd_epi16'.
// Requires AVX2.
func Hadd16(a M256i, b M256i) M256i {
	return M256i(hadd16([32]byte(a), [32]byte(b)))
}

func hadd16(a [32]byte, b [32]byte) [32]byte


// Hadd32: Horizontally add adjacent pairs of 32-bit integers in 'a' and 'b',
// and pack the signed 32-bit results in 'dst'. 
//
//		dst[31:0] := a[63:32] + a[31:0]
//		dst[63:32] := a[127:96] + a[95:64]
//		dst[95:64] := b[63:32] + b[31:0]
//		dst[127:96] := b[127:96] + b[95:64]
//		dst[159:128] := a[191:160] + a[159:128]
//		dst[191:160] := a[255:224] + a[223:192]
//		dst[223:192] := b[191:160] + b[159:128]
//		dst[255:224] := b[255:224] + b[223:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHADDD'. Intrinsic: '_mm256_hadd_epi32'.
// Requires AVX2.
func Hadd32(a M256i, b M256i) M256i {
	return M256i(hadd32([32]byte(a), [32]byte(b)))
}

func hadd32(a [32]byte, b [32]byte) [32]byte


// HaddPd: Horizontally add adjacent pairs of double-precision (64-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[63:0] := a[127:64] + a[63:0]
//		dst[127:64] := b[127:64] + b[63:0]
//		dst[191:128] := a[255:192] + a[191:128]
//		dst[255:192] := b[255:192] + b[191:128]
//		dst[MAX:256] := 0
//
// Instruction: 'VHADDPD'. Intrinsic: '_mm256_hadd_pd'.
// Requires AVX.
func HaddPd(a M256d, b M256d) M256d {
	return M256d(haddPd([4]float64(a), [4]float64(b)))
}

func haddPd(a [4]float64, b [4]float64) [4]float64


// HaddPs: Horizontally add adjacent pairs of single-precision (32-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[31:0] := a[63:32] + a[31:0]
//		dst[63:32] := a[127:96] + a[95:64]
//		dst[95:64] := b[63:32] + b[31:0]
//		dst[127:96] := b[127:96] + b[95:64]
//		dst[159:128] := a[191:160] + a[159:128]
//		dst[191:160] := a[255:224] + a[223:192]
//		dst[223:192] := b[191:160] + b[159:128]
//		dst[255:224] := b[255:224] + b[223:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VHADDPS'. Intrinsic: '_mm256_hadd_ps'.
// Requires AVX.
func HaddPs(a M256, b M256) M256 {
	return M256(haddPs([8]float32(a), [8]float32(b)))
}

func haddPs(a [8]float32, b [8]float32) [8]float32


// Hadds16: Horizontally add adjacent pairs of 16-bit integers in 'a' and 'b'
// using saturation, and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0]= Saturate_To_Int16(a[31:16] + a[15:0])
//		dst[31:16] = Saturate_To_Int16(a[63:48] + a[47:32])
//		dst[47:32] = Saturate_To_Int16(a[95:80] + a[79:64])
//		dst[63:48] = Saturate_To_Int16(a[127:112] + a[111:96])
//		dst[79:64] = Saturate_To_Int16(b[31:16] + b[15:0])
//		dst[95:80] = Saturate_To_Int16(b[63:48] + b[47:32])
//		dst[111:96] = Saturate_To_Int16(b[95:80] + b[79:64])
//		dst[127:112] = Saturate_To_Int16(b[127:112] + b[111:96])
//		dst[143:128] = Saturate_To_Int16(a[159:144] + a[143:128])
//		dst[159:144] = Saturate_To_Int16(a[191:176] + a[175:160])
//		dst[175:160] = Saturate_To_Int16( a[223:208] + a[207:192])
//		dst[191:176] = Saturate_To_Int16(a[255:240] + a[239:224])
//		dst[207:192] = Saturate_To_Int16(b[127:112] + b[143:128])
//		dst[223:208] = Saturate_To_Int16(b[159:144] + b[175:160])
//		dst[239:224] = Saturate_To_Int16(b[191-160] + b[159-128])
//		dst[255:240] = Saturate_To_Int16(b[255:240] + b[239:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPHADDSW'. Intrinsic: '_mm256_hadds_epi16'.
// Requires AVX2.
func Hadds16(a M256i, b M256i) M256i {
	return M256i(hadds16([32]byte(a), [32]byte(b)))
}

func hadds16(a [32]byte, b [32]byte) [32]byte


// Hsub16: Horizontally subtract adjacent pairs of 16-bit integers in 'a' and
// 'b', and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0] := a[15:0] - a[31:16]
//		dst[31:16] := a[47:32] - a[63:48]
//		dst[47:32] := a[79:64] - a[95:80]
//		dst[63:48] := a[111:96] - a[127:112]
//		dst[79:64] := b[15:0] - b[31:16]
//		dst[95:80] := b[47:32] - b[63:48]
//		dst[111:96] := b[79:64] - b[95:80]
//		dst[127:112] := b[111:96] - b[127:112]
//		dst[143:128] := a[143:128] - a[159:144]
//		dst[159:144] := a[175:160] - a[191:176]
//		dst[175:160] := a[207:192] - a[223:208]
//		dst[191:176] := a[239:224] - a[255:240]
//		dst[207:192] := b[143:128] - b[159:144]
//		dst[223:208] := b[175:160] - b[191:176]
//		dst[239:224] := b[207:192] - b[223:208]
//		dst[255:240] := b[239:224] - b[255:240]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHSUBW'. Intrinsic: '_mm256_hsub_epi16'.
// Requires AVX2.
func Hsub16(a M256i, b M256i) M256i {
	return M256i(hsub16([32]byte(a), [32]byte(b)))
}

func hsub16(a [32]byte, b [32]byte) [32]byte


// Hsub32: Horizontally subtract adjacent pairs of 32-bit integers in 'a' and
// 'b', and pack the signed 32-bit results in 'dst'. 
//
//		dst[31:0] := a[31:0] - a[63:32]
//		dst[63:32] := a[95:64] - a[127:96]
//		dst[95:64] := b[31:0] - b[63:32]
//		dst[127:96] := b[95:64] - b[127:96]
//		dst[159:128] := a[159:128] - a[191:160]
//		dst[191:160] := a[223:192] - a[255:224]
//		dst[223:192] := b[159:128] - b[191:160]
//		dst[255:224] := b[223:192] - b[255:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHSUBD'. Intrinsic: '_mm256_hsub_epi32'.
// Requires AVX2.
func Hsub32(a M256i, b M256i) M256i {
	return M256i(hsub32([32]byte(a), [32]byte(b)))
}

func hsub32(a [32]byte, b [32]byte) [32]byte


// HsubPd: Horizontally subtract adjacent pairs of double-precision (64-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[63:0] := a[63:0] - a[127:64]
//		dst[127:64] := b[63:0] - b[127:64]
//		dst[191:128] := a[191:128] - a[255:192]
//		dst[255:192] := b[191:128] - b[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VHSUBPD'. Intrinsic: '_mm256_hsub_pd'.
// Requires AVX.
func HsubPd(a M256d, b M256d) M256d {
	return M256d(hsubPd([4]float64(a), [4]float64(b)))
}

func hsubPd(a [4]float64, b [4]float64) [4]float64


// HsubPs: Horizontally add adjacent pairs of single-precision (32-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[31:0] := a[31:0] - a[63:32]
//		dst[63:32] := a[95:64] - a[127:96]
//		dst[95:64] := b[31:0] - b[63:32]
//		dst[127:96] := b[95:64] - b[127:96]
//		dst[159:128] := a[159:128] - a[191:160]
//		dst[191:160] := a[223:192] - a[255:224]
//		dst[223:192] := b[159:128] - b[191:160]
//		dst[255:224] := b[223:192] - b[255:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VHSUBPS'. Intrinsic: '_mm256_hsub_ps'.
// Requires AVX.
func HsubPs(a M256, b M256) M256 {
	return M256(hsubPs([8]float32(a), [8]float32(b)))
}

func hsubPs(a [8]float32, b [8]float32) [8]float32


// Hsubs16: Horizontally subtract adjacent pairs of 16-bit integers in 'a' and
// 'b' using saturation, and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0]= Saturate_To_Int16(a[15:0] - a[31:16])
//		dst[31:16] = Saturate_To_Int16(a[47:32] - a[63:48])
//		dst[47:32] = Saturate_To_Int16(a[79:64] - a[95:80])
//		dst[63:48] = Saturate_To_Int16(a[111:96] - a[127:112])
//		dst[79:64] = Saturate_To_Int16(b[15:0] - b[31:16])
//		dst[95:80] = Saturate_To_Int16(b[47:32] - b[63:48])
//		dst[111:96] = Saturate_To_Int16(b[79:64] - b[95:80])
//		dst[127:112] = Saturate_To_Int16(b[111:96] - b[127:112])
//		dst[143:128]= Saturate_To_Int16(a[143:128] - a[159:144])
//		dst[159:144] = Saturate_To_Int16(a[175:160] - a[191:176])
//		dst[175:160] = Saturate_To_Int16(a[207:192] - a[223:208])
//		dst[191:176] = Saturate_To_Int16(a[239:224] - a[255:240])
//		dst[207:192] = Saturate_To_Int16(b[143:128] - b[159:144])
//		dst[223:208] = Saturate_To_Int16(b[175:160] - b[191:176])
//		dst[239:224] = Saturate_To_Int16(b[207:192] - b[223:208])
//		dst[255:240] = Saturate_To_Int16(b[239:224] - b[255:240])
//		dst[MAX:256] := 0
//
// Instruction: 'VPHSUBSW'. Intrinsic: '_mm256_hsubs_epi16'.
// Requires AVX2.
func Hsubs16(a M256i, b M256i) M256i {
	return M256i(hsubs16([32]byte(a), [32]byte(b)))
}

func hsubs16(a [32]byte, b [32]byte) [32]byte


// HypotPd: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_hypot_pd'.
// Requires AVX.
func HypotPd(a M256d, b M256d) M256d {
	return M256d(hypotPd([4]float64(a), [4]float64(b)))
}

func hypotPd(a [4]float64, b [4]float64) [4]float64


// HypotPs: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_hypot_ps'.
// Requires AVX.
func HypotPs(a M256, b M256) M256 {
	return M256(hypotPs([8]float32(a), [8]float32(b)))
}

func hypotPs(a [8]float32, b [8]float32) [8]float32


// I32gather32: Gather 32-bit integers from memory using 32-bit indices. 32-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm256_i32gather_epi32'.
// Requires AVX2.
func I32gather32(base_addr int, vindex M256i, scale int) M256i {
	return M256i(i32gather32(base_addr, [32]byte(vindex), scale))
}

func i32gather32(base_addr int, vindex [32]byte, scale int) [32]byte


// MaskI32gather32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm256_mask_i32gather_epi32'.
// Requires AVX2.
func MaskI32gather32(src M256i, base_addr int, vindex M256i, mask M256i, scale int) M256i {
	return M256i(maskI32gather32([32]byte(src), base_addr, [32]byte(vindex), [32]byte(mask), scale))
}

func maskI32gather32(src [32]byte, base_addr int, vindex [32]byte, mask [32]byte, scale int) [32]byte


// MmaskI32gather32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm256_mmask_i32gather_epi32'.
// Requires AVX512F.
func MmaskI32gather32(src M256i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI32gather32([32]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gather32(src [32]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [32]byte


// I32gather64: Gather 64-bit integers from memory using 32-bit indices. 64-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm256_i32gather_epi64'.
// Requires AVX2.
func I32gather64(base_addr int, vindex M128i, scale int) M256i {
	return M256i(i32gather64(base_addr, [16]byte(vindex), scale))
}

func i32gather64(base_addr int, vindex [16]byte, scale int) [32]byte


// MaskI32gather64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm256_mask_i32gather_epi64'.
// Requires AVX2.
func MaskI32gather64(src M256i, base_addr int, vindex M128i, mask M256i, scale int) M256i {
	return M256i(maskI32gather64([32]byte(src), base_addr, [16]byte(vindex), [32]byte(mask), scale))
}

func maskI32gather64(src [32]byte, base_addr int, vindex [16]byte, mask [32]byte, scale int) [32]byte


// MmaskI32gather64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm256_mmask_i32gather_epi64'.
// Requires AVX512F.
func MmaskI32gather64(src M256i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI32gather64([32]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gather64(src [32]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [32]byte


// I32gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm256_i32gather_pd'.
// Requires AVX2.
func I32gatherPd(base_addr float64, vindex M128i, scale int) M256d {
	return M256d(i32gatherPd(base_addr, [16]byte(vindex), scale))
}

func i32gatherPd(base_addr float64, vindex [16]byte, scale int) [4]float64


// MaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm256_mask_i32gather_pd'.
// Requires AVX2.
func MaskI32gatherPd(src M256d, base_addr float64, vindex M128i, mask M256d, scale int) M256d {
	return M256d(maskI32gatherPd([4]float64(src), base_addr, [16]byte(vindex), [4]float64(mask), scale))
}

func maskI32gatherPd(src [4]float64, base_addr float64, vindex [16]byte, mask [4]float64, scale int) [4]float64


// MmaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm256_mmask_i32gather_pd'.
// Requires AVX512F.
func MmaskI32gatherPd(src M256d, k Mmask8, vindex M128i, base_addr uintptr, scale int) M256d {
	return M256d(mmaskI32gatherPd([4]float64(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPd(src [4]float64, k uint8, vindex [16]byte, base_addr uintptr, scale int) [4]float64


// I32gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm256_i32gather_ps'.
// Requires AVX2.
func I32gatherPs(base_addr float32, vindex M256i, scale int) M256 {
	return M256(i32gatherPs(base_addr, [32]byte(vindex), scale))
}

func i32gatherPs(base_addr float32, vindex [32]byte, scale int) [8]float32


// MaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm256_mask_i32gather_ps'.
// Requires AVX2.
func MaskI32gatherPs(src M256, base_addr float32, vindex M256i, mask M256, scale int) M256 {
	return M256(maskI32gatherPs([8]float32(src), base_addr, [32]byte(vindex), [8]float32(mask), scale))
}

func maskI32gatherPs(src [8]float32, base_addr float32, vindex [32]byte, mask [8]float32, scale int) [8]float32


// MmaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm256_mmask_i32gather_ps'.
// Requires AVX512F.
func MmaskI32gatherPs(src M256, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256 {
	return M256(mmaskI32gatherPs([8]float32(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPs(src [8]float32, k uint8, vindex [32]byte, base_addr uintptr, scale int) [8]float32


// I32scatter32: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm256_i32scatter_epi32'.
// Requires AVX512F.
func I32scatter32(base_addr uintptr, vindex M256i, a M256i, scale int)  {
	i32scatter32(uintptr(base_addr), [32]byte(vindex), [32]byte(a), scale)
}

func i32scatter32(base_addr uintptr, vindex [32]byte, a [32]byte, scale int) 


// MaskI32scatter32: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm256_mask_i32scatter_epi32'.
// Requires AVX512F.
func MaskI32scatter32(base_addr uintptr, k Mmask8, vindex M256i, a M256i, scale int)  {
	maskI32scatter32(uintptr(base_addr), uint8(k), [32]byte(vindex), [32]byte(a), scale)
}

func maskI32scatter32(base_addr uintptr, k uint8, vindex [32]byte, a [32]byte, scale int) 


// I32scatter64: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm256_i32scatter_epi64'.
// Requires AVX512F.
func I32scatter64(base_addr uintptr, vindex M128i, a M256i, scale int)  {
	i32scatter64(uintptr(base_addr), [16]byte(vindex), [32]byte(a), scale)
}

func i32scatter64(base_addr uintptr, vindex [16]byte, a [32]byte, scale int) 


// MaskI32scatter64: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm256_mask_i32scatter_epi64'.
// Requires AVX512F.
func MaskI32scatter64(base_addr uintptr, k Mmask8, vindex M128i, a M256i, scale int)  {
	maskI32scatter64(uintptr(base_addr), uint8(k), [16]byte(vindex), [32]byte(a), scale)
}

func maskI32scatter64(base_addr uintptr, k uint8, vindex [16]byte, a [32]byte, scale int) 


// I32scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm256_i32scatter_pd'.
// Requires AVX512F.
func I32scatterPd(base_addr uintptr, vindex M128i, a M256d, scale int)  {
	i32scatterPd(uintptr(base_addr), [16]byte(vindex), [4]float64(a), scale)
}

func i32scatterPd(base_addr uintptr, vindex [16]byte, a [4]float64, scale int) 


// MaskI32scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm256_mask_i32scatter_pd'.
// Requires AVX512F.
func MaskI32scatterPd(base_addr uintptr, k Mmask8, vindex M128i, a M256d, scale int)  {
	maskI32scatterPd(uintptr(base_addr), uint8(k), [16]byte(vindex), [4]float64(a), scale)
}

func maskI32scatterPd(base_addr uintptr, k uint8, vindex [16]byte, a [4]float64, scale int) 


// I32scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm256_i32scatter_ps'.
// Requires AVX512F.
func I32scatterPs(base_addr uintptr, vindex M256i, a M256, scale int)  {
	i32scatterPs(uintptr(base_addr), [32]byte(vindex), [8]float32(a), scale)
}

func i32scatterPs(base_addr uintptr, vindex [32]byte, a [8]float32, scale int) 


// MaskI32scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm256_mask_i32scatter_ps'.
// Requires AVX512F.
func MaskI32scatterPs(base_addr uintptr, k Mmask8, vindex M256i, a M256, scale int)  {
	maskI32scatterPs(uintptr(base_addr), uint8(k), [32]byte(vindex), [8]float32(a), scale)
}

func maskI32scatterPs(base_addr uintptr, k uint8, vindex [32]byte, a [8]float32, scale int) 


// I64gather32: Gather 32-bit integers from memory using 64-bit indices. 32-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm256_i64gather_epi32'.
// Requires AVX2.
func I64gather32(base_addr int, vindex M256i, scale int) M128i {
	return M128i(i64gather32(base_addr, [32]byte(vindex), scale))
}

func i64gather32(base_addr int, vindex [32]byte, scale int) [16]byte


// MaskI64gather32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm256_mask_i64gather_epi32'.
// Requires AVX2.
func MaskI64gather32(src M128i, base_addr int, vindex M256i, mask M128i, scale int) M128i {
	return M128i(maskI64gather32([16]byte(src), base_addr, [32]byte(vindex), [16]byte(mask), scale))
}

func maskI64gather32(src [16]byte, base_addr int, vindex [32]byte, mask [16]byte, scale int) [16]byte


// MmaskI64gather32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm256_mmask_i64gather_epi32'.
// Requires AVX512F.
func MmaskI64gather32(src M128i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI64gather32([16]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gather32(src [16]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [16]byte


// I64gather64: Gather 64-bit integers from memory using 64-bit indices. 64-bit
// elements are loaded from addresses starting at 'base_addr' and offset by
// each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm256_i64gather_epi64'.
// Requires AVX2.
func I64gather64(base_addr int, vindex M256i, scale int) M256i {
	return M256i(i64gather64(base_addr, [32]byte(vindex), scale))
}

func i64gather64(base_addr int, vindex [32]byte, scale int) [32]byte


// MaskI64gather64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm256_mask_i64gather_epi64'.
// Requires AVX2.
func MaskI64gather64(src M256i, base_addr int, vindex M256i, mask M256i, scale int) M256i {
	return M256i(maskI64gather64([32]byte(src), base_addr, [32]byte(vindex), [32]byte(mask), scale))
}

func maskI64gather64(src [32]byte, base_addr int, vindex [32]byte, mask [32]byte, scale int) [32]byte


// MmaskI64gather64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm256_mmask_i64gather_epi64'.
// Requires AVX512F.
func MmaskI64gather64(src M256i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI64gather64([32]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gather64(src [32]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [32]byte


// I64gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm256_i64gather_pd'.
// Requires AVX2.
func I64gatherPd(base_addr float64, vindex M256i, scale int) M256d {
	return M256d(i64gatherPd(base_addr, [32]byte(vindex), scale))
}

func i64gatherPd(base_addr float64, vindex [32]byte, scale int) [4]float64


// MaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm256_mask_i64gather_pd'.
// Requires AVX2.
func MaskI64gatherPd(src M256d, base_addr float64, vindex M256i, mask M256d, scale int) M256d {
	return M256d(maskI64gatherPd([4]float64(src), base_addr, [32]byte(vindex), [4]float64(mask), scale))
}

func maskI64gatherPd(src [4]float64, base_addr float64, vindex [32]byte, mask [4]float64, scale int) [4]float64


// MmaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm256_mmask_i64gather_pd'.
// Requires AVX512F.
func MmaskI64gatherPd(src M256d, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256d {
	return M256d(mmaskI64gatherPd([4]float64(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPd(src [4]float64, k uint8, vindex [32]byte, base_addr uintptr, scale int) [4]float64


// I64gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm256_i64gather_ps'.
// Requires AVX2.
func I64gatherPs(base_addr float32, vindex M256i, scale int) M128 {
	return M128(i64gatherPs(base_addr, [32]byte(vindex), scale))
}

func i64gatherPs(base_addr float32, vindex [32]byte, scale int) [4]float32


// MaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm256_mask_i64gather_ps'.
// Requires AVX2.
func MaskI64gatherPs(src M128, base_addr float32, vindex M256i, mask M128, scale int) M128 {
	return M128(maskI64gatherPs([4]float32(src), base_addr, [32]byte(vindex), [4]float32(mask), scale))
}

func maskI64gatherPs(src [4]float32, base_addr float32, vindex [32]byte, mask [4]float32, scale int) [4]float32


// MmaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm256_mmask_i64gather_ps'.
// Requires AVX512F.
func MmaskI64gatherPs(src M128, k Mmask8, vindex M256i, base_addr uintptr, scale int) M128 {
	return M128(mmaskI64gatherPs([4]float32(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPs(src [4]float32, k uint8, vindex [32]byte, base_addr uintptr, scale int) [4]float32


// I64scatter32: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm256_i64scatter_epi32'.
// Requires AVX512F.
func I64scatter32(base_addr uintptr, vindex M256i, a M128i, scale int)  {
	i64scatter32(uintptr(base_addr), [32]byte(vindex), [16]byte(a), scale)
}

func i64scatter32(base_addr uintptr, vindex [32]byte, a [16]byte, scale int) 


// MaskI64scatter32: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm256_mask_i64scatter_epi32'.
// Requires AVX512F.
func MaskI64scatter32(base_addr uintptr, k Mmask8, vindex M256i, a M128i, scale int)  {
	maskI64scatter32(uintptr(base_addr), uint8(k), [32]byte(vindex), [16]byte(a), scale)
}

func maskI64scatter32(base_addr uintptr, k uint8, vindex [32]byte, a [16]byte, scale int) 


// I64scatter64: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm256_i64scatter_epi64'.
// Requires AVX512F.
func I64scatter64(base_addr uintptr, vindex M256i, a M256i, scale int)  {
	i64scatter64(uintptr(base_addr), [32]byte(vindex), [32]byte(a), scale)
}

func i64scatter64(base_addr uintptr, vindex [32]byte, a [32]byte, scale int) 


// MaskI64scatter64: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale') subject to mask 'k' (elements are not stored when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm256_mask_i64scatter_epi64'.
// Requires AVX512F.
func MaskI64scatter64(base_addr uintptr, k Mmask8, vindex M256i, a M256i, scale int)  {
	maskI64scatter64(uintptr(base_addr), uint8(k), [32]byte(vindex), [32]byte(a), scale)
}

func maskI64scatter64(base_addr uintptr, k uint8, vindex [32]byte, a [32]byte, scale int) 


// I64scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm256_i64scatter_pd'.
// Requires AVX512F.
func I64scatterPd(base_addr uintptr, vindex M256i, a M256d, scale int)  {
	i64scatterPd(uintptr(base_addr), [32]byte(vindex), [4]float64(a), scale)
}

func i64scatterPd(base_addr uintptr, vindex [32]byte, a [4]float64, scale int) 


// MaskI64scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm256_mask_i64scatter_pd'.
// Requires AVX512F.
func MaskI64scatterPd(base_addr uintptr, k Mmask8, vindex M256i, a M256d, scale int)  {
	maskI64scatterPd(uintptr(base_addr), uint8(k), [32]byte(vindex), [4]float64(a), scale)
}

func maskI64scatterPd(base_addr uintptr, k uint8, vindex [32]byte, a [4]float64, scale int) 


// I64scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm256_i64scatter_ps'.
// Requires AVX512F.
func I64scatterPs(base_addr uintptr, vindex M256i, a M128, scale int)  {
	i64scatterPs(uintptr(base_addr), [32]byte(vindex), [4]float32(a), scale)
}

func i64scatterPs(base_addr uintptr, vindex [32]byte, a [4]float32, scale int) 


// MaskI64scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm256_mask_i64scatter_ps'.
// Requires AVX512F.
func MaskI64scatterPs(base_addr uintptr, k Mmask8, vindex M256i, a M128, scale int)  {
	maskI64scatterPs(uintptr(base_addr), uint8(k), [32]byte(vindex), [4]float32(a), scale)
}

func maskI64scatterPs(base_addr uintptr, k uint8, vindex [32]byte, a [4]float32, scale int) 


// Idiv32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_idiv_epi32'.
// Requires AVX.
func Idiv32(a M256i, b M256i) M256i {
	return M256i(idiv32([32]byte(a), [32]byte(b)))
}

func idiv32(a [32]byte, b [32]byte) [32]byte


// Idivrem32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// store the truncated results in 'dst', and store the remainders as packed
// 32-bit integers into memory at 'mem_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_idivrem_epi32'.
// Requires AVX.
func Idivrem32(mem_addr M256i, a M256i, b M256i) M256i {
	return M256i(idivrem32([32]byte(mem_addr), [32]byte(a), [32]byte(b)))
}

func idivrem32(mem_addr [32]byte, a [32]byte, b [32]byte) [32]byte


// Insert16: Copy 'a' to 'dst', and insert the 16-bit integer 'i' into 'dst' at
// the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*16
//		dst[sel+15:sel] := i[15:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi16'.
// Requires AVX.
func Insert16(a M256i, i int16, index int) M256i {
	return M256i(insert16([32]byte(a), i, index))
}

func insert16(a [32]byte, i int16, index int) [32]byte


// Insert32: Copy 'a' to 'dst', and insert the 32-bit integer 'i' into 'dst' at
// the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*32
//		dst[sel+31:sel] := i[31:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi32'.
// Requires AVX.
func Insert32(a M256i, i int32, index int) M256i {
	return M256i(insert32([32]byte(a), i, index))
}

func insert32(a [32]byte, i int32, index int) [32]byte


// Insert64: Copy 'a' to 'dst', and insert the 64-bit integer 'i' into 'dst' at
// the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*64
//		dst[sel+63:sel] := i[63:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi64'.
// Requires AVX.
func Insert64(a M256i, i int64, index int) M256i {
	return M256i(insert64([32]byte(a), i, index))
}

func insert64(a [32]byte, i int64, index int) [32]byte


// Insert8: Copy 'a' to 'dst', and insert the 8-bit integer 'i' into 'dst' at
// the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*8
//		dst[sel+7:sel] := i[7:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi8'.
// Requires AVX.
func Insert8(a M256i, i int8, index int) M256i {
	return M256i(insert8([32]byte(a), i, index))
}

func insert8(a [32]byte, i int8, index int) [32]byte


// Insertf128Pd: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// double-precision (64-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_insertf128_pd'.
// Requires AVX.
func Insertf128Pd(a M256d, b M128d, imm8 int) M256d {
	return M256d(insertf128Pd([4]float64(a), [2]float64(b), imm8))
}

func insertf128Pd(a [4]float64, b [2]float64, imm8 int) [4]float64


// Insertf128Ps: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_insertf128_ps'.
// Requires AVX.
func Insertf128Ps(a M256, b M128, imm8 int) M256 {
	return M256(insertf128Ps([8]float32(a), [4]float32(b), imm8))
}

func insertf128Ps(a [8]float32, b [4]float32, imm8 int) [8]float32


// Insertf128Si256: Copy 'a' to 'dst', then insert 128 bits from 'b' into 'dst'
// at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_insertf128_si256'.
// Requires AVX.
func Insertf128Si256(a M256i, b M128i, imm8 int) M256i {
	return M256i(insertf128Si256([32]byte(a), [16]byte(b), imm8))
}

func insertf128Si256(a [32]byte, b [16]byte, imm8 int) [32]byte


// Insertf32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_insertf32x4'.
// Requires AVX512F.
func Insertf32x4(a M256, b M128, imm8 int) M256 {
	return M256(insertf32x4([8]float32(a), [4]float32(b), imm8))
}

func insertf32x4(a [8]float32, b [4]float32, imm8 int) [8]float32


// MaskInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_mask_insertf32x4'.
// Requires AVX512F.
func MaskInsertf32x4(src M256, k Mmask8, a M256, b M128, imm8 int) M256 {
	return M256(maskInsertf32x4([8]float32(src), uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func maskInsertf32x4(src [8]float32, k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// MaskzInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_maskz_insertf32x4'.
// Requires AVX512F.
func MaskzInsertf32x4(k Mmask8, a M256, b M128, imm8 int) M256 {
	return M256(maskzInsertf32x4(uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func maskzInsertf32x4(k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// Insertf64x2: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// double-precision (64-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm256_insertf64x2'.
// Requires AVX512DQ.
func Insertf64x2(a M256d, b M128d, imm8 int) M256d {
	return M256d(insertf64x2([4]float64(a), [2]float64(b), imm8))
}

func insertf64x2(a [4]float64, b [2]float64, imm8 int) [4]float64


// MaskInsertf64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm256_mask_insertf64x2'.
// Requires AVX512DQ.
func MaskInsertf64x2(src M256d, k Mmask8, a M256d, b M128d, imm8 int) M256d {
	return M256d(maskInsertf64x2([4]float64(src), uint8(k), [4]float64(a), [2]float64(b), imm8))
}

func maskInsertf64x2(src [4]float64, k uint8, a [4]float64, b [2]float64, imm8 int) [4]float64


// MaskzInsertf64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm256_maskz_insertf64x2'.
// Requires AVX512DQ.
func MaskzInsertf64x2(k Mmask8, a M256d, b M128d, imm8 int) M256d {
	return M256d(maskzInsertf64x2(uint8(k), [4]float64(a), [2]float64(b), imm8))
}

func maskzInsertf64x2(k uint8, a [4]float64, b [2]float64, imm8 int) [4]float64


// Inserti128Si256: Copy 'a' to 'dst', then insert 128 bits (composed of
// integer data) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI128'. Intrinsic: '_mm256_inserti128_si256'.
// Requires AVX2.
func Inserti128Si256(a M256i, b M128i, imm8 int) M256i {
	return M256i(inserti128Si256([32]byte(a), [16]byte(b), imm8))
}

func inserti128Si256(a [32]byte, b [16]byte, imm8 int) [32]byte


// Inserti32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// 32-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_inserti32x4'.
// Requires AVX512F.
func Inserti32x4(a M256i, b M128i, imm8 int) M256i {
	return M256i(inserti32x4([32]byte(a), [16]byte(b), imm8))
}

func inserti32x4(a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_mask_inserti32x4'.
// Requires AVX512F.
func MaskInserti32x4(src M256i, k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskInserti32x4([32]byte(src), uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskInserti32x4(src [32]byte, k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskzInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_maskz_inserti32x4'.
// Requires AVX512F.
func MaskzInserti32x4(k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskzInserti32x4(uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskzInserti32x4(k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// Inserti64x2: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// 64-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm256_inserti64x2'.
// Requires AVX512DQ.
func Inserti64x2(a M256i, b M128i, imm8 int) M256i {
	return M256i(inserti64x2([32]byte(a), [16]byte(b), imm8))
}

func inserti64x2(a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskInserti64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm256_mask_inserti64x2'.
// Requires AVX512DQ.
func MaskInserti64x2(src M256i, k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskInserti64x2([32]byte(src), uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskInserti64x2(src [32]byte, k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskzInserti64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm256_maskz_inserti64x2'.
// Requires AVX512DQ.
func MaskzInserti64x2(k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskzInserti64x2(uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskzInserti64x2(k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// InvcbrtPd: Compute the inverse cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := InvCubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invcbrt_pd'.
// Requires AVX.
func InvcbrtPd(a M256d) M256d {
	return M256d(invcbrtPd([4]float64(a)))
}

func invcbrtPd(a [4]float64) [4]float64


// InvcbrtPs: Compute the inverse cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := InvCubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invcbrt_ps'.
// Requires AVX.
func InvcbrtPs(a M256) M256 {
	return M256(invcbrtPs([8]float32(a)))
}

func invcbrtPs(a [8]float32) [8]float32


// InvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := InvSQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invsqrt_pd'.
// Requires AVX.
func InvsqrtPd(a M256d) M256d {
	return M256d(invsqrtPd([4]float64(a)))
}

func invsqrtPd(a [4]float64) [4]float64


// InvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := InvSQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invsqrt_ps'.
// Requires AVX.
func InvsqrtPs(a M256) M256 {
	return M256(invsqrtPs([8]float32(a)))
}

func invsqrtPs(a [8]float32) [8]float32


// Irem32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_irem_epi32'.
// Requires AVX.
func Irem32(a M256i, b M256i) M256i {
	return M256i(irem32([32]byte(a), [32]byte(b)))
}

func irem32(a [32]byte, b [32]byte) [32]byte


// LddquSi256: Load 256-bits of integer data from unaligned memory into 'dst'.
// This intrinsic may perform better than '_mm256_loadu_si256' when the data
// crosses a cache line boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VLDDQU'. Intrinsic: '_mm256_lddqu_si256'.
// Requires AVX.
func LddquSi256(mem_addr M256iConst) M256i {
	return M256i(lddquSi256(mem_addr))
}

func lddquSi256(mem_addr M256iConst) [32]byte


// MaskLoad32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_load_epi32'.
// Requires AVX512F.
func MaskLoad32(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoad32([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoad32(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoad32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_maskz_load_epi32'.
// Requires AVX512F.
func MaskzLoad32(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoad32(uint8(k), uintptr(mem_addr)))
}

func maskzLoad32(k uint8, mem_addr uintptr) [32]byte


// MaskLoad64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_load_epi64'.
// Requires AVX512F.
func MaskLoad64(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoad64([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoad64(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoad64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_maskz_load_epi64'.
// Requires AVX512F.
func MaskzLoad64(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoad64(uint8(k), uintptr(mem_addr)))
}

func maskzLoad64(k uint8, mem_addr uintptr) [32]byte


// LoadPd: Load 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_load_pd'.
// Requires AVX.
func LoadPd(mem_addr float64) M256d {
	return M256d(loadPd(mem_addr))
}

func loadPd(mem_addr float64) [4]float64


// MaskLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 32-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_load_pd'.
// Requires AVX512F.
func MaskLoadPd(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskLoadPd([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPd(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 32-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_maskz_load_pd'.
// Requires AVX512F.
func MaskzLoadPd(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzLoadPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPd(k uint8, mem_addr uintptr) [4]float64


// LoadPs: Load 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_load_ps'.
// Requires AVX.
func LoadPs(mem_addr float32) M256 {
	return M256(loadPs(mem_addr))
}

func loadPs(mem_addr float32) [8]float32


// MaskLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 32-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_load_ps'.
// Requires AVX512F.
func MaskLoadPs(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskLoadPs([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPs(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 32-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_maskz_load_ps'.
// Requires AVX512F.
func MaskzLoadPs(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzLoadPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPs(k uint8, mem_addr uintptr) [8]float32


// LoadSi256: Load 256-bits of integer data from memory into 'dst'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA'. Intrinsic: '_mm256_load_si256'.
// Requires AVX.
func LoadSi256(mem_addr M256iConst) M256i {
	return M256i(loadSi256(mem_addr))
}

func loadSi256(mem_addr M256iConst) [32]byte


// MaskLoadu16: Load packed 16-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_mask_loadu_epi16'.
// Requires AVX512BW.
func MaskLoadu16(src M256i, k Mmask16, mem_addr uintptr) M256i {
	return M256i(maskLoadu16([32]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskLoadu16(src [32]byte, k uint16, mem_addr uintptr) [32]byte


// MaskzLoadu16: Load packed 16-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_maskz_loadu_epi16'.
// Requires AVX512BW.
func MaskzLoadu16(k Mmask16, mem_addr uintptr) M256i {
	return M256i(maskzLoadu16(uint16(k), uintptr(mem_addr)))
}

func maskzLoadu16(k uint16, mem_addr uintptr) [32]byte


// MaskLoadu32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_mask_loadu_epi32'.
// Requires AVX512F.
func MaskLoadu32(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoadu32([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadu32(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoadu32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_maskz_loadu_epi32'.
// Requires AVX512F.
func MaskzLoadu32(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoadu32(uint8(k), uintptr(mem_addr)))
}

func maskzLoadu32(k uint8, mem_addr uintptr) [32]byte


// MaskLoadu64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_mask_loadu_epi64'.
// Requires AVX512F.
func MaskLoadu64(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoadu64([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadu64(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoadu64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_maskz_loadu_epi64'.
// Requires AVX512F.
func MaskzLoadu64(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoadu64(uint8(k), uintptr(mem_addr)))
}

func maskzLoadu64(k uint8, mem_addr uintptr) [32]byte


// MaskLoadu8: Load packed 8-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_mask_loadu_epi8'.
// Requires AVX512BW.
func MaskLoadu8(src M256i, k Mmask32, mem_addr uintptr) M256i {
	return M256i(maskLoadu8([32]byte(src), uint32(k), uintptr(mem_addr)))
}

func maskLoadu8(src [32]byte, k uint32, mem_addr uintptr) [32]byte


// MaskzLoadu8: Load packed 8-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_maskz_loadu_epi8'.
// Requires AVX512BW.
func MaskzLoadu8(k Mmask32, mem_addr uintptr) M256i {
	return M256i(maskzLoadu8(uint32(k), uintptr(mem_addr)))
}

func maskzLoadu8(k uint32, mem_addr uintptr) [32]byte


// LoaduPd: Load 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_loadu_pd'.
// Requires AVX.
func LoaduPd(mem_addr float64) M256d {
	return M256d(loaduPd(mem_addr))
}

func loaduPd(mem_addr float64) [4]float64


// MaskLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_mask_loadu_pd'.
// Requires AVX512F.
func MaskLoaduPd(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskLoaduPd([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPd(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_maskz_loadu_pd'.
// Requires AVX512F.
func MaskzLoaduPd(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzLoaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPd(k uint8, mem_addr uintptr) [4]float64


// LoaduPs: Load 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_loadu_ps'.
// Requires AVX.
func LoaduPs(mem_addr float32) M256 {
	return M256(loaduPs(mem_addr))
}

func loaduPs(mem_addr float32) [8]float32


// MaskLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_mask_loadu_ps'.
// Requires AVX512F.
func MaskLoaduPs(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskLoaduPs([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPs(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_maskz_loadu_ps'.
// Requires AVX512F.
func MaskzLoaduPs(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzLoaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPs(k uint8, mem_addr uintptr) [8]float32


// LoaduSi256: Load 256-bits of integer data from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU'. Intrinsic: '_mm256_loadu_si256'.
// Requires AVX.
func LoaduSi256(mem_addr M256iConst) M256i {
	return M256i(loaduSi256(mem_addr))
}

func loaduSi256(mem_addr M256iConst) [32]byte


// Loadu2M128: Load two 128-bit values (composed of 4 packed single-precision
// (32-bit) floating-point elements) from memory, and combine them into a
// 256-bit value in 'dst'.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[loaddr+127:loaddr]
//		dst[255:128] := MEM[hiaddr+127:hiaddr]
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_loadu2_m128'.
// Requires AVX.
func Loadu2M128(hiaddr float32, loaddr float32) M256 {
	return M256(loadu2M128(hiaddr, loaddr))
}

func loadu2M128(hiaddr float32, loaddr float32) [8]float32


// Loadu2M128d: Load two 128-bit values (composed of 2 packed double-precision
// (64-bit) floating-point elements) from memory, and combine them into a
// 256-bit value in 'dst'.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[loaddr+127:loaddr]
//		dst[255:128] := MEM[hiaddr+127:hiaddr]
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_loadu2_m128d'.
// Requires AVX.
func Loadu2M128d(hiaddr float64, loaddr float64) M256d {
	return M256d(loadu2M128d(hiaddr, loaddr))
}

func loadu2M128d(hiaddr float64, loaddr float64) [4]float64


// Loadu2M128i: Load two 128-bit values (composed of integer data) from memory,
// and combine them into a 256-bit value in 'dst'.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[loaddr+127:loaddr]
//		dst[255:128] := MEM[hiaddr+127:hiaddr]
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_loadu2_m128i'.
// Requires AVX.
func Loadu2M128i(hiaddr M128iConst, loaddr M128iConst) M256i {
	return M256i(loadu2M128i(hiaddr, loaddr))
}

func loadu2M128i(hiaddr M128iConst, loaddr M128iConst) [32]byte


// LogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ln(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log_pd'.
// Requires AVX.
func LogPd(a M256d) M256d {
	return M256d(logPd([4]float64(a)))
}

func logPd(a [4]float64) [4]float64


// LogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log_ps'.
// Requires AVX.
func LogPs(a M256) M256 {
	return M256(logPs([8]float32(a)))
}

func logPs(a [8]float32) [8]float32


// Log10Pd: Compute the base-10 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := log10(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log10_pd'.
// Requires AVX.
func Log10Pd(a M256d) M256d {
	return M256d(log10Pd([4]float64(a)))
}

func log10Pd(a [4]float64) [4]float64


// Log10Ps: Compute the base-10 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := log10(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log10_ps'.
// Requires AVX.
func Log10Ps(a M256) M256 {
	return M256(log10Ps([8]float32(a)))
}

func log10Ps(a [8]float32) [8]float32


// Log1pPd: Compute the natural logarithm of one plus packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ln(1.0 + a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log1p_pd'.
// Requires AVX.
func Log1pPd(a M256d) M256d {
	return M256d(log1pPd([4]float64(a)))
}

func log1pPd(a [4]float64) [4]float64


// Log1pPs: Compute the natural logarithm of one plus packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ln(1.0 + a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log1p_ps'.
// Requires AVX.
func Log1pPs(a M256) M256 {
	return M256(log1pPs([8]float32(a)))
}

func log1pPs(a [8]float32) [8]float32


// Log2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := log2(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log2_pd'.
// Requires AVX.
func Log2Pd(a M256d) M256d {
	return M256d(log2Pd([4]float64(a)))
}

func log2Pd(a [4]float64) [4]float64


// Log2Ps: Compute the base-2 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := log2(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log2_ps'.
// Requires AVX.
func Log2Ps(a M256) M256 {
	return M256(log2Ps([8]float32(a)))
}

func log2Ps(a [8]float32) [8]float32


// LogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_logb_pd'.
// Requires AVX.
func LogbPd(a M256d) M256d {
	return M256d(logbPd([4]float64(a)))
}

func logbPd(a [4]float64) [4]float64


// LogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_logb_ps'.
// Requires AVX.
func LogbPs(a M256) M256 {
	return M256(logbPs([8]float32(a)))
}

func logbPs(a [8]float32) [8]float32


// Lzcnt32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			tmp := 31
//			dst[i+31:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+31:i] := dst[i+31:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm256_lzcnt_epi32'.
// Requires AVX512CD.
func Lzcnt32(a M256i) M256i {
	return M256i(lzcnt32([32]byte(a)))
}

func lzcnt32(a [32]byte) [32]byte


// MaskLzcnt32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm256_mask_lzcnt_epi32'.
// Requires AVX512CD.
func MaskLzcnt32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskLzcnt32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskLzcnt32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzLzcnt32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm256_maskz_lzcnt_epi32'.
// Requires AVX512CD.
func MaskzLzcnt32(k Mmask8, a M256i) M256i {
	return M256i(maskzLzcnt32(uint8(k), [32]byte(a)))
}

func maskzLzcnt32(k uint8, a [32]byte) [32]byte


// Lzcnt64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp := 63
//			dst[i+63:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+63:i] := dst[i+63:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm256_lzcnt_epi64'.
// Requires AVX512CD.
func Lzcnt64(a M256i) M256i {
	return M256i(lzcnt64([32]byte(a)))
}

func lzcnt64(a [32]byte) [32]byte


// MaskLzcnt64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm256_mask_lzcnt_epi64'.
// Requires AVX512CD.
func MaskLzcnt64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskLzcnt64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskLzcnt64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzLzcnt64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm256_maskz_lzcnt_epi64'.
// Requires AVX512CD.
func MaskzLzcnt64(k Mmask8, a M256i) M256i {
	return M256i(maskzLzcnt64(uint8(k), [32]byte(a)))
}

func maskzLzcnt64(k uint8, a [32]byte) [32]byte


// Madd16: Multiply packed signed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm256_madd_epi16'.
// Requires AVX2.
func Madd16(a M256i, b M256i) M256i {
	return M256i(madd16([32]byte(a), [32]byte(b)))
}

func madd16(a [32]byte, b [32]byte) [32]byte


// MaskMadd16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm256_mask_madd_epi16'.
// Requires AVX512BW.
func MaskMadd16(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMadd16([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMadd16(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMadd16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm256_maskz_madd_epi16'.
// Requires AVX512BW.
func MaskzMadd16(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMadd16(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMadd16(k uint8, a [32]byte, b [32]byte) [32]byte


// Madd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm256_madd52hi_epu64'.
// Requires AVX512VL.
func Madd52hiEpu64(a M256i, b M256i, c M256i) M256i {
	return M256i(madd52hiEpu64([32]byte(a), [32]byte(b), [32]byte(c)))
}

func madd52hiEpu64(a [32]byte, b [32]byte, c [32]byte) [32]byte


// MaskMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm256_mask_madd52hi_epu64'.
// Requires AVX512VL.
func MaskMadd52hiEpu64(a M256i, k Mmask8, b M256i, c M256i) M256i {
	return M256i(maskMadd52hiEpu64([32]byte(a), uint8(k), [32]byte(b), [32]byte(c)))
}

func maskMadd52hiEpu64(a [32]byte, k uint8, b [32]byte, c [32]byte) [32]byte


// MaskzMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm256_maskz_madd52hi_epu64'.
// Requires AVX512VL.
func MaskzMadd52hiEpu64(k Mmask8, a M256i, b M256i, c M256i) M256i {
	return M256i(maskzMadd52hiEpu64(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c)))
}

func maskzMadd52hiEpu64(k uint8, a [32]byte, b [32]byte, c [32]byte) [32]byte


// Madd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm256_madd52lo_epu64'.
// Requires AVX512VL.
func Madd52loEpu64(a M256i, b M256i, c M256i) M256i {
	return M256i(madd52loEpu64([32]byte(a), [32]byte(b), [32]byte(c)))
}

func madd52loEpu64(a [32]byte, b [32]byte, c [32]byte) [32]byte


// MaskMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm256_mask_madd52lo_epu64'.
// Requires AVX512VL.
func MaskMadd52loEpu64(a M256i, k Mmask8, b M256i, c M256i) M256i {
	return M256i(maskMadd52loEpu64([32]byte(a), uint8(k), [32]byte(b), [32]byte(c)))
}

func maskMadd52loEpu64(a [32]byte, k uint8, b [32]byte, c [32]byte) [32]byte


// MaskzMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm256_maskz_madd52lo_epu64'.
// Requires AVX512VL.
func MaskzMadd52loEpu64(k Mmask8, a M256i, b M256i, c M256i) M256i {
	return M256i(maskzMadd52loEpu64(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c)))
}

func maskzMadd52loEpu64(k uint8, a [32]byte, b [32]byte, c [32]byte) [32]byte


// Maddubs16: Vertically multiply each unsigned 8-bit integer from 'a' with the
// corresponding signed 8-bit integer from 'b', producing intermediate signed
// 16-bit integers. Horizontally add adjacent pairs of intermediate signed
// 16-bit integers, and pack the saturated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm256_maddubs_epi16'.
// Requires AVX2.
func Maddubs16(a M256i, b M256i) M256i {
	return M256i(maddubs16([32]byte(a), [32]byte(b)))
}

func maddubs16(a [32]byte, b [32]byte) [32]byte


// MaskMaddubs16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm256_mask_maddubs_epi16'.
// Requires AVX512BW.
func MaskMaddubs16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMaddubs16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMaddubs16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMaddubs16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm256_maskz_maddubs_epi16'.
// Requires AVX512BW.
func MaskzMaddubs16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMaddubs16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMaddubs16(k uint16, a [32]byte, b [32]byte) [32]byte


// Maskload32: Load packed 32-bit integers from memory into 'dst' using 'mask'
// (elements are zeroed out when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMASKMOVD'. Intrinsic: '_mm256_maskload_epi32'.
// Requires AVX2.
func Maskload32(mem_addr int, mask M256i) M256i {
	return M256i(maskload32(mem_addr, [32]byte(mask)))
}

func maskload32(mem_addr int, mask [32]byte) [32]byte


// Maskload64: Load packed 64-bit integers from memory into 'dst' using 'mask'
// (elements are zeroed out when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMASKMOVQ'. Intrinsic: '_mm256_maskload_epi64'.
// Requires AVX2.
func Maskload64(mem_addr int, mask M256i) M256i {
	return M256i(maskload64(mem_addr, [32]byte(mask)))
}

func maskload64(mem_addr int, mask [32]byte) [32]byte


// MaskloadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using 'mask' (elements are zeroed out when the high
// bit of the corresponding element is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMASKMOVPD'. Intrinsic: '_mm256_maskload_pd'.
// Requires AVX.
func MaskloadPd(mem_addr float64, mask M256i) M256d {
	return M256d(maskloadPd(mem_addr, [32]byte(mask)))
}

func maskloadPd(mem_addr float64, mask [32]byte) [4]float64


// MaskloadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using 'mask' (elements are zeroed out when the high
// bit of the corresponding element is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMASKMOVPS'. Intrinsic: '_mm256_maskload_ps'.
// Requires AVX.
func MaskloadPs(mem_addr float32, mask M256i) M256 {
	return M256(maskloadPs(mem_addr, [32]byte(mask)))
}

func maskloadPs(mem_addr float32, mask [32]byte) [8]float32


// Maskstore32: Store packed 32-bit integers from 'a' into memory using 'mask'
// (elements are not stored when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VPMASKMOVD'. Intrinsic: '_mm256_maskstore_epi32'.
// Requires AVX2.
func Maskstore32(mem_addr int, mask M256i, a M256i)  {
	maskstore32(mem_addr, [32]byte(mask), [32]byte(a))
}

func maskstore32(mem_addr int, mask [32]byte, a [32]byte) 


// Maskstore64: Store packed 64-bit integers from 'a' into memory using 'mask'
// (elements are not stored when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VPMASKMOVQ'. Intrinsic: '_mm256_maskstore_epi64'.
// Requires AVX2.
func Maskstore64(mem_addr int64, mask M256i, a M256i)  {
	maskstore64(mem_addr, [32]byte(mask), [32]byte(a))
}

func maskstore64(mem_addr int64, mask [32]byte, a [32]byte) 


// MaskstorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using 'mask'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMASKMOVPD'. Intrinsic: '_mm256_maskstore_pd'.
// Requires AVX.
func MaskstorePd(mem_addr float64, mask M256i, a M256d)  {
	maskstorePd(mem_addr, [32]byte(mask), [4]float64(a))
}

func maskstorePd(mem_addr float64, mask [32]byte, a [4]float64) 


// MaskstorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using 'mask'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMASKMOVPS'. Intrinsic: '_mm256_maskstore_ps'.
// Requires AVX.
func MaskstorePs(mem_addr float32, mask M256i, a M256)  {
	maskstorePs(mem_addr, [32]byte(mask), [8]float32(a))
}

func maskstorePs(mem_addr float32, mask [32]byte, a [8]float32) 


// MaskMax16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm256_mask_max_epi16'.
// Requires AVX512BW.
func MaskMax16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMax16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMax16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMax16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm256_maskz_max_epi16'.
// Requires AVX512BW.
func MaskzMax16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMax16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMax16(k uint16, a [32]byte, b [32]byte) [32]byte


// Max16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm256_max_epi16'.
// Requires AVX2.
func Max16(a M256i, b M256i) M256i {
	return M256i(max16([32]byte(a), [32]byte(b)))
}

func max16(a [32]byte, b [32]byte) [32]byte


// MaskMax32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_mask_max_epi32'.
// Requires AVX512F.
func MaskMax32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMax32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMax32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMax32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMax32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMax32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMax32(k uint8, a [32]byte, b [32]byte) [32]byte


// Max32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_max_epi32'.
// Requires AVX2.
func Max32(a M256i, b M256i) M256i {
	return M256i(max32([32]byte(a), [32]byte(b)))
}

func max32(a [32]byte, b [32]byte) [32]byte


// MaskMax64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_mask_max_epi64'.
// Requires AVX512F.
func MaskMax64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMax64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMax64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMax64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMax64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMax64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMax64(k uint8, a [32]byte, b [32]byte) [32]byte


// Max64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_max_epi64'.
// Requires AVX512F.
func Max64(a M256i, b M256i) M256i {
	return M256i(max64([32]byte(a), [32]byte(b)))
}

func max64(a [32]byte, b [32]byte) [32]byte


// MaskMax8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm256_mask_max_epi8'.
// Requires AVX512BW.
func MaskMax8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMax8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMax8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMax8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm256_maskz_max_epi8'.
// Requires AVX512BW.
func MaskzMax8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMax8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMax8(k uint32, a [32]byte, b [32]byte) [32]byte


// Max8: Compare packed 8-bit integers in 'a' and 'b', and store packed maximum
// values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm256_max_epi8'.
// Requires AVX2.
func Max8(a M256i, b M256i) M256i {
	return M256i(max8([32]byte(a), [32]byte(b)))
}

func max8(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm256_mask_max_epu16'.
// Requires AVX512BW.
func MaskMaxEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm256_maskz_max_epu16'.
// Requires AVX512BW.
func MaskzMaxEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// MaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm256_max_epu16'.
// Requires AVX2.
func MaxEpu16(a M256i, b M256i) M256i {
	return M256i(maxEpu16([32]byte(a), [32]byte(b)))
}

func maxEpu16(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_mask_max_epu32'.
// Requires AVX512F.
func MaskMaxEpu32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_max_epu32'.
// Requires AVX2.
func MaxEpu32(a M256i, b M256i) M256i {
	return M256i(maxEpu32([32]byte(a), [32]byte(b)))
}

func maxEpu32(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_max_epu64'.
// Requires AVX512F.
func MaxEpu64(a M256i, b M256i) M256i {
	return M256i(maxEpu64([32]byte(a), [32]byte(b)))
}

func maxEpu64(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm256_mask_max_epu8'.
// Requires AVX512BW.
func MaskMaxEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm256_maskz_max_epu8'.
// Requires AVX512BW.
func MaskzMaxEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// MaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm256_max_epu8'.
// Requires AVX2.
func MaxEpu8(a M256i, b M256i) M256i {
	return M256i(maxEpu8([32]byte(a), [32]byte(b)))
}

func maxEpu8(a [32]byte, b [32]byte) [32]byte


// MaskMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMaxPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMaxPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMaxPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMaxPd(k uint8, a [4]float64, b [4]float64) [4]float64


// MaxPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_max_pd'.
// Requires AVX.
func MaxPd(a M256d, b M256d) M256d {
	return M256d(maxPd([4]float64(a), [4]float64(b)))
}

func maxPd(a [4]float64, b [4]float64) [4]float64


// MaskMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMaxPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMaxPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMaxPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMaxPs(k uint8, a [8]float32, b [8]float32) [8]float32


// MaxPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_max_ps'.
// Requires AVX.
func MaxPs(a M256, b M256) M256 {
	return M256(maxPs([8]float32(a), [8]float32(b)))
}

func maxPs(a [8]float32, b [8]float32) [8]float32


// MaskMin16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm256_mask_min_epi16'.
// Requires AVX512BW.
func MaskMin16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMin16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMin16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMin16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm256_maskz_min_epi16'.
// Requires AVX512BW.
func MaskzMin16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMin16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMin16(k uint16, a [32]byte, b [32]byte) [32]byte


// Min16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm256_min_epi16'.
// Requires AVX2.
func Min16(a M256i, b M256i) M256i {
	return M256i(min16([32]byte(a), [32]byte(b)))
}

func min16(a [32]byte, b [32]byte) [32]byte


// MaskMin32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_mask_min_epi32'.
// Requires AVX512F.
func MaskMin32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMin32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMin32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMin32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMin32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMin32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMin32(k uint8, a [32]byte, b [32]byte) [32]byte


// Min32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_min_epi32'.
// Requires AVX2.
func Min32(a M256i, b M256i) M256i {
	return M256i(min32([32]byte(a), [32]byte(b)))
}

func min32(a [32]byte, b [32]byte) [32]byte


// MaskMin64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_mask_min_epi64'.
// Requires AVX512F.
func MaskMin64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMin64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMin64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMin64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMin64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMin64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMin64(k uint8, a [32]byte, b [32]byte) [32]byte


// Min64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_min_epi64'.
// Requires AVX512F.
func Min64(a M256i, b M256i) M256i {
	return M256i(min64([32]byte(a), [32]byte(b)))
}

func min64(a [32]byte, b [32]byte) [32]byte


// MaskMin8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm256_mask_min_epi8'.
// Requires AVX512BW.
func MaskMin8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMin8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMin8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMin8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm256_maskz_min_epi8'.
// Requires AVX512BW.
func MaskzMin8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMin8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMin8(k uint32, a [32]byte, b [32]byte) [32]byte


// Min8: Compare packed 8-bit integers in 'a' and 'b', and store packed minimum
// values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm256_min_epi8'.
// Requires AVX2.
func Min8(a M256i, b M256i) M256i {
	return M256i(min8([32]byte(a), [32]byte(b)))
}

func min8(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm256_mask_min_epu16'.
// Requires AVX512BW.
func MaskMinEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMinEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm256_maskz_min_epu16'.
// Requires AVX512BW.
func MaskzMinEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// MinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm256_min_epu16'.
// Requires AVX2.
func MinEpu16(a M256i, b M256i) M256i {
	return M256i(minEpu16([32]byte(a), [32]byte(b)))
}

func minEpu16(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_mask_min_epu32'.
// Requires AVX512F.
func MaskMinEpu32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_min_epu32'.
// Requires AVX2.
func MinEpu32(a M256i, b M256i) M256i {
	return M256i(minEpu32([32]byte(a), [32]byte(b)))
}

func minEpu32(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpu64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu64(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_min_epu64'.
// Requires AVX512F.
func MinEpu64(a M256i, b M256i) M256i {
	return M256i(minEpu64([32]byte(a), [32]byte(b)))
}

func minEpu64(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm256_mask_min_epu8'.
// Requires AVX512BW.
func MaskMinEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMinEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm256_maskz_min_epu8'.
// Requires AVX512BW.
func MaskzMinEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// MinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm256_min_epu8'.
// Requires AVX2.
func MinEpu8(a M256i, b M256i) M256i {
	return M256i(minEpu8([32]byte(a), [32]byte(b)))
}

func minEpu8(a [32]byte, b [32]byte) [32]byte


// MaskMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMinPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMinPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMinPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMinPd(k uint8, a [4]float64, b [4]float64) [4]float64


// MinPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_min_pd'.
// Requires AVX.
func MinPd(a M256d, b M256d) M256d {
	return M256d(minPd([4]float64(a), [4]float64(b)))
}

func minPd(a [4]float64, b [4]float64) [4]float64


// MaskMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMinPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMinPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMinPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMinPs(k uint8, a [8]float32, b [8]float32) [8]float32


// MinPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_min_ps'.
// Requires AVX.
func MinPs(a M256, b M256) M256 {
	return M256(minPs([8]float32(a), [8]float32(b)))
}

func minPs(a [8]float32, b [8]float32) [8]float32


// MaskMov16: Move packed 16-bit integers from 'a' into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_mask_mov_epi16'.
// Requires AVX512BW.
func MaskMov16(src M256i, k Mmask16, a M256i) M256i {
	return M256i(maskMov16([32]byte(src), uint16(k), [32]byte(a)))
}

func maskMov16(src [32]byte, k uint16, a [32]byte) [32]byte


// MaskzMov16: Move packed 16-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_maskz_mov_epi16'.
// Requires AVX512BW.
func MaskzMov16(k Mmask16, a M256i) M256i {
	return M256i(maskzMov16(uint16(k), [32]byte(a)))
}

func maskzMov16(k uint16, a [32]byte) [32]byte


// MaskMov32: Move packed 32-bit integers from 'a' to 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_mov_epi32'.
// Requires AVX512F.
func MaskMov32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskMov32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskMov32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzMov32: Move packed 32-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMov32(k Mmask8, a M256i) M256i {
	return M256i(maskzMov32(uint8(k), [32]byte(a)))
}

func maskzMov32(k uint8, a [32]byte) [32]byte


// MaskMov64: Move packed 64-bit integers from 'a' to 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_mov_epi64'.
// Requires AVX512F.
func MaskMov64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskMov64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskMov64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzMov64: Move packed 64-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMov64(k Mmask8, a M256i) M256i {
	return M256i(maskzMov64(uint8(k), [32]byte(a)))
}

func maskzMov64(k uint8, a [32]byte) [32]byte


// MaskMov8: Move packed 8-bit integers from 'a' into 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_mask_mov_epi8'.
// Requires AVX512BW.
func MaskMov8(src M256i, k Mmask32, a M256i) M256i {
	return M256i(maskMov8([32]byte(src), uint32(k), [32]byte(a)))
}

func maskMov8(src [32]byte, k uint32, a [32]byte) [32]byte


// MaskzMov8: Move packed 8-bit integers from 'a' into 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_maskz_mov_epi8'.
// Requires AVX512BW.
func MaskzMov8(k Mmask32, a M256i) M256i {
	return M256i(maskzMov8(uint32(k), [32]byte(a)))
}

func maskzMov8(k uint32, a [32]byte) [32]byte


// MaskMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_mov_pd'.
// Requires AVX512F.
func MaskMovPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskMovPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskMovPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd(k Mmask8, a M256d) M256d {
	return M256d(maskzMovPd(uint8(k), [4]float64(a)))
}

func maskzMovPd(k uint8, a [4]float64) [4]float64


// MaskMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_mov_ps'.
// Requires AVX512F.
func MaskMovPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskMovPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMovPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs(k Mmask8, a M256) M256 {
	return M256(maskzMovPs(uint8(k), [8]float32(a)))
}

func maskzMovPs(k uint8, a [8]float32) [8]float32


// MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskMovedupPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskMovedupPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd(k Mmask8, a M256d) M256d {
	return M256d(maskzMovedupPd(uint8(k), [4]float64(a)))
}

func maskzMovedupPd(k uint8, a [4]float64) [4]float64


// MovedupPd: Duplicate even-indexed double-precision (64-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := a[63:0]
//		dst[191:128] := a[191:128]
//		dst[255:192] := a[191:128]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_movedup_pd'.
// Requires AVX.
func MovedupPd(a M256d) M256d {
	return M256d(movedupPd([4]float64(a)))
}

func movedupPd(a [4]float64) [4]float64


// MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskMovehdupPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMovehdupPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs(k Mmask8, a M256) M256 {
	return M256(maskzMovehdupPs(uint8(k), [8]float32(a)))
}

func maskzMovehdupPs(k uint8, a [8]float32) [8]float32


// MovehdupPs: Duplicate odd-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[63:32] 
//		dst[63:32] := a[63:32] 
//		dst[95:64] := a[127:96] 
//		dst[127:96] := a[127:96]
//		dst[159:128] := a[191:160] 
//		dst[191:160] := a[191:160] 
//		dst[223:192] := a[255:224] 
//		dst[255:224] := a[255:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_movehdup_ps'.
// Requires AVX.
func MovehdupPs(a M256) M256 {
	return M256(movehdupPs([8]float32(a)))
}

func movehdupPs(a [8]float32) [8]float32


// MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskMoveldupPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMoveldupPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs(k Mmask8, a M256) M256 {
	return M256(maskzMoveldupPs(uint8(k), [8]float32(a)))
}

func maskzMoveldupPs(k uint8, a [8]float32) [8]float32


// MoveldupPs: Duplicate even-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[31:0] 
//		dst[63:32] := a[31:0] 
//		dst[95:64] := a[95:64] 
//		dst[127:96] := a[95:64]
//		dst[159:128] := a[159:128] 
//		dst[191:160] := a[159:128] 
//		dst[223:192] := a[223:192] 
//		dst[255:224] := a[223:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_moveldup_ps'.
// Requires AVX.
func MoveldupPs(a M256) M256 {
	return M256(moveldupPs([8]float32(a)))
}

func moveldupPs(a [8]float32) [8]float32


// Movemask8: Create mask from the most significant bit of each 8-bit element
// in 'a', and store the result in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[j] := a[i+7]
//		ENDFOR
//
// Instruction: 'VPMOVMSKB'. Intrinsic: '_mm256_movemask_epi8'.
// Requires AVX2.
func Movemask8(a M256i) int {
	return int(movemask8([32]byte(a)))
}

func movemask8(a [32]byte) int


// MovemaskPd: Set each bit of mask 'dst' based on the most significant bit of
// the corresponding packed double-precision (64-bit) floating-point element in
// 'a'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63]
//				dst[j] := 1
//			ELSE
//				dst[j] := 0
//			FI
//		ENDFOR
//		dst[MAX:4] := 0
//
// Instruction: 'VMOVMSKPD'. Intrinsic: '_mm256_movemask_pd'.
// Requires AVX.
func MovemaskPd(a M256d) int {
	return int(movemaskPd([4]float64(a)))
}

func movemaskPd(a [4]float64) int


// MovemaskPs: Set each bit of mask 'dst' based on the most significant bit of
// the corresponding packed single-precision (32-bit) floating-point element in
// 'a'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31]
//				dst[j] := 1
//			ELSE
//				dst[j] := 0
//			FI
//		ENDFOR
//		dst[MAX:8] := 0
//
// Instruction: 'VMOVMSKPS'. Intrinsic: '_mm256_movemask_ps'.
// Requires AVX.
func MovemaskPs(a M256) int {
	return int(movemaskPs([8]float32(a)))
}

func movemaskPs(a [8]float32) int


// Movepi16Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 16-bit integer in 'a'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPMOVW2M'. Intrinsic: '_mm256_movepi16_mask'.
// Requires AVX512BW.
func Movepi16Mask(a M256i) Mmask16 {
	return Mmask16(movepi16Mask([32]byte(a)))
}

func movepi16Mask(a [32]byte) uint16


// Movepi32Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 32-bit integer in 'a'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPMOVD2M'. Intrinsic: '_mm256_movepi32_mask'.
// Requires AVX512DQ.
func Movepi32Mask(a M256i) Mmask8 {
	return Mmask8(movepi32Mask([32]byte(a)))
}

func movepi32Mask(a [32]byte) uint8


// Movepi64Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 64-bit integer in 'a'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPMOVQ2M'. Intrinsic: '_mm256_movepi64_mask'.
// Requires AVX512DQ.
func Movepi64Mask(a M256i) Mmask8 {
	return Mmask8(movepi64Mask([32]byte(a)))
}

func movepi64Mask(a [32]byte) uint8


// Movepi8Mask: Set each bit of mask register 'k' based on the most significant
// bit of the corresponding packed 8-bit integer in 'a'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPMOVB2M'. Intrinsic: '_mm256_movepi8_mask'.
// Requires AVX512BW.
func Movepi8Mask(a M256i) Mmask32 {
	return Mmask32(movepi8Mask([32]byte(a)))
}

func movepi8Mask(a [32]byte) uint32


// Movm16: Set each packed 16-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := 0xFFFF
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2W'. Intrinsic: '_mm256_movm_epi16'.
// Requires AVX512BW.
func Movm16(k Mmask16) M256i {
	return M256i(movm16(uint16(k)))
}

func movm16(k uint16) [32]byte


// Movm32: Set each packed 32-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 0xFFFFFFFF
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2D'. Intrinsic: '_mm256_movm_epi32'.
// Requires AVX512DQ.
func Movm32(k Mmask8) M256i {
	return M256i(movm32(uint8(k)))
}

func movm32(k uint8) [32]byte


// Movm64: Set each packed 64-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 0xFFFFFFFFffffffff
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2Q'. Intrinsic: '_mm256_movm_epi64'.
// Requires AVX512DQ.
func Movm64(k Mmask8) M256i {
	return M256i(movm64(uint8(k)))
}

func movm64(k uint8) [32]byte


// Movm8: Set each packed 8-bit integer in 'dst' to all ones or all zeros based
// on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := 0xFF
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2B'. Intrinsic: '_mm256_movm_epi8'.
// Requires AVX512BW.
func Movm8(k Mmask32) M256i {
	return M256i(movm8(uint32(k)))
}

func movm8(k uint32) [32]byte


// MpsadbwEpu8: Compute the sum of absolute differences (SADs) of quadruplets
// of unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst'.
// 	Eight SADs are performed for each 128-bit lane using one quadruplet from
// 'b' and eight quadruplets from 'a'. One quadruplet is selected from 'b'
// starting at on the offset specified in 'imm8'. Eight quadruplets are formed
// from sequential 8-bit integers selected from 'a' starting at the offset
// specified in 'imm8'. 
//
//		MPSADBW(a[127:0], b[127:0], imm8[2:0]) {
//			i := imm8[2]*32
//			b_offset := imm8[1:0]*32
//			FOR j := 0 to 7
//				i := j*8
//				k := a_offset+i
//				l := b_offset
//				tmp[i+15:i] := ABS(a[k+7:k] - b[l+7:l]) + ABS(a[k+15:k+8] - b[l+15:l+8]) + ABS(a[k+23:k+16] - b[l+23:l+16]) + ABS(a[k+31:k+24] - b[l+31:l+24])
//			ENDFOR
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := MPSADBW(a[127:0], b[127:0], imm8[2:0])
//		dst[255:128] := MPSADBW(a[255:128], b[255:128], imm8[5:3])
//		dst[MAX:256] := 0
//
// Instruction: 'VMPSADBW'. Intrinsic: '_mm256_mpsadbw_epu8'.
// Requires AVX2.
func MpsadbwEpu8(a M256i, b M256i, imm8 int) M256i {
	return M256i(mpsadbwEpu8([32]byte(a), [32]byte(b), imm8))
}

func mpsadbwEpu8(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskMul32: Multiply the low 32-bit integers from each packed 64-bit element
// in 'a' and 'b', and store the signed 64-bit results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_mask_mul_epi32'.
// Requires AVX512F.
func MaskMul32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMul32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMul32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMul32: Multiply the low 32-bit integers from each packed 64-bit element
// in 'a' and 'b', and store the signed 64-bit results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMul32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMul32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMul32(k uint8, a [32]byte, b [32]byte) [32]byte


// Mul32: Multiply the low 32-bit integers from each packed 64-bit element in
// 'a' and 'b', and store the signed 64-bit results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_mul_epi32'.
// Requires AVX2.
func Mul32(a M256i, b M256i) M256i {
	return M256i(mul32([32]byte(a), [32]byte(b)))
}

func mul32(a [32]byte, b [32]byte) [32]byte


// MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// MulEpu32: Multiply the low unsigned 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the unsigned 64-bit results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_mul_epu32'.
// Requires AVX2.
func MulEpu32(a M256i, b M256i) M256i {
	return M256i(mulEpu32([32]byte(a), [32]byte(b)))
}

func mulEpu32(a [32]byte, b [32]byte) [32]byte


// MaskMulPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_mask_mul_pd'.
// Requires AVX512F.
func MaskMulPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMulPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMulPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMulPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMulPd(k uint8, a [4]float64, b [4]float64) [4]float64


// MulPd: Multiply packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_mul_pd'.
// Requires AVX.
func MulPd(a M256d, b M256d) M256d {
	return M256d(mulPd([4]float64(a), [4]float64(b)))
}

func mulPd(a [4]float64, b [4]float64) [4]float64


// MaskMulPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).  RM. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_mask_mul_ps'.
// Requires AVX512F.
func MaskMulPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMulPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMulPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMulPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMulPs(k uint8, a [8]float32, b [8]float32) [8]float32


// MulPs: Multiply packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_mul_ps'.
// Requires AVX.
func MulPs(a M256, b M256) M256 {
	return M256(mulPs([8]float32(a), [8]float32(b)))
}

func mulPs(a [8]float32, b [8]float32) [8]float32


// MaskMulhi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm256_mask_mulhi_epi16'.
// Requires AVX512BW.
func MaskMulhi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMulhi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMulhi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMulhi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm256_maskz_mulhi_epi16'.
// Requires AVX512BW.
func MaskzMulhi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMulhi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMulhi16(k uint16, a [32]byte, b [32]byte) [32]byte


// Mulhi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm256_mulhi_epi16'.
// Requires AVX2.
func Mulhi16(a M256i, b M256i) M256i {
	return M256i(mulhi16([32]byte(a), [32]byte(b)))
}

func mulhi16(a [32]byte, b [32]byte) [32]byte


// MaskMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm256_mask_mulhi_epu16'.
// Requires AVX512BW.
func MaskMulhiEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMulhiEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMulhiEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and
// 'b', producing intermediate 32-bit integers, and store the high 16 bits of
// the intermediate integers in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := o
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm256_maskz_mulhi_epu16'.
// Requires AVX512BW.
func MaskzMulhiEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMulhiEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMulhiEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// MulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm256_mulhi_epu16'.
// Requires AVX2.
func MulhiEpu16(a M256i, b M256i) M256i {
	return M256i(mulhiEpu16([32]byte(a), [32]byte(b)))
}

func mulhiEpu16(a [32]byte, b [32]byte) [32]byte


// MaskMulhrs16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm256_mask_mulhrs_epi16'.
// Requires AVX512BW.
func MaskMulhrs16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMulhrs16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMulhrs16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMulhrs16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := 9
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm256_maskz_mulhrs_epi16'.
// Requires AVX512BW.
func MaskzMulhrs16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMulhrs16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMulhrs16(k uint16, a [32]byte, b [32]byte) [32]byte


// Mulhrs16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//			dst[i+15:i] := tmp[16:1]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm256_mulhrs_epi16'.
// Requires AVX2.
func Mulhrs16(a M256i, b M256i) M256i {
	return M256i(mulhrs16([32]byte(a), [32]byte(b)))
}

func mulhrs16(a [32]byte, b [32]byte) [32]byte


// MaskMullo16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm256_mask_mullo_epi16'.
// Requires AVX512BW.
func MaskMullo16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMullo16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMullo16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMullo16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm256_maskz_mullo_epi16'.
// Requires AVX512BW.
func MaskzMullo16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMullo16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMullo16(k uint16, a [32]byte, b [32]byte) [32]byte


// Mullo16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[15:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm256_mullo_epi16'.
// Requires AVX2.
func Mullo16(a M256i, b M256i) M256i {
	return M256i(mullo16([32]byte(a), [32]byte(b)))
}

func mullo16(a [32]byte, b [32]byte) [32]byte


// MaskMullo32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_mask_mullo_epi32'.
// Requires AVX512F.
func MaskMullo32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMullo32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMullo32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMullo32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMullo32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMullo32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMullo32(k uint8, a [32]byte, b [32]byte) [32]byte


// Mullo32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			tmp[63:0] := a[i+31:i] * b[i+31:i]
//			dst[i+31:i] := tmp[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_mullo_epi32'.
// Requires AVX2.
func Mullo32(a M256i, b M256i) M256i {
	return M256i(mullo32([32]byte(a), [32]byte(b)))
}

func mullo32(a [32]byte, b [32]byte) [32]byte


// MaskMullo64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm256_mask_mullo_epi64'.
// Requires AVX512DQ.
func MaskMullo64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMullo64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMullo64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMullo64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm256_maskz_mullo_epi64'.
// Requires AVX512DQ.
func MaskzMullo64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMullo64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMullo64(k uint8, a [32]byte, b [32]byte) [32]byte


// Mullo64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp[127:0] := a[i+63:i] * b[i+63:i]
//			dst[i+63:i] := tmp[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm256_mullo_epi64'.
// Requires AVX512DQ.
func Mullo64(a M256i, b M256i) M256i {
	return M256i(mullo64([32]byte(a), [32]byte(b)))
}

func mullo64(a [32]byte, b [32]byte) [32]byte


// MaskMultishift64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR i := 0 to 3
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm256_mask_multishift_epi64_epi8'.
// Requires AVX512VL.
func MaskMultishift64Epi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMultishift64Epi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMultishift64Epi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMultishift64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR i := 0 to 3
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := 0
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm256_maskz_multishift_epi64_epi8'.
// Requires AVX512VL.
func MaskzMultishift64Epi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMultishift64Epi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMultishift64Epi8(k uint32, a [32]byte, b [32]byte) [32]byte


// Multishift64Epi8: For each 64-bit element in 'b', select 8 unaligned bytes
// using a byte-granular shift control within the corresponding 64-bit element
// of 'a', and store the 8 assembled bytes to the corresponding 64-bit element
// of 'dst'. 
//
//		FOR i := 0 to 3
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				dst[q+j*8+7:q+j*8] := tmp8[7:0]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm256_multishift_epi64_epi8'.
// Requires AVX512VL.
func Multishift64Epi8(a M256i, b M256i) M256i {
	return M256i(multishift64Epi8([32]byte(a), [32]byte(b)))
}

func multishift64Epi8(a [32]byte, b [32]byte) [32]byte


// MaskOr32: Compute the bitwise OR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_mask_or_epi32'.
// Requires AVX512F.
func MaskOr32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskOr32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskOr32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOr32: Compute the bitwise OR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOr32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzOr32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzOr32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskOr64: Compute the bitwise OR of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_mask_or_epi64'.
// Requires AVX512F.
func MaskOr64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskOr64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskOr64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOr64: Compute the bitwise OR of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOr64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzOr64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzOr64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm256_mask_or_pd'.
// Requires AVX512DQ.
func MaskOrPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskOrPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskOrPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm256_maskz_or_pd'.
// Requires AVX512DQ.
func MaskzOrPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzOrPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzOrPd(k uint8, a [4]float64, b [4]float64) [4]float64


// OrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm256_or_pd'.
// Requires AVX.
func OrPd(a M256d, b M256d) M256d {
	return M256d(orPd([4]float64(a), [4]float64(b)))
}

func orPd(a [4]float64, b [4]float64) [4]float64


// MaskOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm256_mask_or_ps'.
// Requires AVX512DQ.
func MaskOrPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskOrPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskOrPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm256_maskz_or_ps'.
// Requires AVX512DQ.
func MaskzOrPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzOrPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzOrPs(k uint8, a [8]float32, b [8]float32) [8]float32


// OrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm256_or_ps'.
// Requires AVX.
func OrPs(a M256, b M256) M256 {
	return M256(orPs([8]float32(a), [8]float32(b)))
}

func orPs(a [8]float32, b [8]float32) [8]float32


// OrSi256: Compute the bitwise OR of 256 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := (a[255:0] OR b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPOR'. Intrinsic: '_mm256_or_si256'.
// Requires AVX2.
func OrSi256(a M256i, b M256i) M256i {
	return M256i(orSi256([32]byte(a), [32]byte(b)))
}

func orSi256(a [32]byte, b [32]byte) [32]byte


// MaskPacks16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm256_mask_packs_epi16'.
// Requires AVX512BW.
func MaskPacks16(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskPacks16([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskPacks16(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzPacks16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm256_maskz_packs_epi16'.
// Requires AVX512BW.
func MaskzPacks16(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzPacks16(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzPacks16(k uint32, a [32]byte, b [32]byte) [32]byte


// Packs16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm256_packs_epi16'.
// Requires AVX2.
func Packs16(a M256i, b M256i) M256i {
	return M256i(packs16([32]byte(a), [32]byte(b)))
}

func packs16(a [32]byte, b [32]byte) [32]byte


// MaskPacks32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm256_mask_packs_epi32'.
// Requires AVX512BW.
func MaskPacks32(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskPacks32([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskPacks32(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzPacks32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm256_maskz_packs_epi32'.
// Requires AVX512BW.
func MaskzPacks32(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzPacks32(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzPacks32(k uint16, a [32]byte, b [32]byte) [32]byte


// Packs32: Convert packed 32-bit integers from 'a' and 'b' to packed 16-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm256_packs_epi32'.
// Requires AVX2.
func Packs32(a M256i, b M256i) M256i {
	return M256i(packs32([32]byte(a), [32]byte(b)))
}

func packs32(a [32]byte, b [32]byte) [32]byte


// MaskPackus16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm256_mask_packus_epi16'.
// Requires AVX512BW.
func MaskPackus16(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskPackus16([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskPackus16(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzPackus16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm256_maskz_packus_epi16'.
// Requires AVX512BW.
func MaskzPackus16(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzPackus16(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzPackus16(k uint32, a [32]byte, b [32]byte) [32]byte


// Packus16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm256_packus_epi16'.
// Requires AVX2.
func Packus16(a M256i, b M256i) M256i {
	return M256i(packus16([32]byte(a), [32]byte(b)))
}

func packus16(a [32]byte, b [32]byte) [32]byte


// MaskPackus32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm256_mask_packus_epi32'.
// Requires AVX512BW.
func MaskPackus32(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskPackus32([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskPackus32(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzPackus32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm256_maskz_packus_epi32'.
// Requires AVX512BW.
func MaskzPackus32(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzPackus32(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzPackus32(k uint16, a [32]byte, b [32]byte) [32]byte


// Packus32: Convert packed 32-bit integers from 'a' and 'b' to packed 16-bit
// integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm256_packus_epi32'.
// Requires AVX2.
func Packus32(a M256i, b M256i) M256i {
	return M256i(packus32([32]byte(a), [32]byte(b)))
}

func packus32(a [32]byte, b [32]byte) [32]byte


// MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskPermutePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskPermutePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzPermutePd(uint8(k), [4]float64(a), imm8))
}

func maskzPermutePd(k uint8, a [4]float64, imm8 int) [4]float64


// PermutePd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		IF (imm8[0] == 0) dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) dst[255:192] := a[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_permute_pd'.
// Requires AVX.
func PermutePd(a M256d, imm8 int) M256d {
	return M256d(permutePd([4]float64(a), imm8))
}

func permutePd(a [4]float64, imm8 int) [4]float64


// MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskPermutePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskPermutePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzPermutePs(uint8(k), [8]float32(a), imm8))
}

func maskzPermutePs(k uint8, a [8]float32, imm8 int) [8]float32


// PermutePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_permute_ps'.
// Requires AVX.
func PermutePs(a M256, imm8 int) M256 {
	return M256(permutePs([8]float32(a), imm8))
}

func permutePs(a [8]float32, imm8 int) [8]float32


// Permute2f128Pd: Shuffle 128-bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2F128'. Intrinsic: '_mm256_permute2f128_pd'.
// Requires AVX.
func Permute2f128Pd(a M256d, b M256d, imm8 int) M256d {
	return M256d(permute2f128Pd([4]float64(a), [4]float64(b), imm8))
}

func permute2f128Pd(a [4]float64, b [4]float64, imm8 int) [4]float64


// Permute2f128Ps: Shuffle 128-bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2F128'. Intrinsic: '_mm256_permute2f128_ps'.
// Requires AVX.
func Permute2f128Ps(a M256, b M256, imm8 int) M256 {
	return M256(permute2f128Ps([8]float32(a), [8]float32(b), imm8))
}

func permute2f128Ps(a [8]float32, b [8]float32, imm8 int) [8]float32


// Permute2f128Si256: Shuffle 128-bits (composed of integer data) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2F128'. Intrinsic: '_mm256_permute2f128_si256'.
// Requires AVX.
func Permute2f128Si256(a M256i, b M256i, imm8 int) M256i {
	return M256i(permute2f128Si256([32]byte(a), [32]byte(b), imm8))
}

func permute2f128Si256(a [32]byte, b [32]byte, imm8 int) [32]byte


// Permute2x128Si256: Shuffle 128-bits (composed of integer data) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2I128'. Intrinsic: '_mm256_permute2x128_si256'.
// Requires AVX2.
func Permute2x128Si256(a M256i, b M256i, imm8 int) M256i {
	return M256i(permute2x128Si256([32]byte(a), [32]byte(b), imm8))
}

func permute2x128Si256(a [32]byte, b [32]byte, imm8 int) [32]byte


// Permute4x6464: Shuffle 64-bit integers in 'a' across lanes using the control
// in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permute4x64_epi64'.
// Requires AVX2.
func Permute4x6464(a M256i, imm8 int) M256i {
	return M256i(permute4x6464([32]byte(a), imm8))
}

func permute4x6464(a [32]byte, imm8 int) [32]byte


// Permute4x64Pd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permute4x64_pd'.
// Requires AVX2.
func Permute4x64Pd(a M256d, imm8 int) M256d {
	return M256d(permute4x64Pd([4]float64(a), imm8))
}

func permute4x64Pd(a [4]float64, imm8 int) [4]float64


// MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd(src M256d, k Mmask8, a M256d, b M256i) M256d {
	return M256d(maskPermutevarPd([4]float64(src), uint8(k), [4]float64(a), [32]byte(b)))
}

func maskPermutevarPd(src [4]float64, k uint8, a [4]float64, b [32]byte) [4]float64


// MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd(k Mmask8, a M256d, b M256i) M256d {
	return M256d(maskzPermutevarPd(uint8(k), [4]float64(a), [32]byte(b)))
}

func maskzPermutevarPd(k uint8, a [4]float64, b [32]byte) [4]float64


// PermutevarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		IF (b[1] == 0) dst[63:0] := a[63:0]
//		IF (b[1] == 1) dst[63:0] := a[127:64]
//		IF (b[65] == 0) dst[127:64] := a[63:0]
//		IF (b[65] == 1) dst[127:64] := a[127:64]
//		IF (b[129] == 0) dst[191:128] := a[191:128]
//		IF (b[129] == 1) dst[191:128] := a[255:192]
//		IF (b[193] == 0) dst[255:192] := a[191:128]
//		IF (b[193] == 1) dst[255:192] := a[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_permutevar_pd'.
// Requires AVX.
func PermutevarPd(a M256d, b M256i) M256d {
	return M256d(permutevarPd([4]float64(a), [32]byte(b)))
}

func permutevarPd(a [4]float64, b [32]byte) [4]float64


// MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs(src M256, k Mmask8, a M256, b M256i) M256 {
	return M256(maskPermutevarPs([8]float32(src), uint8(k), [8]float32(a), [32]byte(b)))
}

func maskPermutevarPs(src [8]float32, k uint8, a [8]float32, b [32]byte) [8]float32


// MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs(k Mmask8, a M256, b M256i) M256 {
	return M256(maskzPermutevarPs(uint8(k), [8]float32(a), [32]byte(b)))
}

func maskzPermutevarPs(k uint8, a [8]float32, b [32]byte) [8]float32


// PermutevarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], b[1:0])
//		dst[63:32] := SELECT4(a[127:0], b[33:32])
//		dst[95:64] := SELECT4(a[127:0], b[65:64])
//		dst[127:96] := SELECT4(a[127:0], b[97:96])
//		dst[159:128] := SELECT4(a[255:128], b[129:128])
//		dst[191:160] := SELECT4(a[255:128], b[161:160])
//		dst[223:192] := SELECT4(a[255:128], b[193:192])
//		dst[255:224] := SELECT4(a[255:128], b[225:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_permutevar_ps'.
// Requires AVX.
func PermutevarPs(a M256, b M256i) M256 {
	return M256(permutevarPs([8]float32(a), [32]byte(b)))
}

func permutevarPs(a [8]float32, b [32]byte) [8]float32


// Permutevar8x3232: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_permutevar8x32_epi32'.
// Requires AVX2.
func Permutevar8x3232(a M256i, idx M256i) M256i {
	return M256i(permutevar8x3232([32]byte(a), [32]byte(idx)))
}

func permutevar8x3232(a [32]byte, idx [32]byte) [32]byte


// Permutevar8x32Ps: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_permutevar8x32_ps'.
// Requires AVX2.
func Permutevar8x32Ps(a M256, idx M256i) M256 {
	return M256(permutevar8x32Ps([8]float32(a), [32]byte(idx)))
}

func permutevar8x32Ps(a [8]float32, idx [32]byte) [8]float32


// MaskPermutex64: Shuffle 64-bit integers in 'a' across lanes lanes using the
// control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutex_epi64'.
// Requires AVX512F.
func MaskPermutex64(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskPermutex64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskPermutex64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzPermutex64: Shuffle 64-bit integers in 'a' across lanes using the
// control in 'imm8', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutex_epi64'.
// Requires AVX512F.
func MaskzPermutex64(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzPermutex64(uint8(k), [32]byte(a), imm8))
}

func maskzPermutex64(k uint8, a [32]byte, imm8 int) [32]byte


// Permutex64: Shuffle 64-bit integers in 'a' across lanes using the control in
// 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutex_epi64'.
// Requires AVX512F.
func Permutex64(a M256i, imm8 int) M256i {
	return M256i(permutex64([32]byte(a), imm8))
}

func permutex64(a [32]byte, imm8 int) [32]byte


// MaskPermutexPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the control in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutex_pd'.
// Requires AVX512F.
func MaskPermutexPd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskPermutexPd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskPermutexPd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzPermutexPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutex_pd'.
// Requires AVX512F.
func MaskzPermutexPd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzPermutexPd(uint8(k), [4]float64(a), imm8))
}

func maskzPermutexPd(k uint8, a [4]float64, imm8 int) [4]float64


// PermutexPd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// across lanes using the control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutex_pd'.
// Requires AVX512F.
func PermutexPd(a M256d, imm8 int) M256d {
	return M256d(permutexPd([4]float64(a), imm8))
}

func permutexPd(a [4]float64, imm8 int) [4]float64


// MaskPermutex2var16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+3:i]
//				dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2W'. Intrinsic: '_mm256_mask_permutex2var_epi16'.
// Requires AVX512BW.
func MaskPermutex2var16(a M256i, k Mmask16, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2var16([32]byte(a), uint16(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2var16(a [32]byte, k uint16, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2var16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+3:i]
//				dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := idx[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2W'. Intrinsic: '_mm256_mask2_permutex2var_epi16'.
// Requires AVX512BW.
func Mask2Permutex2var16(a M256i, idx M256i, k Mmask16, b M256i) M256i {
	return M256i(mask2Permutex2var16([32]byte(a), [32]byte(idx), uint16(k), [32]byte(b)))
}

func mask2Permutex2var16(a [32]byte, idx [32]byte, k uint16, b [32]byte) [32]byte


// MaskzPermutex2var16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+3:i]
//				dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm256_maskz_permutex2var_epi16'.
// Requires AVX512BW.
func MaskzPermutex2var16(k Mmask16, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2var16(uint16(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2var16(k uint16, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2var16: Shuffle 16-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			off := 16*idx[i+3:i]
//			dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm256_permutex2var_epi16'.
// Requires AVX512BW.
func Permutex2var16(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2var16([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2var16(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2var32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm256_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2var32(a M256i, k Mmask8, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2var32([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2var32(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2var32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm256_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2var32(a M256i, idx M256i, k Mmask8, b M256i) M256i {
	return M256i(mask2Permutex2var32([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func mask2Permutex2var32(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// MaskzPermutex2var32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2var32(k Mmask8, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2var32(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2var32(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2var32: Shuffle 32-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2var32(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2var32([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2var32(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2var64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm256_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2var64(a M256i, k Mmask8, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2var64([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2var64(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2var64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm256_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2var64(a M256i, idx M256i, k Mmask8, b M256i) M256i {
	return M256i(mask2Permutex2var64([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func mask2Permutex2var64(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// MaskzPermutex2var64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2var64(k Mmask8, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2var64(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2var64(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2var64: Shuffle 64-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2var64(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2var64([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2var64(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2var8: Shuffle 8-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+4:i]
//				dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2B'. Intrinsic: '_mm256_mask_permutex2var_epi8'.
// Requires AVX512VL.
func MaskPermutex2var8(a M256i, k Mmask32, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2var8([32]byte(a), uint32(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2var8(a [32]byte, k uint32, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2var8: Shuffle 8-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+4:i]
//				dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm256_mask2_permutex2var_epi8'.
// Requires AVX512VL.
func Mask2Permutex2var8(a M256i, idx M256i, k Mmask32, b M256i) M256i {
	return M256i(mask2Permutex2var8([32]byte(a), [32]byte(idx), uint32(k), [32]byte(b)))
}

func mask2Permutex2var8(a [32]byte, idx [32]byte, k uint32, b [32]byte) [32]byte


// MaskzPermutex2var8: Shuffle 8-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+4:i]
//				dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2B, VPERMT2B'. Intrinsic: '_mm256_maskz_permutex2var_epi8'.
// Requires AVX512VL.
func MaskzPermutex2var8(k Mmask32, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2var8(uint32(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2var8(k uint32, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2var8: Shuffle 8-bit integers in 'a' and 'b' across lanes using the
// corresponding selector and index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			off := 8*idx[i+4:i]
//			dst[i+7:i] := idx[i+6] ? b[off+5:off] : a[off+7:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm256_permutex2var_epi8'.
// Requires AVX512VL.
func Permutex2var8(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2var8([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2var8(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm256_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd(a M256d, k Mmask8, idx M256i, b M256d) M256d {
	return M256d(maskPermutex2varPd([4]float64(a), uint8(k), [32]byte(idx), [4]float64(b)))
}

func maskPermutex2varPd(a [4]float64, k uint8, idx [32]byte, b [4]float64) [4]float64


// Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm256_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd(a M256d, idx M256i, k Mmask8, b M256d) M256d {
	return M256d(mask2Permutex2varPd([4]float64(a), [32]byte(idx), uint8(k), [4]float64(b)))
}

func mask2Permutex2varPd(a [4]float64, idx [32]byte, k uint8, b [4]float64) [4]float64


// MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd(k Mmask8, a M256d, idx M256i, b M256d) M256d {
	return M256d(maskzPermutex2varPd(uint8(k), [4]float64(a), [32]byte(idx), [4]float64(b)))
}

func maskzPermutex2varPd(k uint8, a [4]float64, idx [32]byte, b [4]float64) [4]float64


// Permutex2varPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd(a M256d, idx M256i, b M256d) M256d {
	return M256d(permutex2varPd([4]float64(a), [32]byte(idx), [4]float64(b)))
}

func permutex2varPd(a [4]float64, idx [32]byte, b [4]float64) [4]float64


// MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm256_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs(a M256, k Mmask8, idx M256i, b M256) M256 {
	return M256(maskPermutex2varPs([8]float32(a), uint8(k), [32]byte(idx), [8]float32(b)))
}

func maskPermutex2varPs(a [8]float32, k uint8, idx [32]byte, b [8]float32) [8]float32


// Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm256_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs(a M256, idx M256i, k Mmask8, b M256) M256 {
	return M256(mask2Permutex2varPs([8]float32(a), [32]byte(idx), uint8(k), [8]float32(b)))
}

func mask2Permutex2varPs(a [8]float32, idx [32]byte, k uint8, b [8]float32) [8]float32


// MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs(k Mmask8, a M256, idx M256i, b M256) M256 {
	return M256(maskzPermutex2varPs(uint8(k), [8]float32(a), [32]byte(idx), [8]float32(b)))
}

func maskzPermutex2varPs(k uint8, a [8]float32, idx [32]byte, b [8]float32) [8]float32


// Permutex2varPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs(a M256, idx M256i, b M256) M256 {
	return M256(permutex2varPs([8]float32(a), [32]byte(idx), [8]float32(b)))
}

func permutex2varPs(a [8]float32, idx [32]byte, b [8]float32) [8]float32


// MaskPermutexvar16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			id := idx[i+3:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm256_mask_permutexvar_epi16'.
// Requires AVX512BW.
func MaskPermutexvar16(src M256i, k Mmask16, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvar16([32]byte(src), uint16(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvar16(src [32]byte, k uint16, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvar16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			id := idx[i+3:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm256_maskz_permutexvar_epi16'.
// Requires AVX512BW.
func MaskzPermutexvar16(k Mmask16, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvar16(uint16(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvar16(k uint16, idx [32]byte, a [32]byte) [32]byte


// Permutexvar16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			id := idx[i+3:i]*16
//			dst[i+15:i] := a[id+15:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm256_permutexvar_epi16'.
// Requires AVX512BW.
func Permutexvar16(idx M256i, a M256i) M256i {
	return M256i(permutexvar16([32]byte(idx), [32]byte(a)))
}

func permutexvar16(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvar32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_mask_permutexvar_epi32'.
// Requires AVX512F.
func MaskPermutexvar32(src M256i, k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvar32([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvar32(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvar32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_maskz_permutexvar_epi32'.
// Requires AVX512F.
func MaskzPermutexvar32(k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvar32(uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvar32(k uint8, idx [32]byte, a [32]byte) [32]byte


// Permutexvar32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_permutexvar_epi32'.
// Requires AVX512F.
func Permutexvar32(idx M256i, a M256i) M256i {
	return M256i(permutexvar32([32]byte(idx), [32]byte(a)))
}

func permutexvar32(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvar64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutexvar_epi64'.
// Requires AVX512F.
func MaskPermutexvar64(src M256i, k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvar64([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvar64(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvar64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutexvar_epi64'.
// Requires AVX512F.
func MaskzPermutexvar64(k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvar64(uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvar64(k uint8, idx [32]byte, a [32]byte) [32]byte


// Permutexvar64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutexvar_epi64'.
// Requires AVX512F.
func Permutexvar64(idx M256i, a M256i) M256i {
	return M256i(permutexvar64([32]byte(idx), [32]byte(a)))
}

func permutexvar64(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvar8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			id := idx[i+4:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm256_mask_permutexvar_epi8'.
// Requires AVX512VL.
func MaskPermutexvar8(src M256i, k Mmask32, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvar8([32]byte(src), uint32(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvar8(src [32]byte, k uint32, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvar8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			id := idx[i+4:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm256_maskz_permutexvar_epi8'.
// Requires AVX512VL.
func MaskzPermutexvar8(k Mmask32, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvar8(uint32(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvar8(k uint32, idx [32]byte, a [32]byte) [32]byte


// Permutexvar8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			id := idx[i+4:i]*8
//			dst[i+7:i] := a[id+7:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm256_permutexvar_epi8'.
// Requires AVX512VL.
func Permutexvar8(idx M256i, a M256i) M256i {
	return M256i(permutexvar8([32]byte(idx), [32]byte(a)))
}

func permutexvar8(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutexvar_pd'.
// Requires AVX512F.
func MaskPermutexvarPd(src M256d, k Mmask8, idx M256i, a M256d) M256d {
	return M256d(maskPermutexvarPd([4]float64(src), uint8(k), [32]byte(idx), [4]float64(a)))
}

func maskPermutexvarPd(src [4]float64, k uint8, idx [32]byte, a [4]float64) [4]float64


// MaskzPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutexvar_pd'.
// Requires AVX512F.
func MaskzPermutexvarPd(k Mmask8, idx M256i, a M256d) M256d {
	return M256d(maskzPermutexvarPd(uint8(k), [32]byte(idx), [4]float64(a)))
}

func maskzPermutexvarPd(k uint8, idx [32]byte, a [4]float64) [4]float64


// PermutexvarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutexvar_pd'.
// Requires AVX512F.
func PermutexvarPd(idx M256i, a M256d) M256d {
	return M256d(permutexvarPd([32]byte(idx), [4]float64(a)))
}

func permutexvarPd(idx [32]byte, a [4]float64) [4]float64


// MaskPermutexvarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_mask_permutexvar_ps'.
// Requires AVX512F.
func MaskPermutexvarPs(src M256, k Mmask8, idx M256i, a M256) M256 {
	return M256(maskPermutexvarPs([8]float32(src), uint8(k), [32]byte(idx), [8]float32(a)))
}

func maskPermutexvarPs(src [8]float32, k uint8, idx [32]byte, a [8]float32) [8]float32


// MaskzPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_maskz_permutexvar_ps'.
// Requires AVX512F.
func MaskzPermutexvarPs(k Mmask8, idx M256i, a M256) M256 {
	return M256(maskzPermutexvarPs(uint8(k), [32]byte(idx), [8]float32(a)))
}

func maskzPermutexvarPs(k uint8, idx [32]byte, a [8]float32) [8]float32


// PermutexvarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_permutexvar_ps'.
// Requires AVX512F.
func PermutexvarPs(idx M256i, a M256) M256 {
	return M256(permutexvarPs([32]byte(idx), [8]float32(a)))
}

func permutexvarPs(idx [32]byte, a [8]float32) [8]float32


// PowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_pow_pd'.
// Requires AVX.
func PowPd(a M256d, b M256d) M256d {
	return M256d(powPd([4]float64(a), [4]float64(b)))
}

func powPd(a [4]float64, b [4]float64) [4]float64


// PowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_pow_ps'.
// Requires AVX.
func PowPs(a M256, b M256) M256 {
	return M256(powPs([8]float32(a), [8]float32(b)))
}

func powPs(a [8]float32, b [8]float32) [8]float32


// MaskRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm256_mask_range_pd'.
// Requires AVX512DQ.
func MaskRangePd(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskRangePd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskRangePd(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm256_maskz_range_pd'.
// Requires AVX512DQ.
func MaskzRangePd(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzRangePd(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzRangePd(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// RangePd: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm256_range_pd'.
// Requires AVX512DQ.
func RangePd(a M256d, b M256d, imm8 int) M256d {
	return M256d(rangePd([4]float64(a), [4]float64(b), imm8))
}

func rangePd(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm256_mask_range_ps'.
// Requires AVX512DQ.
func MaskRangePs(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskRangePs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskRangePs(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm256_maskz_range_ps'.
// Requires AVX512DQ.
func MaskzRangePs(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzRangePs(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzRangePs(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// RangePs: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm256_range_ps'.
// Requires AVX512DQ.
func RangePs(a M256, b M256, imm8 int) M256 {
	return M256(rangePs([8]float32(a), [8]float32(b), imm8))
}

func rangePs(a [8]float32, b [8]float32, imm8 int) [8]float32


// RcpPs: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 1.5*2^-12. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCPPS'. Intrinsic: '_mm256_rcp_ps'.
// Requires AVX.
func RcpPs(a M256) M256 {
	return M256(rcpPs([8]float32(a)))
}

func rcpPs(a [8]float32) [8]float32


// MaskRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskRcp14Pd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskRcp14Pd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd(k Mmask8, a M256d) M256d {
	return M256d(maskzRcp14Pd(uint8(k), [4]float64(a)))
}

func maskzRcp14Pd(k uint8, a [4]float64) [4]float64


// Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd(a M256d) M256d {
	return M256d(rcp14Pd([4]float64(a)))
}

func rcp14Pd(a [4]float64) [4]float64


// MaskRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps(src M256, k Mmask8, a M256) M256 {
	return M256(maskRcp14Ps([8]float32(src), uint8(k), [8]float32(a)))
}

func maskRcp14Ps(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps(k Mmask8, a M256) M256 {
	return M256(maskzRcp14Ps(uint8(k), [8]float32(a)))
}

func maskzRcp14Ps(k uint8, a [8]float32) [8]float32


// Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps(a M256) M256 {
	return M256(rcp14Ps([8]float32(a)))
}

func rcp14Ps(a [8]float32) [8]float32


// MaskReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm256_mask_reduce_pd'.
// Requires AVX512DQ.
func MaskReducePd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskReducePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskReducePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm256_maskz_reduce_pd'.
// Requires AVX512DQ.
func MaskzReducePd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzReducePd(uint8(k), [4]float64(a), imm8))
}

func maskzReducePd(k uint8, a [4]float64, imm8 int) [4]float64


// ReducePd: Extract the reduced argument of packed double-precision (64-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm256_reduce_pd'.
// Requires AVX512DQ.
func ReducePd(a M256d, imm8 int) M256d {
	return M256d(reducePd([4]float64(a), imm8))
}

func reducePd(a [4]float64, imm8 int) [4]float64


// MaskReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm256_mask_reduce_ps'.
// Requires AVX512DQ.
func MaskReducePs(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskReducePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskReducePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm256_maskz_reduce_ps'.
// Requires AVX512DQ.
func MaskzReducePs(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzReducePs(uint8(k), [8]float32(a), imm8))
}

func maskzReducePs(k uint8, a [8]float32, imm8 int) [8]float32


// ReducePs: Extract the reduced argument of packed single-precision (32-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm256_reduce_ps'.
// Requires AVX512DQ.
func ReducePs(a M256, imm8 int) M256 {
	return M256(reducePs([8]float32(a), imm8))
}

func reducePs(a [8]float32, imm8 int) [8]float32


// Rem16: Divide packed 16-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi16'.
// Requires AVX.
func Rem16(a M256i, b M256i) M256i {
	return M256i(rem16([32]byte(a), [32]byte(b)))
}

func rem16(a [32]byte, b [32]byte) [32]byte


// Rem32: Divide packed 32-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi32'.
// Requires AVX.
func Rem32(a M256i, b M256i) M256i {
	return M256i(rem32([32]byte(a), [32]byte(b)))
}

func rem32(a [32]byte, b [32]byte) [32]byte


// Rem64: Divide packed 64-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi64'.
// Requires AVX.
func Rem64(a M256i, b M256i) M256i {
	return M256i(rem64([32]byte(a), [32]byte(b)))
}

func rem64(a [32]byte, b [32]byte) [32]byte


// Rem8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi8'.
// Requires AVX.
func Rem8(a M256i, b M256i) M256i {
	return M256i(rem8([32]byte(a), [32]byte(b)))
}

func rem8(a [32]byte, b [32]byte) [32]byte


// RemEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu16'.
// Requires AVX.
func RemEpu16(a M256i, b M256i) M256i {
	return M256i(remEpu16([32]byte(a), [32]byte(b)))
}

func remEpu16(a [32]byte, b [32]byte) [32]byte


// RemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu32'.
// Requires AVX.
func RemEpu32(a M256i, b M256i) M256i {
	return M256i(remEpu32([32]byte(a), [32]byte(b)))
}

func remEpu32(a [32]byte, b [32]byte) [32]byte


// RemEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu64'.
// Requires AVX.
func RemEpu64(a M256i, b M256i) M256i {
	return M256i(remEpu64([32]byte(a), [32]byte(b)))
}

func remEpu64(a [32]byte, b [32]byte) [32]byte


// RemEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed unsigned 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu8'.
// Requires AVX.
func RemEpu8(a M256i, b M256i) M256i {
	return M256i(remEpu8([32]byte(a), [32]byte(b)))
}

func remEpu8(a [32]byte, b [32]byte) [32]byte


// MaskRol32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_mask_rol_epi32'.
// Requires AVX512F.
func MaskRol32(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRol32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRol32(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRol32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRol32(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRol32(uint8(k), [32]byte(a), imm8))
}

func maskzRol32(k uint8, a [32]byte, imm8 int) [32]byte


// Rol32: Rotate the bits in each packed 32-bit integer in 'a' to the left by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_rol_epi32'.
// Requires AVX512F.
func Rol32(a M256i, imm8 int) M256i {
	return M256i(rol32([32]byte(a), imm8))
}

func rol32(a [32]byte, imm8 int) [32]byte


// MaskRol64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_mask_rol_epi64'.
// Requires AVX512F.
func MaskRol64(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRol64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRol64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRol64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRol64(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRol64(uint8(k), [32]byte(a), imm8))
}

func maskzRol64(k uint8, a [32]byte, imm8 int) [32]byte


// Rol64: Rotate the bits in each packed 64-bit integer in 'a' to the left by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_rol_epi64'.
// Requires AVX512F.
func Rol64(a M256i, imm8 int) M256i {
	return M256i(rol64([32]byte(a), imm8))
}

func rol64(a [32]byte, imm8 int) [32]byte


// MaskRolv32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolv32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRolv32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRolv32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRolv32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolv32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRolv32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRolv32(k uint8, a [32]byte, b [32]byte) [32]byte


// Rolv32: Rotate the bits in each packed 32-bit integer in 'a' to the left by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_rolv_epi32'.
// Requires AVX512F.
func Rolv32(a M256i, b M256i) M256i {
	return M256i(rolv32([32]byte(a), [32]byte(b)))
}

func rolv32(a [32]byte, b [32]byte) [32]byte


// MaskRolv64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolv64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRolv64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRolv64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRolv64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolv64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRolv64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRolv64(k uint8, a [32]byte, b [32]byte) [32]byte


// Rolv64: Rotate the bits in each packed 64-bit integer in 'a' to the left by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_rolv_epi64'.
// Requires AVX512F.
func Rolv64(a M256i, b M256i) M256i {
	return M256i(rolv64([32]byte(a), [32]byte(b)))
}

func rolv64(a [32]byte, b [32]byte) [32]byte


// MaskRor32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_mask_ror_epi32'.
// Requires AVX512F.
func MaskRor32(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRor32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRor32(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRor32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRor32(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRor32(uint8(k), [32]byte(a), imm8))
}

func maskzRor32(k uint8, a [32]byte, imm8 int) [32]byte


// Ror32: Rotate the bits in each packed 32-bit integer in 'a' to the right by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_ror_epi32'.
// Requires AVX512F.
func Ror32(a M256i, imm8 int) M256i {
	return M256i(ror32([32]byte(a), imm8))
}

func ror32(a [32]byte, imm8 int) [32]byte


// MaskRor64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_mask_ror_epi64'.
// Requires AVX512F.
func MaskRor64(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRor64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRor64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRor64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRor64(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRor64(uint8(k), [32]byte(a), imm8))
}

func maskzRor64(k uint8, a [32]byte, imm8 int) [32]byte


// Ror64: Rotate the bits in each packed 64-bit integer in 'a' to the right by
// the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_ror_epi64'.
// Requires AVX512F.
func Ror64(a M256i, imm8 int) M256i {
	return M256i(ror64([32]byte(a), imm8))
}

func ror64(a [32]byte, imm8 int) [32]byte


// MaskRorv32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorv32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRorv32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRorv32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRorv32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorv32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRorv32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRorv32(k uint8, a [32]byte, b [32]byte) [32]byte


// Rorv32: Rotate the bits in each packed 32-bit integer in 'a' to the right by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_rorv_epi32'.
// Requires AVX512F.
func Rorv32(a M256i, b M256i) M256i {
	return M256i(rorv32([32]byte(a), [32]byte(b)))
}

func rorv32(a [32]byte, b [32]byte) [32]byte


// MaskRorv64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorv64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRorv64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRorv64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRorv64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorv64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRorv64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRorv64(k uint8, a [32]byte, b [32]byte) [32]byte


// Rorv64: Rotate the bits in each packed 64-bit integer in 'a' to the right by
// the number of bits specified in the corresponding element of 'b', and store
// the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_rorv_epi64'.
// Requires AVX512F.
func Rorv64(a M256i, b M256i) M256i {
	return M256i(rorv64([32]byte(a), [32]byte(b)))
}

func rorv64(a [32]byte, b [32]byte) [32]byte


// RoundPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' using the 'rounding' parameter, and store the results as packed
// double-precision floating-point elements in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPD'. Intrinsic: '_mm256_round_pd'.
// Requires AVX.
func RoundPd(a M256d, rounding int) M256d {
	return M256d(roundPd([4]float64(a), rounding))
}

func roundPd(a [4]float64, rounding int) [4]float64


// RoundPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' using the 'rounding' parameter, and store the results as packed
// single-precision floating-point elements in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm256_round_ps'.
// Requires AVX.
func RoundPs(a M256, rounding int) M256 {
	return M256(roundPs([8]float32(a), rounding))
}

func roundPs(a [8]float32, rounding int) [8]float32


// MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskRoundscalePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskRoundscalePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzRoundscalePd(uint8(k), [4]float64(a), imm8))
}

func maskzRoundscalePd(k uint8, a [4]float64, imm8 int) [4]float64


// RoundscalePd: Round packed double-precision (64-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd(a M256d, imm8 int) M256d {
	return M256d(roundscalePd([4]float64(a), imm8))
}

func roundscalePd(a [4]float64, imm8 int) [4]float64


// MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskRoundscalePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskRoundscalePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzRoundscalePs(uint8(k), [8]float32(a), imm8))
}

func maskzRoundscalePs(k uint8, a [8]float32, imm8 int) [8]float32


// RoundscalePs: Round packed single-precision (32-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs(a M256, imm8 int) M256 {
	return M256(roundscalePs([8]float32(a), imm8))
}

func roundscalePs(a [8]float32, imm8 int) [8]float32


// RsqrtPs: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 1.5*2^-12. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRTPS'. Intrinsic: '_mm256_rsqrt_ps'.
// Requires AVX.
func RsqrtPs(a M256) M256 {
	return M256(rsqrtPs([8]float32(a)))
}

func rsqrtPs(a [8]float32) [8]float32


// MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskRsqrt14Pd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskRsqrt14Pd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd(k Mmask8, a M256d) M256d {
	return M256d(maskzRsqrt14Pd(uint8(k), [4]float64(a)))
}

func maskzRsqrt14Pd(k uint8, a [4]float64) [4]float64


// MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps(src M256, k Mmask8, a M256) M256 {
	return M256(maskRsqrt14Ps([8]float32(src), uint8(k), [8]float32(a)))
}

func maskRsqrt14Ps(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps(k Mmask8, a M256) M256 {
	return M256(maskzRsqrt14Ps(uint8(k), [8]float32(a)))
}

func maskzRsqrt14Ps(k uint8, a [8]float32) [8]float32


// SadEpu8: Compute the absolute differences of packed unsigned 8-bit integers
// in 'a' and 'b', then horizontally sum each consecutive 8 differences to
// produce four unsigned 16-bit integers, and pack these unsigned 16-bit
// integers in the low 16 bits of 64-bit elements in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
//		ENDFOR
//		FOR j := 0 to 4
//			i := j*64
//			dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] + tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
//			dst[i+63:i+16] := 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSADBW'. Intrinsic: '_mm256_sad_epu8'.
// Requires AVX2.
func SadEpu8(a M256i, b M256i) M256i {
	return M256i(sadEpu8([32]byte(a), [32]byte(b)))
}

func sadEpu8(a [32]byte, b [32]byte) [32]byte


// MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskScalefPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskScalefPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzScalefPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzScalefPd(k uint8, a [4]float64, b [4]float64) [4]float64


// ScalefPd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_scalef_pd'.
// Requires AVX512F.
func ScalefPd(a M256d, b M256d) M256d {
	return M256d(scalefPd([4]float64(a), [4]float64(b)))
}

func scalefPd(a [4]float64, b [4]float64) [4]float64


// MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskScalefPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskScalefPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzScalefPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzScalefPs(k uint8, a [8]float32, b [8]float32) [8]float32


// ScalefPs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_scalef_ps'.
// Requires AVX512F.
func ScalefPs(a M256, b M256) M256 {
	return M256(scalefPs([8]float32(a), [8]float32(b)))
}

func scalefPs(a [8]float32, b [8]float32) [8]float32


// Set16: Set packed 16-bit integers in 'dst' with the supplied values. 
//
//		dst[15:0] := e0
//		dst[31:16] := e1
//		dst[47:32] := e2
//		dst[63:48] := e3
//		dst[79:64] := e4
//		dst[95:80] := e5
//		dst[111:96] := e6
//		dst[127:112] := e7
//		dst[145:128] := e8
//		dst[159:144] := e9
//		dst[175:160] := e10
//		dst[191:176] := e11
//		dst[207:192] := e12
//		dst[223:208] := e13
//		dst[239:224] := e14
//		dst[255:240] := e15
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi16'.
// Requires AVX.
func Set16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) M256i {
	return M256i(set16(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func set16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) [32]byte


// Set32: Set packed 32-bit integers in 'dst' with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi32'.
// Requires AVX.
func Set32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M256i {
	return M256i(set32(e7, e6, e5, e4, e3, e2, e1, e0))
}

func set32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [32]byte


// Set64x: Set packed 64-bit integers in 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi64x'.
// Requires AVX.
func Set64x(e3 int64, e2 int64, e1 int64, e0 int64) M256i {
	return M256i(set64x(e3, e2, e1, e0))
}

func set64x(e3 int64, e2 int64, e1 int64, e0 int64) [32]byte


// Set8: Set packed 8-bit integers in 'dst' with the supplied values in reverse
// order. 
//
//		dst[7:0] := e0
//		dst[15:8] := e1
//		dst[23:16] := e2
//		dst[31:24] := e3
//		dst[39:32] := e4
//		dst[47:40] := e5
//		dst[55:48] := e6
//		dst[63:56] := e7
//		dst[71:64] := e8
//		dst[79:72] := e9
//		dst[87:80] := e10
//		dst[95:88] := e11
//		dst[103:96] := e12
//		dst[111:104] := e13
//		dst[119:112] := e14
//		dst[127:120] := e15
//		dst[135:128] := e16
//		dst[143:136] := e17
//		dst[151:144] := e18
//		dst[159:152] := e19
//		dst[167:160] := e20
//		dst[175:168] := e21
//		dst[183:176] := e22
//		dst[191:184] := e23
//		dst[199:192] := e24
//		dst[207:200] := e25
//		dst[215:208] := e26
//		dst[223:216] := e27
//		dst[231:224] := e28
//		dst[239:232] := e29
//		dst[247:240] := e30
//		dst[255:248] := e31
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi8'.
// Requires AVX.
func Set8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) M256i {
	return M256i(set8(e31, e30, e29, e28, e27, e26, e25, e24, e23, e22, e21, e20, e19, e18, e17, e16, e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func set8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) [32]byte


// SetM128: Set packed __m256 vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_set_m128'.
// Requires AVX.
func SetM128(hi M128, lo M128) M256 {
	return M256(setM128([4]float32(hi), [4]float32(lo)))
}

func setM128(hi [4]float32, lo [4]float32) [8]float32


// SetM128d: Set packed __m256d vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_set_m128d'.
// Requires AVX.
func SetM128d(hi M128d, lo M128d) M256d {
	return M256d(setM128d([2]float64(hi), [2]float64(lo)))
}

func setM128d(hi [2]float64, lo [2]float64) [4]float64


// SetM128i: Set packed __m256i vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_set_m128i'.
// Requires AVX.
func SetM128i(hi M128i, lo M128i) M256i {
	return M256i(setM128i([16]byte(hi), [16]byte(lo)))
}

func setM128i(hi [16]byte, lo [16]byte) [32]byte


// SetPd: Set packed double-precision (64-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_pd'.
// Requires AVX.
func SetPd(e3 float64, e2 float64, e1 float64, e0 float64) M256d {
	return M256d(setPd(e3, e2, e1, e0))
}

func setPd(e3 float64, e2 float64, e1 float64, e0 float64) [4]float64


// SetPs: Set packed single-precision (32-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_ps'.
// Requires AVX.
func SetPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M256 {
	return M256(setPs(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [8]float32


// MaskSet116: Broadcast the low packed 16-bit integer from 'a' to all elements
// of 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_mask_set1_epi16'.
// Requires AVX512BW.
func MaskSet116(src M256i, k Mmask16, a int16) M256i {
	return M256i(maskSet116([32]byte(src), uint16(k), a))
}

func maskSet116(src [32]byte, k uint16, a int16) [32]byte


// MaskzSet116: Broadcast 16-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_maskz_set1_epi16'.
// Requires AVX512BW.
func MaskzSet116(k Mmask16, a int16) M256i {
	return M256i(maskzSet116(uint16(k), a))
}

func maskzSet116(k uint16, a int16) [32]byte


// Set116: Broadcast 16-bit integer 'a' to all all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastw'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi16'.
// Requires AVX.
func Set116(a int16) M256i {
	return M256i(set116(a))
}

func set116(a int16) [32]byte


// MaskSet132: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet132(src M256i, k Mmask8, a int) M256i {
	return M256i(maskSet132([32]byte(src), uint8(k), a))
}

func maskSet132(src [32]byte, k uint8, a int) [32]byte


// MaskzSet132: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet132(k Mmask8, a int) M256i {
	return M256i(maskzSet132(uint8(k), a))
}

func maskzSet132(k uint8, a int) [32]byte


// Set132: Broadcast 32-bit integer 'a' to all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastd'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi32'.
// Requires AVX.
func Set132(a int) M256i {
	return M256i(set132(a))
}

func set132(a int) [32]byte


// MaskSet164: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet164(src M256i, k Mmask8, a int64) M256i {
	return M256i(maskSet164([32]byte(src), uint8(k), a))
}

func maskSet164(src [32]byte, k uint8, a int64) [32]byte


// MaskzSet164: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet164(k Mmask8, a int64) M256i {
	return M256i(maskzSet164(uint8(k), a))
}

func maskzSet164(k uint8, a int64) [32]byte


// Set164x: Broadcast 64-bit integer 'a' to all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastq'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi64x'.
// Requires AVX.
func Set164x(a int64) M256i {
	return M256i(set164x(a))
}

func set164x(a int64) [32]byte


// MaskSet18: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_mask_set1_epi8'.
// Requires AVX512BW.
func MaskSet18(src M256i, k Mmask32, a byte) M256i {
	return M256i(maskSet18([32]byte(src), uint32(k), a))
}

func maskSet18(src [32]byte, k uint32, a byte) [32]byte


// MaskzSet18: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_maskz_set1_epi8'.
// Requires AVX512BW.
func MaskzSet18(k Mmask32, a byte) M256i {
	return M256i(maskzSet18(uint32(k), a))
}

func maskzSet18(k uint32, a byte) [32]byte


// Set18: Broadcast 8-bit integer 'a' to all elements of 'dst'. This intrinsic
// may generate the 'vpbroadcastb'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi8'.
// Requires AVX.
func Set18(a byte) M256i {
	return M256i(set18(a))
}

func set18(a byte) [32]byte


// Set1Pd: Broadcast double-precision (64-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_pd'.
// Requires AVX.
func Set1Pd(a float64) M256d {
	return M256d(set1Pd(a))
}

func set1Pd(a float64) [4]float64


// Set1Ps: Broadcast single-precision (32-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_ps'.
// Requires AVX.
func Set1Ps(a float32) M256 {
	return M256(set1Ps(a))
}

func set1Ps(a float32) [8]float32


// Setr16: Set packed 16-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[15:0] := e15
//		dst[31:16] := e14
//		dst[47:32] := e13
//		dst[63:48] := e12
//		dst[79:64] := e11
//		dst[95:80] := e10
//		dst[111:96] := e9
//		dst[127:112] := e8
//		dst[145:128] := e7
//		dst[159:144] := e6
//		dst[175:160] := e5
//		dst[191:176] := e4
//		dst[207:192] := e3
//		dst[223:208] := e2
//		dst[239:224] := e1
//		dst[255:240] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi16'.
// Requires AVX.
func Setr16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) M256i {
	return M256i(setr16(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setr16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) [32]byte


// Setr32: Set packed 32-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[31:0] := e7
//		dst[63:32] := e6
//		dst[95:64] := e5
//		dst[127:96] := e4
//		dst[159:128] := e3
//		dst[191:160] := e2
//		dst[223:192] := e1
//		dst[255:224] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi32'.
// Requires AVX.
func Setr32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M256i {
	return M256i(setr32(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setr32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [32]byte


// Setr64x: Set packed 64-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[63:0] := e3
//		dst[127:64] := e2
//		dst[191:128] := e1
//		dst[255:192] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi64x'.
// Requires AVX.
func Setr64x(e3 int64, e2 int64, e1 int64, e0 int64) M256i {
	return M256i(setr64x(e3, e2, e1, e0))
}

func setr64x(e3 int64, e2 int64, e1 int64, e0 int64) [32]byte


// Setr8: Set packed 8-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[7:0] := e31
//		dst[15:8] := e30
//		dst[23:16] := e29
//		dst[31:24] := e28
//		dst[39:32] := e27
//		dst[47:40] := e26
//		dst[55:48] := e25
//		dst[63:56] := e24
//		dst[71:64] := e23
//		dst[79:72] := e22
//		dst[87:80] := e21
//		dst[95:88] := e20
//		dst[103:96] := e19
//		dst[111:104] := e18
//		dst[119:112] := e17
//		dst[127:120] := e16
//		dst[135:128] := e15
//		dst[143:136] := e14
//		dst[151:144] := e13
//		dst[159:152] := e12
//		dst[167:160] := e11
//		dst[175:168] := e10
//		dst[183:176] := e9
//		dst[191:184] := e8
//		dst[199:192] := e7
//		dst[207:200] := e6
//		dst[215:208] := e5
//		dst[223:216] := e4
//		dst[231:224] := e3
//		dst[239:232] := e2
//		dst[247:240] := e1
//		dst[255:248] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi8'.
// Requires AVX.
func Setr8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) M256i {
	return M256i(setr8(e31, e30, e29, e28, e27, e26, e25, e24, e23, e22, e21, e20, e19, e18, e17, e16, e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setr8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) [32]byte


// SetrM128: Set packed __m256 vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_setr_m128'.
// Requires AVX.
func SetrM128(lo M128, hi M128) M256 {
	return M256(setrM128([4]float32(lo), [4]float32(hi)))
}

func setrM128(lo [4]float32, hi [4]float32) [8]float32


// SetrM128d: Set packed __m256d vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_setr_m128d'.
// Requires AVX.
func SetrM128d(lo M128d, hi M128d) M256d {
	return M256d(setrM128d([2]float64(lo), [2]float64(hi)))
}

func setrM128d(lo [2]float64, hi [2]float64) [4]float64


// SetrM128i: Set packed __m256i vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_setr_m128i'.
// Requires AVX.
func SetrM128i(lo M128i, hi M128i) M256i {
	return M256i(setrM128i([16]byte(lo), [16]byte(hi)))
}

func setrM128i(lo [16]byte, hi [16]byte) [32]byte


// SetrPd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[63:0] := e3
//		dst[127:64] := e2
//		dst[191:128] := e1
//		dst[255:192] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_pd'.
// Requires AVX.
func SetrPd(e3 float64, e2 float64, e1 float64, e0 float64) M256d {
	return M256d(setrPd(e3, e2, e1, e0))
}

func setrPd(e3 float64, e2 float64, e1 float64, e0 float64) [4]float64


// SetrPs: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[31:0] := e7
//		dst[63:32] := e6
//		dst[95:64] := e5
//		dst[127:96] := e4
//		dst[159:128] := e3
//		dst[191:160] := e2
//		dst[223:192] := e1
//		dst[255:224] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_ps'.
// Requires AVX.
func SetrPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M256 {
	return M256(setrPs(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [8]float32


// SetzeroPd: Return vector of type __m256d with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_setzero_pd'.
// Requires AVX.
func SetzeroPd() M256d {
	return M256d(setzeroPd())
}

func setzeroPd() [4]float64


// SetzeroPs: Return vector of type __m256 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_setzero_ps'.
// Requires AVX.
func SetzeroPs() M256 {
	return M256(setzeroPs())
}

func setzeroPs() [8]float32


// SetzeroSi256: Return vector of type __m256i with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXOR'. Intrinsic: '_mm256_setzero_si256'.
// Requires AVX.
func SetzeroSi256() M256i {
	return M256i(setzeroSi256())
}

func setzeroSi256() [32]byte


// MaskShuffle32: Shuffle 32-bit integers in 'a' within 128-bit lanes using the
// control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_mask_shuffle_epi32'.
// Requires AVX512F.
func MaskShuffle32(src M256i, k Mmask8, a M256i, imm8 MMPERMENUM) M256i {
	return M256i(maskShuffle32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskShuffle32(src [32]byte, k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// MaskzShuffle32: Shuffle 32-bit integers in 'a' within 128-bit lanes using
// the control in 'imm8', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffle32(k Mmask8, a M256i, imm8 MMPERMENUM) M256i {
	return M256i(maskzShuffle32(uint8(k), [32]byte(a), imm8))
}

func maskzShuffle32(k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// Shuffle32: Shuffle 32-bit integers in 'a' within 128-bit lanes using the
// control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_shuffle_epi32'.
// Requires AVX2.
func Shuffle32(a M256i, imm8 int) M256i {
	return M256i(shuffle32([32]byte(a), imm8))
}

func shuffle32(a [32]byte, imm8 int) [32]byte


// MaskShuffle8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm256_mask_shuffle_epi8'.
// Requires AVX512BW.
func MaskShuffle8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskShuffle8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskShuffle8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzShuffle8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm256_maskz_shuffle_epi8'.
// Requires AVX512BW.
func MaskzShuffle8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzShuffle8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzShuffle8(k uint32, a [32]byte, b [32]byte) [32]byte


// Shuffle8: Shuffle 8-bit integers in 'a' within 128-bit lanes according to
// shuffle control mask in the corresponding 8-bit element of 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF b[i+7] == 1
//				dst[i+7:i] := 0
//			ELSE
//				index[3:0] := b[i+3:i]
//				dst[i+7:i] := a[index*8+7:index*8]
//			FI
//			IF b[128+i+7] == 1
//				dst[128+i+7:i] := 0
//			ELSE
//				index[3:0] := b[128+i+3:128+i]
//				dst[128+i+7:i] := a[128+index*8+7:128+index*8]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm256_shuffle_epi8'.
// Requires AVX2.
func Shuffle8(a M256i, b M256i) M256i {
	return M256i(shuffle8([32]byte(a), [32]byte(b)))
}

func shuffle8(a [32]byte, b [32]byte) [32]byte


// MaskShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_mask_shuffle_f32x4'.
// Requires AVX512F.
func MaskShuffleF32x4(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskShuffleF32x4([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskShuffleF32x4(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_maskz_shuffle_f32x4'.
// Requires AVX512F.
func MaskzShuffleF32x4(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzShuffleF32x4(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzShuffleF32x4(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// ShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_shuffle_f32x4'.
// Requires AVX512F.
func ShuffleF32x4(a M256, b M256, imm8 int) M256 {
	return M256(shuffleF32x4([8]float32(a), [8]float32(b), imm8))
}

func shuffleF32x4(a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_mask_shuffle_f64x2'.
// Requires AVX512F.
func MaskShuffleF64x2(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskShuffleF64x2([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskShuffleF64x2(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_maskz_shuffle_f64x2'.
// Requires AVX512F.
func MaskzShuffleF64x2(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzShuffleF64x2(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzShuffleF64x2(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// ShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_shuffle_f64x2'.
// Requires AVX512F.
func ShuffleF64x2(a M256d, b M256d, imm8 int) M256d {
	return M256d(shuffleF64x2([4]float64(a), [4]float64(b), imm8))
}

func shuffleF64x2(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_mask_shuffle_i32x4'.
// Requires AVX512F.
func MaskShuffleI32x4(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskShuffleI32x4([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskShuffleI32x4(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_maskz_shuffle_i32x4'.
// Requires AVX512F.
func MaskzShuffleI32x4(k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzShuffleI32x4(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzShuffleI32x4(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// ShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_shuffle_i32x4'.
// Requires AVX512F.
func ShuffleI32x4(a M256i, b M256i, imm8 int) M256i {
	return M256i(shuffleI32x4([32]byte(a), [32]byte(b), imm8))
}

func shuffleI32x4(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_mask_shuffle_i64x2'.
// Requires AVX512F.
func MaskShuffleI64x2(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskShuffleI64x2([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskShuffleI64x2(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_maskz_shuffle_i64x2'.
// Requires AVX512F.
func MaskzShuffleI64x2(k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzShuffleI64x2(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzShuffleI64x2(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// ShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_shuffle_i64x2'.
// Requires AVX512F.
func ShuffleI64x2(a M256i, b M256i, imm8 int) M256i {
	return M256i(shuffleI64x2([32]byte(a), [32]byte(b), imm8))
}

func shuffleI64x2(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskShufflePd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskShufflePd(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzShufflePd(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzShufflePd(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// ShufflePd: Shuffle double-precision (64-bit) floating-point elements within
// 128-bit lanes using the control in 'imm8', and store the results in 'dst'. 
//
//		dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_shuffle_pd'.
// Requires AVX.
func ShufflePd(a M256d, b M256d, imm8 int) M256d {
	return M256d(shufflePd([4]float64(a), [4]float64(b), imm8))
}

func shufflePd(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskShufflePs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskShufflePs(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzShufflePs(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzShufflePs(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// ShufflePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_shuffle_ps'.
// Requires AVX.
func ShufflePs(a M256, b M256, imm8 int) M256 {
	return M256(shufflePs([8]float32(a), [8]float32(b), imm8))
}

func shufflePs(a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskShufflehi16: Shuffle 16-bit integers in the high 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the high 64
// bits of 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		tmp_dst[191:128] := a[191:128]
//		tmp_dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		tmp_dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		tmp_dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		tmp_dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm256_mask_shufflehi_epi16'.
// Requires AVX512BW.
func MaskShufflehi16(src M256i, k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskShufflehi16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskShufflehi16(src [32]byte, k uint16, a [32]byte, imm8 int) [32]byte


// MaskzShufflehi16: Shuffle 16-bit integers in the high 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the high 64
// bits of 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		tmp_dst[191:128] := a[191:128]
//		tmp_dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		tmp_dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		tmp_dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		tmp_dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm256_maskz_shufflehi_epi16'.
// Requires AVX512BW.
func MaskzShufflehi16(k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskzShufflehi16(uint16(k), [32]byte(a), imm8))
}

func maskzShufflehi16(k uint16, a [32]byte, imm8 int) [32]byte


// Shufflehi16: Shuffle 16-bit integers in the high 64 bits of 128-bit lanes of
// 'a' using the control in 'imm8'. Store the results in the high 64 bits of
// 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being copied
// from from 'a' to 'dst'. 
//
//		dst[63:0] := a[63:0]
//		dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		dst[191:128] := a[191:128]
//		dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm256_shufflehi_epi16'.
// Requires AVX2.
func Shufflehi16(a M256i, imm8 int) M256i {
	return M256i(shufflehi16([32]byte(a), imm8))
}

func shufflehi16(a [32]byte, imm8 int) [32]byte


// MaskShufflelo16: Shuffle 16-bit integers in the low 64 bits of 128-bit lanes
// of 'a' using the control in 'imm8'. Store the results in the low 64 bits of
// 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being copied
// from from 'a' to 'dst', using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		tmp_dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		tmp_dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		tmp_dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		tmp_dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		tmp_dst[255:192] := a[255:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm256_mask_shufflelo_epi16'.
// Requires AVX512BW.
func MaskShufflelo16(src M256i, k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskShufflelo16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskShufflelo16(src [32]byte, k uint16, a [32]byte, imm8 int) [32]byte


// MaskzShufflelo16: Shuffle 16-bit integers in the low 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the low 64
// bits of 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		tmp_dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		tmp_dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		tmp_dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		tmp_dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		tmp_dst[255:192] := a[255:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm256_maskz_shufflelo_epi16'.
// Requires AVX512BW.
func MaskzShufflelo16(k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskzShufflelo16(uint16(k), [32]byte(a), imm8))
}

func maskzShufflelo16(k uint16, a [32]byte, imm8 int) [32]byte


// Shufflelo16: Shuffle 16-bit integers in the low 64 bits of 128-bit lanes of
// 'a' using the control in 'imm8'. Store the results in the low 64 bits of
// 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being copied
// from from 'a' to 'dst'. 
//
//		dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		dst[127:64] := a[127:64]
//		dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		dst[255:192] := a[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm256_shufflelo_epi16'.
// Requires AVX2.
func Shufflelo16(a M256i, imm8 int) M256i {
	return M256i(shufflelo16([32]byte(a), imm8))
}

func shufflelo16(a [32]byte, imm8 int) [32]byte


// Sign16: Negate packed 16-bit integers in 'a' when the corresponding signed
// 16-bit integer in 'b' is negative, and store the results in 'dst'. Element
// in 'dst' are zeroed out when the corresponding element in 'b' is zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF b[i+15:i] < 0
//				dst[i+15:i] := NEG(a[i+15:i])
//			ELSE IF b[i+15:i] = 0
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSIGNW'. Intrinsic: '_mm256_sign_epi16'.
// Requires AVX2.
func Sign16(a M256i, b M256i) M256i {
	return M256i(sign16([32]byte(a), [32]byte(b)))
}

func sign16(a [32]byte, b [32]byte) [32]byte


// Sign32: Negate packed 32-bit integers in 'a' when the corresponding signed
// 32-bit integer in 'b' is negative, and store the results in 'dst'. Element
// in 'dst' are zeroed out when the corresponding element in 'b' is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF b[i+31:i] < 0
//				dst[i+31:i] := NEG(a[i+31:i])
//			ELSE IF b[i+31:i] = 0
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSIGND'. Intrinsic: '_mm256_sign_epi32'.
// Requires AVX2.
func Sign32(a M256i, b M256i) M256i {
	return M256i(sign32([32]byte(a), [32]byte(b)))
}

func sign32(a [32]byte, b [32]byte) [32]byte


// Sign8: Negate packed 8-bit integers in 'a' when the corresponding signed
// 8-bit integer in 'b' is negative, and store the results in 'dst'. Element in
// 'dst' are zeroed out when the corresponding element in 'b' is zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF b[i+7:i] < 0
//				dst[i+7:i] := NEG(a[i+7:i])
//			ELSE IF b[i+7:i] = 0
//				dst[i+7:i] := 0
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSIGNB'. Intrinsic: '_mm256_sign_epi8'.
// Requires AVX2.
func Sign8(a M256i, b M256i) M256i {
	return M256i(sign8([32]byte(a), [32]byte(b)))
}

func sign8(a [32]byte, b [32]byte) [32]byte


// SinPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sin_pd'.
// Requires AVX.
func SinPd(a M256d) M256d {
	return M256d(sinPd([4]float64(a)))
}

func sinPd(a [4]float64) [4]float64


// SinPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sin_ps'.
// Requires AVX.
func SinPs(a M256) M256 {
	return M256(sinPs([8]float32(a)))
}

func sinPs(a [8]float32) [8]float32


// SincosPd: Compute the sine and cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, store the sine in
// 'dst', and store the cosine into memory at 'mem_addr'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//			MEM[mem_addr+i+63:mem_addr+i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sincos_pd'.
// Requires AVX.
func SincosPd(mem_addr M256d, a M256d) M256d {
	return M256d(sincosPd([4]float64(mem_addr), [4]float64(a)))
}

func sincosPd(mem_addr [4]float64, a [4]float64) [4]float64


// SincosPs: Compute the sine and cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, store the sine in
// 'dst', and store the cosine into memory at 'mem_addr'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sincos_ps'.
// Requires AVX.
func SincosPs(mem_addr M256, a M256) M256 {
	return M256(sincosPs([8]float32(mem_addr), [8]float32(a)))
}

func sincosPs(mem_addr [8]float32, a [8]float32) [8]float32


// SindPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SIND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sind_pd'.
// Requires AVX.
func SindPd(a M256d) M256d {
	return M256d(sindPd([4]float64(a)))
}

func sindPd(a [4]float64) [4]float64


// SindPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SIND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sind_ps'.
// Requires AVX.
func SindPs(a M256) M256 {
	return M256(sindPs([8]float32(a)))
}

func sindPs(a [8]float32) [8]float32


// SinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sinh_pd'.
// Requires AVX.
func SinhPd(a M256d) M256d {
	return M256d(sinhPd([4]float64(a)))
}

func sinhPd(a [4]float64) [4]float64


// SinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sinh_ps'.
// Requires AVX.
func SinhPs(a M256) M256 {
	return M256(sinhPs([8]float32(a)))
}

func sinhPs(a [8]float32) [8]float32


// MaskSll16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_mask_sll_epi16'.
// Requires AVX512BW.
func MaskSll16(src M256i, k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskSll16([32]byte(src), uint16(k), [32]byte(a), [16]byte(count)))
}

func maskSll16(src [32]byte, k uint16, a [32]byte, count [16]byte) [32]byte


// MaskzSll16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_maskz_sll_epi16'.
// Requires AVX512BW.
func MaskzSll16(k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskzSll16(uint16(k), [32]byte(a), [16]byte(count)))
}

func maskzSll16(k uint16, a [32]byte, count [16]byte) [32]byte


// Sll16: Shift packed 16-bit integers in 'a' left by 'count' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_sll_epi16'.
// Requires AVX2.
func Sll16(a M256i, count M128i) M256i {
	return M256i(sll16([32]byte(a), [16]byte(count)))
}

func sll16(a [32]byte, count [16]byte) [32]byte


// MaskSll32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_sll_epi32'.
// Requires AVX512F.
func MaskSll32(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSll32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSll32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSll32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSll32(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSll32(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSll32(k uint8, a [32]byte, count [16]byte) [32]byte


// Sll32: Shift packed 32-bit integers in 'a' left by 'count' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_sll_epi32'.
// Requires AVX2.
func Sll32(a M256i, count M128i) M256i {
	return M256i(sll32([32]byte(a), [16]byte(count)))
}

func sll32(a [32]byte, count [16]byte) [32]byte


// MaskSll64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_sll_epi64'.
// Requires AVX512F.
func MaskSll64(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSll64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSll64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSll64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSll64(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSll64(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSll64(k uint8, a [32]byte, count [16]byte) [32]byte


// Sll64: Shift packed 64-bit integers in 'a' left by 'count' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_sll_epi64'.
// Requires AVX2.
func Sll64(a M256i, count M128i) M256i {
	return M256i(sll64([32]byte(a), [16]byte(count)))
}

func sll64(a [32]byte, count [16]byte) [32]byte


// MaskSlli16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_mask_slli_epi16'.
// Requires AVX512BW.
func MaskSlli16(src M256i, k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskSlli16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskSlli16(src [32]byte, k uint16, a [32]byte, imm8 uint32) [32]byte


// MaskzSlli16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_maskz_slli_epi16'.
// Requires AVX512BW.
func MaskzSlli16(k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlli16(uint16(k), [32]byte(a), imm8))
}

func maskzSlli16(k uint16, a [32]byte, imm8 uint32) [32]byte


// Slli16: Shift packed 16-bit integers in 'a' left by 'imm8' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_slli_epi16'.
// Requires AVX2.
func Slli16(a M256i, imm8 int) M256i {
	return M256i(slli16([32]byte(a), imm8))
}

func slli16(a [32]byte, imm8 int) [32]byte


// MaskSlli32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_slli_epi32'.
// Requires AVX512F.
func MaskSlli32(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSlli32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSlli32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSlli32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlli32(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlli32(uint8(k), [32]byte(a), imm8))
}

func maskzSlli32(k uint8, a [32]byte, imm8 uint32) [32]byte


// Slli32: Shift packed 32-bit integers in 'a' left by 'imm8' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_slli_epi32'.
// Requires AVX2.
func Slli32(a M256i, imm8 int) M256i {
	return M256i(slli32([32]byte(a), imm8))
}

func slli32(a [32]byte, imm8 int) [32]byte


// MaskSlli64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlli64(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSlli64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSlli64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSlli64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlli64(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlli64(uint8(k), [32]byte(a), imm8))
}

func maskzSlli64(k uint8, a [32]byte, imm8 uint32) [32]byte


// Slli64: Shift packed 64-bit integers in 'a' left by 'imm8' while shifting in
// zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_slli_epi64'.
// Requires AVX2.
func Slli64(a M256i, imm8 int) M256i {
	return M256i(slli64([32]byte(a), imm8))
}

func slli64(a [32]byte, imm8 int) [32]byte


// SlliSi256: Shift 128-bit lanes in 'a' left by 'imm8' bytes while shifting in
// zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] << (tmp*8)
//		dst[255:128] := a[255:128] << (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLDQ'. Intrinsic: '_mm256_slli_si256'.
// Requires AVX2.
func SlliSi256(a M256i, imm8 int) M256i {
	return M256i(slliSi256([32]byte(a), imm8))
}

func slliSi256(a [32]byte, imm8 int) [32]byte


// MaskSllv16: Shift packed 16-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm256_mask_sllv_epi16'.
// Requires AVX512BW.
func MaskSllv16(src M256i, k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskSllv16([32]byte(src), uint16(k), [32]byte(a), [32]byte(count)))
}

func maskSllv16(src [32]byte, k uint16, a [32]byte, count [32]byte) [32]byte


// MaskzSllv16: Shift packed 16-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm256_maskz_sllv_epi16'.
// Requires AVX512BW.
func MaskzSllv16(k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskzSllv16(uint16(k), [32]byte(a), [32]byte(count)))
}

func maskzSllv16(k uint16, a [32]byte, count [32]byte) [32]byte


// Sllv16: Shift packed 16-bit integers in 'a' left by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm256_sllv_epi16'.
// Requires AVX512BW.
func Sllv16(a M256i, count M256i) M256i {
	return M256i(sllv16([32]byte(a), [32]byte(count)))
}

func sllv16(a [32]byte, count [32]byte) [32]byte


// MaskSllv32: Shift packed 32-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_mask_sllv_epi32'.
// Requires AVX512F.
func MaskSllv32(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSllv32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSllv32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSllv32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllv32(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSllv32(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSllv32(k uint8, a [32]byte, count [32]byte) [32]byte


// Sllv32: Shift packed 32-bit integers in 'a' left by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_sllv_epi32'.
// Requires AVX2.
func Sllv32(a M256i, count M256i) M256i {
	return M256i(sllv32([32]byte(a), [32]byte(count)))
}

func sllv32(a [32]byte, count [32]byte) [32]byte


// MaskSllv64: Shift packed 64-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllv64(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSllv64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSllv64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSllv64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllv64(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSllv64(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSllv64(k uint8, a [32]byte, count [32]byte) [32]byte


// Sllv64: Shift packed 64-bit integers in 'a' left by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_sllv_epi64'.
// Requires AVX2.
func Sllv64(a M256i, count M256i) M256i {
	return M256i(sllv64([32]byte(a), [32]byte(count)))
}

func sllv64(a [32]byte, count [32]byte) [32]byte


// MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskSqrtPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskSqrtPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd(k Mmask8, a M256d) M256d {
	return M256d(maskzSqrtPd(uint8(k), [4]float64(a)))
}

func maskzSqrtPd(k uint8, a [4]float64) [4]float64


// SqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_sqrt_pd'.
// Requires AVX.
func SqrtPd(a M256d) M256d {
	return M256d(sqrtPd([4]float64(a)))
}

func sqrtPd(a [4]float64) [4]float64


// MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskSqrtPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskSqrtPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs(k Mmask8, a M256) M256 {
	return M256(maskzSqrtPs(uint8(k), [8]float32(a)))
}

func maskzSqrtPs(k uint8, a [8]float32) [8]float32


// SqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_sqrt_ps'.
// Requires AVX.
func SqrtPs(a M256) M256 {
	return M256(sqrtPs([8]float32(a)))
}

func sqrtPs(a [8]float32) [8]float32


// MaskSra16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_mask_sra_epi16'.
// Requires AVX512BW.
func MaskSra16(src M256i, k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskSra16([32]byte(src), uint16(k), [32]byte(a), [16]byte(count)))
}

func maskSra16(src [32]byte, k uint16, a [32]byte, count [16]byte) [32]byte


// MaskzSra16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_maskz_sra_epi16'.
// Requires AVX512BW.
func MaskzSra16(k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskzSra16(uint16(k), [32]byte(a), [16]byte(count)))
}

func maskzSra16(k uint16, a [32]byte, count [16]byte) [32]byte


// Sra16: Shift packed 16-bit integers in 'a' right by 'count' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_sra_epi16'.
// Requires AVX2.
func Sra16(a M256i, count M128i) M256i {
	return M256i(sra16([32]byte(a), [16]byte(count)))
}

func sra16(a [32]byte, count [16]byte) [32]byte


// MaskSra32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_sra_epi32'.
// Requires AVX512F.
func MaskSra32(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSra32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSra32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSra32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSra32(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSra32(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSra32(k uint8, a [32]byte, count [16]byte) [32]byte


// Sra32: Shift packed 32-bit integers in 'a' right by 'count' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_sra_epi32'.
// Requires AVX2.
func Sra32(a M256i, count M128i) M256i {
	return M256i(sra32([32]byte(a), [16]byte(count)))
}

func sra32(a [32]byte, count [16]byte) [32]byte


// MaskSra64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_sra_epi64'.
// Requires AVX512F.
func MaskSra64(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSra64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSra64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSra64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSra64(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSra64(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSra64(k uint8, a [32]byte, count [16]byte) [32]byte


// Sra64: Shift packed 64-bit integers in 'a' right by 'count' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_sra_epi64'.
// Requires AVX512F.
func Sra64(a M256i, count M128i) M256i {
	return M256i(sra64([32]byte(a), [16]byte(count)))
}

func sra64(a [32]byte, count [16]byte) [32]byte


// MaskSrai16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_mask_srai_epi16'.
// Requires AVX512BW.
func MaskSrai16(src M256i, k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskSrai16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskSrai16(src [32]byte, k uint16, a [32]byte, imm8 uint32) [32]byte


// MaskzSrai16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_maskz_srai_epi16'.
// Requires AVX512BW.
func MaskzSrai16(k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrai16(uint16(k), [32]byte(a), imm8))
}

func maskzSrai16(k uint16, a [32]byte, imm8 uint32) [32]byte


// Srai16: Shift packed 16-bit integers in 'a' right by 'imm8' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_srai_epi16'.
// Requires AVX2.
func Srai16(a M256i, imm8 int) M256i {
	return M256i(srai16([32]byte(a), imm8))
}

func srai16(a [32]byte, imm8 int) [32]byte


// MaskSrai32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_srai_epi32'.
// Requires AVX512F.
func MaskSrai32(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrai32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrai32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrai32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSrai32(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrai32(uint8(k), [32]byte(a), imm8))
}

func maskzSrai32(k uint8, a [32]byte, imm8 uint32) [32]byte


// Srai32: Shift packed 32-bit integers in 'a' right by 'imm8' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_srai_epi32'.
// Requires AVX2.
func Srai32(a M256i, imm8 int) M256i {
	return M256i(srai32([32]byte(a), imm8))
}

func srai32(a [32]byte, imm8 int) [32]byte


// MaskSrai64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_srai_epi64'.
// Requires AVX512F.
func MaskSrai64(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrai64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrai64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrai64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSrai64(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrai64(uint8(k), [32]byte(a), imm8))
}

func maskzSrai64(k uint8, a [32]byte, imm8 uint32) [32]byte


// Srai64: Shift packed 64-bit integers in 'a' right by 'imm8' while shifting
// in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_srai_epi64'.
// Requires AVX512F.
func Srai64(a M256i, imm8 uint32) M256i {
	return M256i(srai64([32]byte(a), imm8))
}

func srai64(a [32]byte, imm8 uint32) [32]byte


// MaskSrav16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm256_mask_srav_epi16'.
// Requires AVX512BW.
func MaskSrav16(src M256i, k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskSrav16([32]byte(src), uint16(k), [32]byte(a), [32]byte(count)))
}

func maskSrav16(src [32]byte, k uint16, a [32]byte, count [32]byte) [32]byte


// MaskzSrav16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm256_maskz_srav_epi16'.
// Requires AVX512BW.
func MaskzSrav16(k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskzSrav16(uint16(k), [32]byte(a), [32]byte(count)))
}

func maskzSrav16(k uint16, a [32]byte, count [32]byte) [32]byte


// Srav16: Shift packed 16-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in sign bits, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm256_srav_epi16'.
// Requires AVX512BW.
func Srav16(a M256i, count M256i) M256i {
	return M256i(srav16([32]byte(a), [32]byte(count)))
}

func srav16(a [32]byte, count [32]byte) [32]byte


// MaskSrav32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_mask_srav_epi32'.
// Requires AVX512F.
func MaskSrav32(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrav32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrav32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrav32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSrav32(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrav32(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrav32(k uint8, a [32]byte, count [32]byte) [32]byte


// Srav32: Shift packed 32-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in sign bits, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_srav_epi32'.
// Requires AVX2.
func Srav32(a M256i, count M256i) M256i {
	return M256i(srav32([32]byte(a), [32]byte(count)))
}

func srav32(a [32]byte, count [32]byte) [32]byte


// MaskSrav64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_mask_srav_epi64'.
// Requires AVX512F.
func MaskSrav64(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrav64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrav64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrav64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSrav64(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrav64(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrav64(k uint8, a [32]byte, count [32]byte) [32]byte


// Srav64: Shift packed 64-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in sign bits, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_srav_epi64'.
// Requires AVX512F.
func Srav64(a M256i, count M256i) M256i {
	return M256i(srav64([32]byte(a), [32]byte(count)))
}

func srav64(a [32]byte, count [32]byte) [32]byte


// MaskSrl16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_mask_srl_epi16'.
// Requires AVX512BW.
func MaskSrl16(src M256i, k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskSrl16([32]byte(src), uint16(k), [32]byte(a), [16]byte(count)))
}

func maskSrl16(src [32]byte, k uint16, a [32]byte, count [16]byte) [32]byte


// MaskzSrl16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_maskz_srl_epi16'.
// Requires AVX512BW.
func MaskzSrl16(k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskzSrl16(uint16(k), [32]byte(a), [16]byte(count)))
}

func maskzSrl16(k uint16, a [32]byte, count [16]byte) [32]byte


// Srl16: Shift packed 16-bit integers in 'a' right by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_srl_epi16'.
// Requires AVX2.
func Srl16(a M256i, count M128i) M256i {
	return M256i(srl16([32]byte(a), [16]byte(count)))
}

func srl16(a [32]byte, count [16]byte) [32]byte


// MaskSrl32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrl32(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSrl32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSrl32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSrl32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrl32(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSrl32(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSrl32(k uint8, a [32]byte, count [16]byte) [32]byte


// Srl32: Shift packed 32-bit integers in 'a' right by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_srl_epi32'.
// Requires AVX2.
func Srl32(a M256i, count M128i) M256i {
	return M256i(srl32([32]byte(a), [16]byte(count)))
}

func srl32(a [32]byte, count [16]byte) [32]byte


// MaskSrl64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrl64(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSrl64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSrl64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSrl64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrl64(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSrl64(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSrl64(k uint8, a [32]byte, count [16]byte) [32]byte


// Srl64: Shift packed 64-bit integers in 'a' right by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_srl_epi64'.
// Requires AVX2.
func Srl64(a M256i, count M128i) M256i {
	return M256i(srl64([32]byte(a), [16]byte(count)))
}

func srl64(a [32]byte, count [16]byte) [32]byte


// MaskSrli16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_mask_srli_epi16'.
// Requires AVX512BW.
func MaskSrli16(src M256i, k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskSrli16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskSrli16(src [32]byte, k uint16, a [32]byte, imm8 int) [32]byte


// MaskzSrli16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_maskz_srli_epi16'.
// Requires AVX512BW.
func MaskzSrli16(k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskzSrli16(uint16(k), [32]byte(a), imm8))
}

func maskzSrli16(k uint16, a [32]byte, imm8 int) [32]byte


// Srli16: Shift packed 16-bit integers in 'a' right by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_srli_epi16'.
// Requires AVX2.
func Srli16(a M256i, imm8 int) M256i {
	return M256i(srli16([32]byte(a), imm8))
}

func srli16(a [32]byte, imm8 int) [32]byte


// MaskSrli32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srli_epi32'.
// Requires AVX512F.
func MaskSrli32(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrli32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrli32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrli32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrli32(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrli32(uint8(k), [32]byte(a), imm8))
}

func maskzSrli32(k uint8, a [32]byte, imm8 uint32) [32]byte


// Srli32: Shift packed 32-bit integers in 'a' right by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_srli_epi32'.
// Requires AVX2.
func Srli32(a M256i, imm8 int) M256i {
	return M256i(srli32([32]byte(a), imm8))
}

func srli32(a [32]byte, imm8 int) [32]byte


// MaskSrli64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrli64(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrli64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrli64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrli64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrli64(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrli64(uint8(k), [32]byte(a), imm8))
}

func maskzSrli64(k uint8, a [32]byte, imm8 uint32) [32]byte


// Srli64: Shift packed 64-bit integers in 'a' right by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_srli_epi64'.
// Requires AVX2.
func Srli64(a M256i, imm8 int) M256i {
	return M256i(srli64([32]byte(a), imm8))
}

func srli64(a [32]byte, imm8 int) [32]byte


// SrliSi256: Shift 128-bit lanes in 'a' right by 'imm8' bytes while shifting
// in zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] >> (tmp*8)
//		dst[255:128] := a[255:128] >> (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLDQ'. Intrinsic: '_mm256_srli_si256'.
// Requires AVX2.
func SrliSi256(a M256i, imm8 int) M256i {
	return M256i(srliSi256([32]byte(a), imm8))
}

func srliSi256(a [32]byte, imm8 int) [32]byte


// MaskSrlv16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+63:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm256_mask_srlv_epi16'.
// Requires AVX512BW.
func MaskSrlv16(src M256i, k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskSrlv16([32]byte(src), uint16(k), [32]byte(a), [32]byte(count)))
}

func maskSrlv16(src [32]byte, k uint16, a [32]byte, count [32]byte) [32]byte


// MaskzSrlv16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm256_maskz_srlv_epi16'.
// Requires AVX512BW.
func MaskzSrlv16(k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskzSrlv16(uint16(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlv16(k uint16, a [32]byte, count [32]byte) [32]byte


// Srlv16: Shift packed 16-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm256_srlv_epi16'.
// Requires AVX512BW.
func Srlv16(a M256i, count M256i) M256i {
	return M256i(srlv16([32]byte(a), [32]byte(count)))
}

func srlv16(a [32]byte, count [32]byte) [32]byte


// MaskSrlv32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_mask_srlv_epi32'.
// Requires AVX512F.
func MaskSrlv32(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrlv32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrlv32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrlv32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlv32(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrlv32(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlv32(k uint8, a [32]byte, count [32]byte) [32]byte


// Srlv32: Shift packed 32-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_srlv_epi32'.
// Requires AVX2.
func Srlv32(a M256i, count M256i) M256i {
	return M256i(srlv32([32]byte(a), [32]byte(count)))
}

func srlv32(a [32]byte, count [32]byte) [32]byte


// MaskSrlv64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlv64(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrlv64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrlv64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrlv64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlv64(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrlv64(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlv64(k uint8, a [32]byte, count [32]byte) [32]byte


// Srlv64: Shift packed 64-bit integers in 'a' right by the amount specified by
// the corresponding element in 'count' while shifting in zeros, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_srlv_epi64'.
// Requires AVX2.
func Srlv64(a M256i, count M256i) M256i {
	return M256i(srlv64([32]byte(a), [32]byte(count)))
}

func srlv64(a [32]byte, count [32]byte) [32]byte


// MaskStore32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_store_epi32'.
// Requires AVX512F.
func MaskStore32(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStore32(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStore32(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStore64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_store_epi64'.
// Requires AVX512F.
func MaskStore64(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStore64(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStore64(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_store_pd'.
// Requires AVX512F.
func MaskStorePd(mem_addr uintptr, k Mmask8, a M256d)  {
	maskStorePd(uintptr(mem_addr), uint8(k), [4]float64(a))
}

func maskStorePd(mem_addr uintptr, k uint8, a [4]float64) 


// StorePd: Store 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_store_pd'.
// Requires AVX.
func StorePd(mem_addr float64, a M256d)  {
	storePd(mem_addr, [4]float64(a))
}

func storePd(mem_addr float64, a [4]float64) 


// MaskStorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_store_ps'.
// Requires AVX512F.
func MaskStorePs(mem_addr uintptr, k Mmask8, a M256)  {
	maskStorePs(uintptr(mem_addr), uint8(k), [8]float32(a))
}

func maskStorePs(mem_addr uintptr, k uint8, a [8]float32) 


// StorePs: Store 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_store_ps'.
// Requires AVX.
func StorePs(mem_addr float32, a M256)  {
	storePs(mem_addr, [8]float32(a))
}

func storePs(mem_addr float32, a [8]float32) 


// StoreSi256: Store 256-bits of integer data from 'a' into memory.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVDQA'. Intrinsic: '_mm256_store_si256'.
// Requires AVX.
func StoreSi256(mem_addr M256i, a M256i)  {
	storeSi256([32]byte(mem_addr), [32]byte(a))
}

func storeSi256(mem_addr [32]byte, a [32]byte) 


// MaskStoreu16: Store packed 16-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_mask_storeu_epi16'.
// Requires AVX512BW.
func MaskStoreu16(mem_addr uintptr, k Mmask16, a M256i)  {
	maskStoreu16(uintptr(mem_addr), uint16(k), [32]byte(a))
}

func maskStoreu16(mem_addr uintptr, k uint16, a [32]byte) 


// MaskStoreu32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_mask_storeu_epi32'.
// Requires AVX512F.
func MaskStoreu32(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreu32(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreu32(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreu64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_mask_storeu_epi64'.
// Requires AVX512F.
func MaskStoreu64(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreu64(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreu64(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreu8: Store packed 8-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_mask_storeu_epi8'.
// Requires AVX512BW.
func MaskStoreu8(mem_addr uintptr, k Mmask32, a M256i)  {
	maskStoreu8(uintptr(mem_addr), uint32(k), [32]byte(a))
}

func maskStoreu8(mem_addr uintptr, k uint32, a [32]byte) 


// MaskStoreuPd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_mask_storeu_pd'.
// Requires AVX512F.
func MaskStoreuPd(mem_addr uintptr, k Mmask8, a M256d)  {
	maskStoreuPd(uintptr(mem_addr), uint8(k), [4]float64(a))
}

func maskStoreuPd(mem_addr uintptr, k uint8, a [4]float64) 


// StoreuPd: Store 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_storeu_pd'.
// Requires AVX.
func StoreuPd(mem_addr float64, a M256d)  {
	storeuPd(mem_addr, [4]float64(a))
}

func storeuPd(mem_addr float64, a [4]float64) 


// MaskStoreuPs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_mask_storeu_ps'.
// Requires AVX512F.
func MaskStoreuPs(mem_addr uintptr, k Mmask8, a M256)  {
	maskStoreuPs(uintptr(mem_addr), uint8(k), [8]float32(a))
}

func maskStoreuPs(mem_addr uintptr, k uint8, a [8]float32) 


// StoreuPs: Store 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_storeu_ps'.
// Requires AVX.
func StoreuPs(mem_addr float32, a M256)  {
	storeuPs(mem_addr, [8]float32(a))
}

func storeuPs(mem_addr float32, a [8]float32) 


// StoreuSi256: Store 256-bits of integer data from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVDQU'. Intrinsic: '_mm256_storeu_si256'.
// Requires AVX.
func StoreuSi256(mem_addr M256i, a M256i)  {
	storeuSi256([32]byte(mem_addr), [32]byte(a))
}

func storeuSi256(mem_addr [32]byte, a [32]byte) 


// Storeu2M128: Store the high and low 128-bit halves (each composed of 4
// packed single-precision (32-bit) floating-point elements) from 'a' into
// memory two different 128-bit locations.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		MEM[loaddr+127:loaddr] := a[127:0]
//		MEM[hiaddr+127:hiaddr] := a[255:128]
//
// Instruction: '...'. Intrinsic: '_mm256_storeu2_m128'.
// Requires AVX.
func Storeu2M128(hiaddr float32, loaddr float32, a M256)  {
	storeu2M128(hiaddr, loaddr, [8]float32(a))
}

func storeu2M128(hiaddr float32, loaddr float32, a [8]float32) 


// Storeu2M128d: Store the high and low 128-bit halves (each composed of 2
// packed double-precision (64-bit) floating-point elements) from 'a' into
// memory two different 128-bit locations.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		MEM[loaddr+127:loaddr] := a[127:0]
//		MEM[hiaddr+127:hiaddr] := a[255:128]
//
// Instruction: '...'. Intrinsic: '_mm256_storeu2_m128d'.
// Requires AVX.
func Storeu2M128d(hiaddr float64, loaddr float64, a M256d)  {
	storeu2M128d(hiaddr, loaddr, [4]float64(a))
}

func storeu2M128d(hiaddr float64, loaddr float64, a [4]float64) 


// Storeu2M128i: Store the high and low 128-bit halves (each composed of
// integer data) from 'a' into memory two different 128-bit locations.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		MEM[loaddr+127:loaddr] := a[127:0]
//		MEM[hiaddr+127:hiaddr] := a[255:128]
//
// Instruction: '...'. Intrinsic: '_mm256_storeu2_m128i'.
// Requires AVX.
func Storeu2M128i(hiaddr M128i, loaddr M128i, a M256i)  {
	storeu2M128i([16]byte(hiaddr), [16]byte(loaddr), [32]byte(a))
}

func storeu2M128i(hiaddr [16]byte, loaddr [16]byte, a [32]byte) 


// StreamLoadSi256: Load 256-bits of integer data from memory into 'dst' using
// a non-temporal memory hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVNTDQA'. Intrinsic: '_mm256_stream_load_si256'.
// Requires AVX2.
func StreamLoadSi256(mem_addr M256iConst) M256i {
	return M256i(streamLoadSi256(mem_addr))
}

func streamLoadSi256(mem_addr M256iConst) [32]byte


// StreamPd: Store 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVNTPD'. Intrinsic: '_mm256_stream_pd'.
// Requires AVX.
func StreamPd(mem_addr float64, a M256d)  {
	streamPd(mem_addr, [4]float64(a))
}

func streamPd(mem_addr float64, a [4]float64) 


// StreamPs: Store 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVNTPS'. Intrinsic: '_mm256_stream_ps'.
// Requires AVX.
func StreamPs(mem_addr float32, a M256)  {
	streamPs(mem_addr, [8]float32(a))
}

func streamPs(mem_addr float32, a [8]float32) 


// StreamSi256: Store 256-bits of integer data from 'a' into memory using a
// non-temporal memory hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVNTDQ'. Intrinsic: '_mm256_stream_si256'.
// Requires AVX.
func StreamSi256(mem_addr M256i, a M256i)  {
	streamSi256([32]byte(mem_addr), [32]byte(a))
}

func streamSi256(mem_addr [32]byte, a [32]byte) 


// MaskSub16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm256_mask_sub_epi16'.
// Requires AVX512BW.
func MaskSub16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskSub16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskSub16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzSub16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm256_maskz_sub_epi16'.
// Requires AVX512BW.
func MaskzSub16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzSub16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzSub16(k uint16, a [32]byte, b [32]byte) [32]byte


// Sub16: Subtract packed 16-bit integers in 'b' from packed 16-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[i+15:i] - b[i+15:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm256_sub_epi16'.
// Requires AVX2.
func Sub16(a M256i, b M256i) M256i {
	return M256i(sub16([32]byte(a), [32]byte(b)))
}

func sub16(a [32]byte, b [32]byte) [32]byte


// MaskSub32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_mask_sub_epi32'.
// Requires AVX512F.
func MaskSub32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskSub32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskSub32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzSub32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSub32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzSub32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzSub32(k uint8, a [32]byte, b [32]byte) [32]byte


// Sub32: Subtract packed 32-bit integers in 'b' from packed 32-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_sub_epi32'.
// Requires AVX2.
func Sub32(a M256i, b M256i) M256i {
	return M256i(sub32([32]byte(a), [32]byte(b)))
}

func sub32(a [32]byte, b [32]byte) [32]byte


// MaskSub64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_mask_sub_epi64'.
// Requires AVX512F.
func MaskSub64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskSub64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskSub64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzSub64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSub64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzSub64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzSub64(k uint8, a [32]byte, b [32]byte) [32]byte


// Sub64: Subtract packed 64-bit integers in 'b' from packed 64-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_sub_epi64'.
// Requires AVX2.
func Sub64(a M256i, b M256i) M256i {
	return M256i(sub64([32]byte(a), [32]byte(b)))
}

func sub64(a [32]byte, b [32]byte) [32]byte


// MaskSub8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm256_mask_sub_epi8'.
// Requires AVX512BW.
func MaskSub8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskSub8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskSub8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzSub8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm256_maskz_sub_epi8'.
// Requires AVX512BW.
func MaskzSub8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzSub8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzSub8(k uint32, a [32]byte, b [32]byte) [32]byte


// Sub8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[i+7:i] - b[i+7:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm256_sub_epi8'.
// Requires AVX2.
func Sub8(a M256i, b M256i) M256i {
	return M256i(sub8([32]byte(a), [32]byte(b)))
}

func sub8(a [32]byte, b [32]byte) [32]byte


// MaskSubPd: Subtract packed double-precision (64-bit) floating-point elements
// in 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_mask_sub_pd'.
// Requires AVX512F.
func MaskSubPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskSubPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskSubPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzSubPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzSubPd(k uint8, a [4]float64, b [4]float64) [4]float64


// SubPd: Subtract packed double-precision (64-bit) floating-point elements in
// 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_sub_pd'.
// Requires AVX.
func SubPd(a M256d, b M256d) M256d {
	return M256d(subPd([4]float64(a), [4]float64(b)))
}

func subPd(a [4]float64, b [4]float64) [4]float64


// MaskSubPs: Subtract packed single-precision (32-bit) floating-point elements
// in 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_mask_sub_ps'.
// Requires AVX512F.
func MaskSubPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskSubPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskSubPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzSubPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzSubPs(k uint8, a [8]float32, b [8]float32) [8]float32


// SubPs: Subtract packed single-precision (32-bit) floating-point elements in
// 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_sub_ps'.
// Requires AVX.
func SubPs(a M256, b M256) M256 {
	return M256(subPs([8]float32(a), [8]float32(b)))
}

func subPs(a [8]float32, b [8]float32) [8]float32


// MaskSubs16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm256_mask_subs_epi16'.
// Requires AVX512BW.
func MaskSubs16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskSubs16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskSubs16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzSubs16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm256_maskz_subs_epi16'.
// Requires AVX512BW.
func MaskzSubs16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzSubs16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzSubs16(k uint16, a [32]byte, b [32]byte) [32]byte


// Subs16: Subtract packed 16-bit integers in 'b' from packed 16-bit integers
// in 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm256_subs_epi16'.
// Requires AVX2.
func Subs16(a M256i, b M256i) M256i {
	return M256i(subs16([32]byte(a), [32]byte(b)))
}

func subs16(a [32]byte, b [32]byte) [32]byte


// MaskSubs8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a' using saturation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm256_mask_subs_epi8'.
// Requires AVX512BW.
func MaskSubs8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskSubs8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskSubs8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzSubs8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a' using saturation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm256_maskz_subs_epi8'.
// Requires AVX512BW.
func MaskzSubs8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzSubs8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzSubs8(k uint32, a [32]byte, b [32]byte) [32]byte


// Subs8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers in
// 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm256_subs_epi8'.
// Requires AVX2.
func Subs8(a M256i, b M256i) M256i {
	return M256i(subs8([32]byte(a), [32]byte(b)))
}

func subs8(a [32]byte, b [32]byte) [32]byte


// MaskSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm256_mask_subs_epu16'.
// Requires AVX512BW.
func MaskSubsEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskSubsEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskSubsEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm256_maskz_subs_epu16'.
// Requires AVX512BW.
func MaskzSubsEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzSubsEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzSubsEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// SubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm256_subs_epu16'.
// Requires AVX2.
func SubsEpu16(a M256i, b M256i) M256i {
	return M256i(subsEpu16([32]byte(a), [32]byte(b)))
}

func subsEpu16(a [32]byte, b [32]byte) [32]byte


// MaskSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm256_mask_subs_epu8'.
// Requires AVX512BW.
func MaskSubsEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskSubsEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskSubsEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm256_maskz_subs_epu8'.
// Requires AVX512BW.
func MaskzSubsEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzSubsEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzSubsEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// SubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm256_subs_epu8'.
// Requires AVX2.
func SubsEpu8(a M256i, b M256i) M256i {
	return M256i(subsEpu8([32]byte(a), [32]byte(b)))
}

func subsEpu8(a [32]byte, b [32]byte) [32]byte


// SvmlCeilPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_ceil_pd'.
// Requires AVX.
func SvmlCeilPd(a M256d) M256d {
	return M256d(svmlCeilPd([4]float64(a)))
}

func svmlCeilPd(a [4]float64) [4]float64


// SvmlCeilPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_ceil_ps'.
// Requires AVX.
func SvmlCeilPs(a M256) M256 {
	return M256(svmlCeilPs([8]float32(a)))
}

func svmlCeilPs(a [8]float32) [8]float32


// SvmlFloorPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_floor_pd'.
// Requires AVX.
func SvmlFloorPd(a M256d) M256d {
	return M256d(svmlFloorPd([4]float64(a)))
}

func svmlFloorPd(a [4]float64) [4]float64


// SvmlFloorPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_floor_ps'.
// Requires AVX.
func SvmlFloorPs(a M256) M256 {
	return M256(svmlFloorPs([8]float32(a)))
}

func svmlFloorPs(a [8]float32) [8]float32


// SvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_round_pd'.
// Requires AVX.
func SvmlRoundPd(a M256d) M256d {
	return M256d(svmlRoundPd([4]float64(a)))
}

func svmlRoundPd(a [4]float64) [4]float64


// SvmlRoundPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_round_ps'.
// Requires AVX.
func SvmlRoundPs(a M256) M256 {
	return M256(svmlRoundPs([8]float32(a)))
}

func svmlRoundPs(a [8]float32) [8]float32


// SvmlSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. Note that
// this intrinsic is less efficient than '_mm_sqrt_pd'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_sqrt_pd'.
// Requires AVX.
func SvmlSqrtPd(a M256d) M256d {
	return M256d(svmlSqrtPd([4]float64(a)))
}

func svmlSqrtPd(a [4]float64) [4]float64


// SvmlSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. Note that
// this intrinsic is less efficient than '_mm_sqrt_ps'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_sqrt_ps'.
// Requires AVX.
func SvmlSqrtPs(a M256) M256 {
	return M256(svmlSqrtPs([8]float32(a)))
}

func svmlSqrtPs(a [8]float32) [8]float32


// TanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tan_pd'.
// Requires AVX.
func TanPd(a M256d) M256d {
	return M256d(tanPd([4]float64(a)))
}

func tanPd(a [4]float64) [4]float64


// TanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tan_ps'.
// Requires AVX.
func TanPs(a M256) M256 {
	return M256(tanPs([8]float32(a)))
}

func tanPs(a [8]float32) [8]float32


// TandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TAND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tand_pd'.
// Requires AVX.
func TandPd(a M256d) M256d {
	return M256d(tandPd([4]float64(a)))
}

func tandPd(a [4]float64) [4]float64


// TandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TAND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tand_ps'.
// Requires AVX.
func TandPs(a M256) M256 {
	return M256(tandPs([8]float32(a)))
}

func tandPs(a [8]float32) [8]float32


// TanhPd: Compute the hyperbolic tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tanh_pd'.
// Requires AVX.
func TanhPd(a M256d) M256d {
	return M256d(tanhPd([4]float64(a)))
}

func tanhPd(a [4]float64) [4]float64


// TanhPs: Compute the hyperbolic tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tanh_ps'.
// Requires AVX.
func TanhPs(a M256) M256 {
	return M256(tanhPs([8]float32(a)))
}

func tanhPs(a [8]float32) [8]float32


// MaskTernarylogic32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogic32(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskTernarylogic32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskTernarylogic32(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzTernarylogic32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogic32(k Mmask8, a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(maskzTernarylogic32(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func maskzTernarylogic32(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// Ternarylogic32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_ternarylogic_epi32'.
// Requires AVX512F.
func Ternarylogic32(a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(ternarylogic32([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func ternarylogic32(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// MaskTernarylogic64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogic64(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskTernarylogic64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskTernarylogic64(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzTernarylogic64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogic64(k Mmask8, a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(maskzTernarylogic64(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func maskzTernarylogic64(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// Ternarylogic64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_ternarylogic_epi64'.
// Requires AVX512F.
func Ternarylogic64(a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(ternarylogic64([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func ternarylogic64(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// MaskTest16Mask: Compute the bitwise AND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm256_mask_test_epi16_mask'.
// Requires AVX512BW.
func MaskTest16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskTest16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskTest16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Test16Mask: Compute the bitwise AND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm256_test_epi16_mask'.
// Requires AVX512BW.
func Test16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(test16Mask([32]byte(a), [32]byte(b)))
}

func test16Mask(a [32]byte, b [32]byte) uint16


// MaskTest32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_mask_test_epi32_mask'.
// Requires AVX512F.
func MaskTest32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTest32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTest32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Test32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_test_epi32_mask'.
// Requires AVX512F.
func Test32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(test32Mask([32]byte(a), [32]byte(b)))
}

func test32Mask(a [32]byte, b [32]byte) uint8


// MaskTest64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTest64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTest64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTest64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Test64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_test_epi64_mask'.
// Requires AVX512F.
func Test64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(test64Mask([32]byte(a), [32]byte(b)))
}

func test64Mask(a [32]byte, b [32]byte) uint8


// MaskTest8Mask: Compute the bitwise AND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm256_mask_test_epi8_mask'.
// Requires AVX512BW.
func MaskTest8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskTest8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskTest8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// Test8Mask: Compute the bitwise AND of packed 8-bit integers in 'a' and 'b',
// producing intermediate 8-bit values, and set the corresponding bit in result
// mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm256_test_epi8_mask'.
// Requires AVX512BW.
func Test8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(test8Mask([32]byte(a), [32]byte(b)))
}

func test8Mask(a [32]byte, b [32]byte) uint32


// TestcPd: Compute the bitwise AND of 256 bits (representing double-precision
// (64-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 64-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 64-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm256_testc_pd'.
// Requires AVX.
func TestcPd(a M256d, b M256d) int {
	return int(testcPd([4]float64(a), [4]float64(b)))
}

func testcPd(a [4]float64, b [4]float64) int


// TestcPs: Compute the bitwise AND of 256 bits (representing single-precision
// (32-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 32-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 32-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm256_testc_ps'.
// Requires AVX.
func TestcPs(a M256, b M256) int {
	return int(testcPs([8]float32(a), [8]float32(b)))
}

func testcPs(a [8]float32, b [8]float32) int


// TestcSi256: Compute the bitwise AND of 256 bits (representing integer data)
// in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set 'ZF'
// to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if the
// result is zero, otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		IF (a[255:0] AND b[255:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[255:0] AND NOT b[255:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VPTEST'. Intrinsic: '_mm256_testc_si256'.
// Requires AVX.
func TestcSi256(a M256i, b M256i) int {
	return int(testcSi256([32]byte(a), [32]byte(b)))
}

func testcSi256(a [32]byte, b [32]byte) int


// MaskTestn16Mask: Compute the bitwise NAND of packed 16-bit integers in 'a'
// and 'b', producing intermediate 16-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm256_mask_testn_epi16_mask'.
// Requires AVX512BW.
func MaskTestn16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskTestn16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskTestn16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// Testn16Mask: Compute the bitwise NAND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm256_testn_epi16_mask'.
// Requires AVX512BW.
func Testn16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(testn16Mask([32]byte(a), [32]byte(b)))
}

func testn16Mask(a [32]byte, b [32]byte) uint16


// MaskTestn32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestn32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestn32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestn32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Testn32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_testn_epi32_mask'.
// Requires AVX512F.
func Testn32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(testn32Mask([32]byte(a), [32]byte(b)))
}

func testn32Mask(a [32]byte, b [32]byte) uint8


// MaskTestn64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestn64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestn64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestn64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// Testn64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_testn_epi64_mask'.
// Requires AVX512F.
func Testn64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(testn64Mask([32]byte(a), [32]byte(b)))
}

func testn64Mask(a [32]byte, b [32]byte) uint8


// MaskTestn8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm256_mask_testn_epi8_mask'.
// Requires AVX512BW.
func MaskTestn8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskTestn8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskTestn8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// Testn8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm256_testn_epi8_mask'.
// Requires AVX512BW.
func Testn8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(testn8Mask([32]byte(a), [32]byte(b)))
}

func testn8Mask(a [32]byte, b [32]byte) uint32


// TestnzcPd: Compute the bitwise AND of 256 bits (representing
// double-precision (64-bit) floating-point elements) in 'a' and 'b', producing
// an intermediate 256-bit value, and set 'ZF' to 1 if the sign bit of each
// 64-bit element in the intermediate value is zero, otherwise set 'ZF' to 0.
// Compute the bitwise AND NOT of 'a' and 'b', producing an intermediate value,
// and set 'CF' to 1 if the sign bit of each 64-bit element in the intermediate
// value is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and 'CF'
// values are zero, otherwise return 0. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm256_testnzc_pd'.
// Requires AVX.
func TestnzcPd(a M256d, b M256d) int {
	return int(testnzcPd([4]float64(a), [4]float64(b)))
}

func testnzcPd(a [4]float64, b [4]float64) int


// TestnzcPs: Compute the bitwise AND of 256 bits (representing
// single-precision (32-bit) floating-point elements) in 'a' and 'b', producing
// an intermediate 256-bit value, and set 'ZF' to 1 if the sign bit of each
// 32-bit element in the intermediate value is zero, otherwise set 'ZF' to 0.
// Compute the bitwise AND NOT of 'a' and 'b', producing an intermediate value,
// and set 'CF' to 1 if the sign bit of each 32-bit element in the intermediate
// value is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and 'CF'
// values are zero, otherwise return 0. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255]  == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255]  == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm256_testnzc_ps'.
// Requires AVX.
func TestnzcPs(a M256, b M256) int {
	return int(testnzcPs([8]float32(a), [8]float32(b)))
}

func testnzcPs(a [8]float32, b [8]float32) int


// TestnzcSi256: Compute the bitwise AND of 256 bits (representing integer
// data) in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set
// 'ZF' to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if
// the result is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and
// 'CF' values are zero, otherwise return 0. 
//
//		IF (a[255:0] AND b[255:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[255:0] AND NOT b[255:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VPTEST'. Intrinsic: '_mm256_testnzc_si256'.
// Requires AVX.
func TestnzcSi256(a M256i, b M256i) int {
	return int(testnzcSi256([32]byte(a), [32]byte(b)))
}

func testnzcSi256(a [32]byte, b [32]byte) int


// TestzPd: Compute the bitwise AND of 256 bits (representing double-precision
// (64-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 64-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 64-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm256_testz_pd'.
// Requires AVX.
func TestzPd(a M256d, b M256d) int {
	return int(testzPd([4]float64(a), [4]float64(b)))
}

func testzPd(a [4]float64, b [4]float64) int


// TestzPs: Compute the bitwise AND of 256 bits (representing single-precision
// (32-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 32-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 32-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm256_testz_ps'.
// Requires AVX.
func TestzPs(a M256, b M256) int {
	return int(testzPs([8]float32(a), [8]float32(b)))
}

func testzPs(a [8]float32, b [8]float32) int


// TestzSi256: Compute the bitwise AND of 256 bits (representing integer data)
// in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set 'ZF'
// to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if the
// result is zero, otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		IF (a[255:0] AND b[255:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[255:0] AND NOT b[255:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VPTEST'. Intrinsic: '_mm256_testz_si256'.
// Requires AVX.
func TestzSi256(a M256i, b M256i) int {
	return int(testzSi256([32]byte(a), [32]byte(b)))
}

func testzSi256(a [32]byte, b [32]byte) int


// TruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst'. This intrinsic may generate the
// 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TRUNCATE(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_trunc_pd'.
// Requires AVX.
func TruncPd(a M256d) M256d {
	return M256d(truncPd([4]float64(a)))
}

func truncPd(a [4]float64) [4]float64


// TruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst'. This intrinsic may generate the
// 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TRUNCATE(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_trunc_ps'.
// Requires AVX.
func TruncPs(a M256) M256 {
	return M256(truncPs([8]float32(a)))
}

func truncPs(a [8]float32) [8]float32


// Udiv32: Divide packed unsigned 32-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_udiv_epi32'.
// Requires AVX.
func Udiv32(a M256i, b M256i) M256i {
	return M256i(udiv32([32]byte(a), [32]byte(b)))
}

func udiv32(a [32]byte, b [32]byte) [32]byte


// Udivrem32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', store the truncated results in 'dst', and store the remainders as
// packed unsigned 32-bit integers into memory at 'mem_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_udivrem_epi32'.
// Requires AVX.
func Udivrem32(mem_addr M256i, a M256i, b M256i) M256i {
	return M256i(udivrem32([32]byte(mem_addr), [32]byte(a), [32]byte(b)))
}

func udivrem32(mem_addr [32]byte, a [32]byte, b [32]byte) [32]byte


// UndefinedPd: Return vector of type __m256d with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_undefined_pd'.
// Requires AVX.
func UndefinedPd() M256d {
	return M256d(undefinedPd())
}

func undefinedPd() [4]float64


// UndefinedPs: Return vector of type __m256 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_undefined_ps'.
// Requires AVX.
func UndefinedPs() M256 {
	return M256(undefinedPs())
}

func undefinedPs() [8]float32


// UndefinedSi256: Return vector of type __m256i with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_undefined_si256'.
// Requires AVX.
func UndefinedSi256() M256i {
	return M256i(undefinedSi256())
}

func undefinedSi256() [32]byte


// MaskUnpackhi16: Unpack and interleave 16-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm256_mask_unpackhi_epi16'.
// Requires AVX512BW.
func MaskUnpackhi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskUnpackhi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhi16: Unpack and interleave 16-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm256_maskz_unpackhi_epi16'.
// Requires AVX512BW.
func MaskzUnpackhi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhi16(k uint16, a [32]byte, b [32]byte) [32]byte


// Unpackhi16: Unpack and interleave 16-bit integers from the high half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm256_unpackhi_epi16'.
// Requires AVX2.
func Unpackhi16(a M256i, b M256i) M256i {
	return M256i(unpackhi16([32]byte(a), [32]byte(b)))
}

func unpackhi16(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhi32: Unpack and interleave 32-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackhi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhi32: Unpack and interleave 32-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhi32(k uint8, a [32]byte, b [32]byte) [32]byte


// Unpackhi32: Unpack and interleave 32-bit integers from the high half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_unpackhi_epi32'.
// Requires AVX2.
func Unpackhi32(a M256i, b M256i) M256i {
	return M256i(unpackhi32([32]byte(a), [32]byte(b)))
}

func unpackhi32(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhi64: Unpack and interleave 64-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackhi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhi64: Unpack and interleave 64-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhi64(k uint8, a [32]byte, b [32]byte) [32]byte


// Unpackhi64: Unpack and interleave 64-bit integers from the high half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_unpackhi_epi64'.
// Requires AVX2.
func Unpackhi64(a M256i, b M256i) M256i {
	return M256i(unpackhi64([32]byte(a), [32]byte(b)))
}

func unpackhi64(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhi8: Unpack and interleave 8-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm256_mask_unpackhi_epi8'.
// Requires AVX512BW.
func MaskUnpackhi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskUnpackhi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhi8: Unpack and interleave 8-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm256_maskz_unpackhi_epi8'.
// Requires AVX512BW.
func MaskzUnpackhi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhi8(k uint32, a [32]byte, b [32]byte) [32]byte


// Unpackhi8: Unpack and interleave 8-bit integers from the high half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm256_unpackhi_epi8'.
// Requires AVX2.
func Unpackhi8(a M256i, b M256i) M256i {
	return M256i(unpackhi8([32]byte(a), [32]byte(b)))
}

func unpackhi8(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskUnpackhiPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskUnpackhiPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzUnpackhiPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzUnpackhiPd(k uint8, a [4]float64, b [4]float64) [4]float64


// UnpackhiPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_unpackhi_pd'.
// Requires AVX.
func UnpackhiPd(a M256d, b M256d) M256d {
	return M256d(unpackhiPd([4]float64(a), [4]float64(b)))
}

func unpackhiPd(a [4]float64, b [4]float64) [4]float64


// MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskUnpackhiPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskUnpackhiPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzUnpackhiPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzUnpackhiPs(k uint8, a [8]float32, b [8]float32) [8]float32


// UnpackhiPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_unpackhi_ps'.
// Requires AVX.
func UnpackhiPs(a M256, b M256) M256 {
	return M256(unpackhiPs([8]float32(a), [8]float32(b)))
}

func unpackhiPs(a [8]float32, b [8]float32) [8]float32


// MaskUnpacklo16: Unpack and interleave 16-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm256_mask_unpacklo_epi16'.
// Requires AVX512BW.
func MaskUnpacklo16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskUnpacklo16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskUnpacklo16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzUnpacklo16: Unpack and interleave 16-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm256_maskz_unpacklo_epi16'.
// Requires AVX512BW.
func MaskzUnpacklo16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzUnpacklo16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpacklo16(k uint16, a [32]byte, b [32]byte) [32]byte


// Unpacklo16: Unpack and interleave 16-bit integers from the low half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm256_unpacklo_epi16'.
// Requires AVX2.
func Unpacklo16(a M256i, b M256i) M256i {
	return M256i(unpacklo16([32]byte(a), [32]byte(b)))
}

func unpacklo16(a [32]byte, b [32]byte) [32]byte


// MaskUnpacklo32: Unpack and interleave 32-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpacklo32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpacklo32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpacklo32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpacklo32: Unpack and interleave 32-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpacklo32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpacklo32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpacklo32(k uint8, a [32]byte, b [32]byte) [32]byte


// Unpacklo32: Unpack and interleave 32-bit integers from the low half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_unpacklo_epi32'.
// Requires AVX2.
func Unpacklo32(a M256i, b M256i) M256i {
	return M256i(unpacklo32([32]byte(a), [32]byte(b)))
}

func unpacklo32(a [32]byte, b [32]byte) [32]byte


// MaskUnpacklo64: Unpack and interleave 64-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpacklo64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpacklo64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpacklo64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpacklo64: Unpack and interleave 64-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpacklo64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpacklo64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpacklo64(k uint8, a [32]byte, b [32]byte) [32]byte


// Unpacklo64: Unpack and interleave 64-bit integers from the low half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_unpacklo_epi64'.
// Requires AVX2.
func Unpacklo64(a M256i, b M256i) M256i {
	return M256i(unpacklo64([32]byte(a), [32]byte(b)))
}

func unpacklo64(a [32]byte, b [32]byte) [32]byte


// MaskUnpacklo8: Unpack and interleave 8-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm256_mask_unpacklo_epi8'.
// Requires AVX512BW.
func MaskUnpacklo8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskUnpacklo8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskUnpacklo8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzUnpacklo8: Unpack and interleave 8-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm256_maskz_unpacklo_epi8'.
// Requires AVX512BW.
func MaskzUnpacklo8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzUnpacklo8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpacklo8(k uint32, a [32]byte, b [32]byte) [32]byte


// Unpacklo8: Unpack and interleave 8-bit integers from the low half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm256_unpacklo_epi8'.
// Requires AVX2.
func Unpacklo8(a M256i, b M256i) M256i {
	return M256i(unpacklo8([32]byte(a), [32]byte(b)))
}

func unpacklo8(a [32]byte, b [32]byte) [32]byte


// MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskUnpackloPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskUnpackloPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzUnpackloPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzUnpackloPd(k uint8, a [4]float64, b [4]float64) [4]float64


// UnpackloPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_unpacklo_pd'.
// Requires AVX.
func UnpackloPd(a M256d, b M256d) M256d {
	return M256d(unpackloPd([4]float64(a), [4]float64(b)))
}

func unpackloPd(a [4]float64, b [4]float64) [4]float64


// MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskUnpackloPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskUnpackloPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzUnpackloPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzUnpackloPs(k uint8, a [8]float32, b [8]float32) [8]float32


// UnpackloPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_unpacklo_ps'.
// Requires AVX.
func UnpackloPs(a M256, b M256) M256 {
	return M256(unpackloPs([8]float32(a), [8]float32(b)))
}

func unpackloPs(a [8]float32, b [8]float32) [8]float32


// Urem32: Divide packed unsigned 32-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed unsigned 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_urem_epi32'.
// Requires AVX.
func Urem32(a M256i, b M256i) M256i {
	return M256i(urem32([32]byte(a), [32]byte(b)))
}

func urem32(a [32]byte, b [32]byte) [32]byte


// MaskXor32: Compute the bitwise XOR of packed 32-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_mask_xor_epi32'.
// Requires AVX512F.
func MaskXor32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskXor32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskXor32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXor32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXor32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzXor32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzXor32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskXor64: Compute the bitwise XOR of packed 64-bit integers in 'a' and 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_mask_xor_epi64'.
// Requires AVX512F.
func MaskXor64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskXor64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskXor64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXor64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXor64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzXor64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzXor64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_mask_xor_pd'.
// Requires AVX512DQ.
func MaskXorPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskXorPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskXorPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_maskz_xor_pd'.
// Requires AVX512DQ.
func MaskzXorPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzXorPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzXorPd(k uint8, a [4]float64, b [4]float64) [4]float64


// XorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_xor_pd'.
// Requires AVX.
func XorPd(a M256d, b M256d) M256d {
	return M256d(xorPd([4]float64(a), [4]float64(b)))
}

func xorPd(a [4]float64, b [4]float64) [4]float64


// MaskXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_mask_xor_ps'.
// Requires AVX512DQ.
func MaskXorPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskXorPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskXorPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_maskz_xor_ps'.
// Requires AVX512DQ.
func MaskzXorPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzXorPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzXorPs(k uint8, a [8]float32, b [8]float32) [8]float32


// XorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_xor_ps'.
// Requires AVX.
func XorPs(a M256, b M256) M256 {
	return M256(xorPs([8]float32(a), [8]float32(b)))
}

func xorPs(a [8]float32, b [8]float32) [8]float32


// XorSi256: Compute the bitwise XOR of 256 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := (a[255:0] XOR b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPXOR'. Intrinsic: '_mm256_xor_si256'.
// Requires AVX2.
func XorSi256(a M256i, b M256i) M256i {
	return M256i(xorSi256([32]byte(a), [32]byte(b)))
}

func xorSi256(a [32]byte, b [32]byte) [32]byte


// Zeroall: Zero the contents of all XMM or YMM registers. 
//
//		YMM0[MAX:0] := 0
//		YMM1[MAX:0] := 0
//		YMM2[MAX:0] := 0
//		YMM3[MAX:0] := 0
//		YMM4[MAX:0] := 0
//		YMM5[MAX:0] := 0
//		YMM6[MAX:0] := 0
//		YMM7[MAX:0] := 0
//		IF 64-bit mode
//			YMM8[MAX:0] := 0
//			YMM9[MAX:0] := 0
//			YMM10[MAX:0] := 0
//			YMM11[MAX:0] := 0
//			YMM12[MAX:0] := 0
//			YMM13[MAX:0] := 0
//			YMM14[MAX:0] := 0
//			YMM15[MAX:0] := 0
//		FI
//
// Instruction: 'VZEROALL'. Intrinsic: '_mm256_zeroall'.
// Requires AVX.
func Zeroall()  {
	zeroall()
}

func zeroall() 


// Zeroupper: Zero the upper 128 bits of all YMM registers; the lower 128-bits
// of the registers are unmodified. 
//
//		YMM0[MAX:128] := 0
//		YMM1[MAX:128] := 0
//		YMM2[MAX:128] := 0
//		YMM3[MAX:128] := 0
//		YMM4[MAX:128] := 0
//		YMM5[MAX:128] := 0
//		YMM6[MAX:128] := 0
//		YMM7[MAX:128] := 0
//		IF 64-bit mode
//			YMM8[MAX:128] := 0
//			YMM9[MAX:128] := 0
//			YMM10[MAX:128] := 0
//			YMM11[MAX:128] := 0
//			YMM12[MAX:128] := 0
//			YMM13[MAX:128] := 0
//			YMM14[MAX:128] := 0
//			YMM15[MAX:128] := 0
//		FI
//
// Instruction: 'VZEROUPPER'. Intrinsic: '_mm256_zeroupper'.
// Requires AVX.
func Zeroupper()  {
	zeroupper()
}

func zeroupper() 

