package m256

import . "github.com/klauspost/intrinsics/x86"

// AbsEpi16: Compute the absolute value of packed 16-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := ABS(a[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm256_abs_epi16'.
// Requires AVX2.
func AbsEpi16(a M256i) M256i {
	return M256i(absEpi16([32]byte(a)))
}

func absEpi16(a [32]byte) [32]byte


// MaskAbsEpi16: Compute the absolute value of packed 16-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm256_mask_abs_epi16'.
// Requires AVX512BW.
func MaskAbsEpi16(src M256i, k Mmask16, a M256i) M256i {
	return M256i(maskAbsEpi16([32]byte(src), uint16(k), [32]byte(a)))
}

func maskAbsEpi16(src [32]byte, k uint16, a [32]byte) [32]byte


// MaskzAbsEpi16: Compute the absolute value of packed 16-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ABS(a[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSW'. Intrinsic: '_mm256_maskz_abs_epi16'.
// Requires AVX512BW.
func MaskzAbsEpi16(k Mmask16, a M256i) M256i {
	return M256i(maskzAbsEpi16(uint16(k), [32]byte(a)))
}

func maskzAbsEpi16(k uint16, a [32]byte) [32]byte


// AbsEpi32: Compute the absolute value of packed 32-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ABS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_abs_epi32'.
// Requires AVX2.
func AbsEpi32(a M256i) M256i {
	return M256i(absEpi32([32]byte(a)))
}

func absEpi32(a [32]byte) [32]byte


// MaskAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_mask_abs_epi32'.
// Requires AVX512F.
func MaskAbsEpi32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskAbsEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskAbsEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzAbsEpi32: Compute the absolute value of packed 32-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ABS(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSD'. Intrinsic: '_mm256_maskz_abs_epi32'.
// Requires AVX512F.
func MaskzAbsEpi32(k Mmask8, a M256i) M256i {
	return M256i(maskzAbsEpi32(uint8(k), [32]byte(a)))
}

func maskzAbsEpi32(k uint8, a [32]byte) [32]byte


// AbsEpi64: Compute the absolute value of packed 64-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ABS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_abs_epi64'.
// Requires AVX512F.
func AbsEpi64(a M256i) M256i {
	return M256i(absEpi64([32]byte(a)))
}

func absEpi64(a [32]byte) [32]byte


// MaskAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_mask_abs_epi64'.
// Requires AVX512F.
func MaskAbsEpi64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskAbsEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskAbsEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzAbsEpi64: Compute the absolute value of packed 64-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ABS(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSQ'. Intrinsic: '_mm256_maskz_abs_epi64'.
// Requires AVX512F.
func MaskzAbsEpi64(k Mmask8, a M256i) M256i {
	return M256i(maskzAbsEpi64(uint8(k), [32]byte(a)))
}

func maskzAbsEpi64(k uint8, a [32]byte) [32]byte


// AbsEpi8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := ABS(a[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm256_abs_epi8'.
// Requires AVX2.
func AbsEpi8(a M256i) M256i {
	return M256i(absEpi8([32]byte(a)))
}

func absEpi8(a [32]byte) [32]byte


// MaskAbsEpi8: Compute the absolute value of packed 8-bit integers in 'a', and
// store the unsigned results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm256_mask_abs_epi8'.
// Requires AVX512BW.
func MaskAbsEpi8(src M256i, k Mmask32, a M256i) M256i {
	return M256i(maskAbsEpi8([32]byte(src), uint32(k), [32]byte(a)))
}

func maskAbsEpi8(src [32]byte, k uint32, a [32]byte) [32]byte


// MaskzAbsEpi8: Compute the absolute value of packed 8-bit integers in 'a',
// and store the unsigned results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := ABS(a[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPABSB'. Intrinsic: '_mm256_maskz_abs_epi8'.
// Requires AVX512BW.
func MaskzAbsEpi8(k Mmask32, a M256i) M256i {
	return M256i(maskzAbsEpi8(uint32(k), [32]byte(a)))
}

func maskzAbsEpi8(k uint32, a [32]byte) [32]byte


// AcosPd: Compute the inverse cosine of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ACOS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acos_pd'.
// Requires AVX.
func AcosPd(a M256d) M256d {
	return M256d(acosPd([4]float64(a)))
}

func acosPd(a [4]float64) [4]float64


// AcosPs: Compute the inverse cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ACOS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acos_ps'.
// Requires AVX.
func AcosPs(a M256) M256 {
	return M256(acosPs([8]float32(a)))
}

func acosPs(a [8]float32) [8]float32


// AcoshPd: Compute the inverse hyperbolic cosine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ACOSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acosh_pd'.
// Requires AVX.
func AcoshPd(a M256d) M256d {
	return M256d(acoshPd([4]float64(a)))
}

func acoshPd(a [4]float64) [4]float64


// AcoshPs: Compute the inverse hyperbolic cosine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ACOSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_acosh_ps'.
// Requires AVX.
func AcoshPs(a M256) M256 {
	return M256(acoshPs([8]float32(a)))
}

func acoshPs(a [8]float32) [8]float32


// AddEpi16: Add packed 16-bit integers in 'a' and 'b', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[i+15:i] + b[i+15:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm256_add_epi16'.
// Requires AVX2.
func AddEpi16(a M256i, b M256i) M256i {
	return M256i(addEpi16([32]byte(a), [32]byte(b)))
}

func addEpi16(a [32]byte, b [32]byte) [32]byte


// MaskAddEpi16: Add packed 16-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm256_mask_add_epi16'.
// Requires AVX512BW.
func MaskAddEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAddEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAddEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAddEpi16: Add packed 16-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] + b[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDW'. Intrinsic: '_mm256_maskz_add_epi16'.
// Requires AVX512BW.
func MaskzAddEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAddEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAddEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// AddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_add_epi32'.
// Requires AVX2.
func AddEpi32(a M256i, b M256i) M256i {
	return M256i(addEpi32([32]byte(a), [32]byte(b)))
}

func addEpi32(a [32]byte, b [32]byte) [32]byte


// MaskAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_mask_add_epi32'.
// Requires AVX512F.
func MaskAddEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAddEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAddEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAddEpi32: Add packed 32-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDD'. Intrinsic: '_mm256_maskz_add_epi32'.
// Requires AVX512F.
func MaskzAddEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAddEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAddEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// AddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_add_epi64'.
// Requires AVX2.
func AddEpi64(a M256i, b M256i) M256i {
	return M256i(addEpi64([32]byte(a), [32]byte(b)))
}

func addEpi64(a [32]byte, b [32]byte) [32]byte


// MaskAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_mask_add_epi64'.
// Requires AVX512F.
func MaskAddEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAddEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAddEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAddEpi64: Add packed 64-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] :=0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDQ'. Intrinsic: '_mm256_maskz_add_epi64'.
// Requires AVX512F.
func MaskzAddEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAddEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAddEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// AddEpi8: Add packed 8-bit integers in 'a' and 'b', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[i+7:i] + b[i+7:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm256_add_epi8'.
// Requires AVX2.
func AddEpi8(a M256i, b M256i) M256i {
	return M256i(addEpi8([32]byte(a), [32]byte(b)))
}

func addEpi8(a [32]byte, b [32]byte) [32]byte


// MaskAddEpi8: Add packed 8-bit integers in 'a' and 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm256_mask_add_epi8'.
// Requires AVX512BW.
func MaskAddEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAddEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAddEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAddEpi8: Add packed 8-bit integers in 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] + b[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDB'. Intrinsic: '_mm256_maskz_add_epi8'.
// Requires AVX512BW.
func MaskzAddEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAddEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAddEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// AddPd: Add packed double-precision (64-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] + b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm256_add_pd'.
// Requires AVX.
func AddPd(a M256d, b M256d) M256d {
	return M256d(addPd([4]float64(a), [4]float64(b)))
}

func addPd(a [4]float64, b [4]float64) [4]float64


// MaskAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm256_mask_add_pd'.
// Requires AVX512VL.
func MaskAddPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskAddPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskAddPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzAddPd: Add packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPD'. Intrinsic: '_mm256_maskz_add_pd'.
// Requires AVX512VL.
func MaskzAddPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzAddPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzAddPd(k uint8, a [4]float64, b [4]float64) [4]float64


// AddPs: Add packed single-precision (32-bit) floating-point elements in 'a'
// and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] + b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm256_add_ps'.
// Requires AVX.
func AddPs(a M256, b M256) M256 {
	return M256(addPs([8]float32(a), [8]float32(b)))
}

func addPs(a [8]float32, b [8]float32) [8]float32


// MaskAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm256_mask_add_ps'.
// Requires AVX512VL.
func MaskAddPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskAddPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskAddPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzAddPs: Add packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDPS'. Intrinsic: '_mm256_maskz_add_ps'.
// Requires AVX512VL.
func MaskzAddPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzAddPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzAddPs(k uint8, a [8]float32, b [8]float32) [8]float32


// AddsEpi16: Add packed 16-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm256_adds_epi16'.
// Requires AVX2.
func AddsEpi16(a M256i, b M256i) M256i {
	return M256i(addsEpi16([32]byte(a), [32]byte(b)))
}

func addsEpi16(a [32]byte, b [32]byte) [32]byte


// MaskAddsEpi16: Add packed 16-bit integers in 'a' and 'b' using saturation,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm256_mask_adds_epi16'.
// Requires AVX512BW.
func MaskAddsEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAddsEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAddsEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAddsEpi16: Add packed 16-bit integers in 'a' and 'b' using saturation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSW'. Intrinsic: '_mm256_maskz_adds_epi16'.
// Requires AVX512BW.
func MaskzAddsEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAddsEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAddsEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// AddsEpi8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm256_adds_epi8'.
// Requires AVX2.
func AddsEpi8(a M256i, b M256i) M256i {
	return M256i(addsEpi8([32]byte(a), [32]byte(b)))
}

func addsEpi8(a [32]byte, b [32]byte) [32]byte


// MaskAddsEpi8: Add packed 8-bit integers in 'a' and 'b' using saturation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm256_mask_adds_epi8'.
// Requires AVX512BW.
func MaskAddsEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAddsEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAddsEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAddsEpi8: Add packed 8-bit integers in 'a' and 'b' using saturation,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDSB'. Intrinsic: '_mm256_maskz_adds_epi8'.
// Requires AVX512BW.
func MaskzAddsEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAddsEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAddsEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// AddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm256_adds_epu16'.
// Requires AVX2.
func AddsEpu16(a M256i, b M256i) M256i {
	return M256i(addsEpu16([32]byte(a), [32]byte(b)))
}

func addsEpu16(a [32]byte, b [32]byte) [32]byte


// MaskAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm256_mask_adds_epu16'.
// Requires AVX512BW.
func MaskAddsEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAddsEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAddsEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAddsEpu16: Add packed unsigned 16-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16( a[i+15:i] + b[i+15:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSW'. Intrinsic: '_mm256_maskz_adds_epu16'.
// Requires AVX512BW.
func MaskzAddsEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAddsEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAddsEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// AddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm256_adds_epu8'.
// Requires AVX2.
func AddsEpu8(a M256i, b M256i) M256i {
	return M256i(addsEpu8([32]byte(a), [32]byte(b)))
}

func addsEpu8(a [32]byte, b [32]byte) [32]byte


// MaskAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm256_mask_adds_epu8'.
// Requires AVX512BW.
func MaskAddsEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAddsEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAddsEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAddsEpu8: Add packed unsigned 8-bit integers in 'a' and 'b' using
// saturation, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8( a[i+7:i] + b[i+7:i] )
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPADDUSB'. Intrinsic: '_mm256_maskz_adds_epu8'.
// Requires AVX512BW.
func MaskzAddsEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAddsEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAddsEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// AddsubPd: Alternatively add and subtract packed double-precision (64-bit)
// floating-point elements in 'a' to/from packed elements in 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i] + b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDSUBPD'. Intrinsic: '_mm256_addsub_pd'.
// Requires AVX.
func AddsubPd(a M256d, b M256d) M256d {
	return M256d(addsubPd([4]float64(a), [4]float64(b)))
}

func addsubPd(a [4]float64, b [4]float64) [4]float64


// AddsubPs: Alternatively add and subtract packed single-precision (32-bit)
// floating-point elements in 'a' to/from packed elements in 'b', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i] + b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VADDSUBPS'. Intrinsic: '_mm256_addsub_ps'.
// Requires AVX.
func AddsubPs(a M256, b M256) M256 {
	return M256(addsubPs([8]float32(a), [8]float32(b)))
}

func addsubPs(a [8]float32, b [8]float32) [8]float32


// AlignrEpi32: Concatenate 'a' and 'b' into a 64-byte immediate result, shift
// the result right by 'count' 32-bit elements, and store the low 32 bytes (8
// elements) in 'dst'. 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (32*count)
//		dst[255:0] := temp[255:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm256_alignr_epi32'.
// Requires AVX512VL.
func AlignrEpi32(a M256i, b M256i, count int) M256i {
	return M256i(alignrEpi32([32]byte(a), [32]byte(b), count))
}

func alignrEpi32(a [32]byte, b [32]byte, count int) [32]byte


// MaskAlignrEpi32: Concatenate 'a' and 'b' into a 64-byte immediate result,
// shift the result right by 'count' 32-bit elements, and store the low 32
// bytes (8 elements) in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (32*count)
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm256_mask_alignr_epi32'.
// Requires AVX512VL.
func MaskAlignrEpi32(src M256i, k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskAlignrEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskAlignrEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte, count int) [32]byte


// MaskzAlignrEpi32: Concatenate 'a' and 'b' into a 64-byte immediate result,
// shift the result right by 'count' 32-bit elements, and store the low 32
// bytes (8 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (32*count)
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := temp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGND'. Intrinsic: '_mm256_maskz_alignr_epi32'.
// Requires AVX512VL.
func MaskzAlignrEpi32(k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskzAlignrEpi32(uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskzAlignrEpi32(k uint8, a [32]byte, b [32]byte, count int) [32]byte


// AlignrEpi64: Concatenate 'a' and 'b' into a 64-byte immediate result, shift
// the result right by 'count' 64-bit elements, and store the low 32 bytes (4
// elements) in 'dst'. 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (64*count)
//		dst[255:0] := temp[255:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm256_alignr_epi64'.
// Requires AVX512VL.
func AlignrEpi64(a M256i, b M256i, count int) M256i {
	return M256i(alignrEpi64([32]byte(a), [32]byte(b), count))
}

func alignrEpi64(a [32]byte, b [32]byte, count int) [32]byte


// MaskAlignrEpi64: Concatenate 'a' and 'b' into a 64-byte immediate result,
// shift the result right by 'count' 64-bit elements, and store the low 32
// bytes (4 elements) in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (64*count)
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm256_mask_alignr_epi64'.
// Requires AVX512VL.
func MaskAlignrEpi64(src M256i, k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskAlignrEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskAlignrEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte, count int) [32]byte


// MaskzAlignrEpi64: Concatenate 'a' and 'b' into a 64-byte immediate result,
// shift the result right by 'count' 64-bit elements, and store the low 32
// bytes (4 elements) in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		temp[511:256] := a[255:0]
//		temp[255:0] := b[255:0]
//		temp[511:0] := temp[511:0] >> (64*count)
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := temp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VALIGNQ'. Intrinsic: '_mm256_maskz_alignr_epi64'.
// Requires AVX512VL.
func MaskzAlignrEpi64(k Mmask8, a M256i, b M256i, count int) M256i {
	return M256i(maskzAlignrEpi64(uint8(k), [32]byte(a), [32]byte(b), count))
}

func maskzAlignrEpi64(k uint8, a [32]byte, b [32]byte, count int) [32]byte


// AlignrEpi8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst'. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm256_alignr_epi8'.
// Requires AVX2.
func AlignrEpi8(a M256i, b M256i, count int) M256i {
	return M256i(alignrEpi8([32]byte(a), [32]byte(b), count))
}

func alignrEpi8(a [32]byte, b [32]byte, count int) [32]byte


// MaskAlignrEpi8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			tmp_dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm256_mask_alignr_epi8'.
// Requires AVX512BW.
func MaskAlignrEpi8(src M256i, k Mmask32, a M256i, b M256i, count int) M256i {
	return M256i(maskAlignrEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b), count))
}

func maskAlignrEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte, count int) [32]byte


// MaskzAlignrEpi8: Concatenate pairs of 16-byte blocks in 'a' and 'b' into a
// 32-byte temporary result, shift the result right by 'count' bytes, and store
// the low 16 bytes in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[255:0] := ((a[i+127:i] << 128) OR b[i+127:i]) >> (count[7:0]*8)
//			tmp_dst[i+127:i] := tmp[127:0]
//		ENDFOR
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPALIGNR'. Intrinsic: '_mm256_maskz_alignr_epi8'.
// Requires AVX512BW.
func MaskzAlignrEpi8(k Mmask32, a M256i, b M256i, count int) M256i {
	return M256i(maskzAlignrEpi8(uint32(k), [32]byte(a), [32]byte(b), count))
}

func maskzAlignrEpi8(k uint32, a [32]byte, b [32]byte, count int) [32]byte


// MaskAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_mask_and_epi32'.
// Requires AVX512F.
func MaskAndEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndEpi32: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDD'. Intrinsic: '_mm256_maskz_and_epi32'.
// Requires AVX512F.
func MaskzAndEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_mask_and_epi64'.
// Requires AVX512F.
func MaskAndEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndEpi64: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDQ'. Intrinsic: '_mm256_maskz_and_epi64'.
// Requires AVX512F.
func MaskzAndEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// AndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm256_and_pd'.
// Requires AVX.
func AndPd(a M256d, b M256d) M256d {
	return M256d(andPd([4]float64(a), [4]float64(b)))
}

func andPd(a [4]float64, b [4]float64) [4]float64


// MaskAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm256_mask_and_pd'.
// Requires AVX512DQ.
func MaskAndPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskAndPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskAndPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzAndPd: Compute the bitwise AND of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0 
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPD'. Intrinsic: '_mm256_maskz_and_pd'.
// Requires AVX512DQ.
func MaskzAndPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzAndPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzAndPd(k uint8, a [4]float64, b [4]float64) [4]float64


// AndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm256_and_ps'.
// Requires AVX.
func AndPs(a M256, b M256) M256 {
	return M256(andPs([8]float32(a), [8]float32(b)))
}

func andPs(a [8]float32, b [8]float32) [8]float32


// MaskAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm256_mask_and_ps'.
// Requires AVX512DQ.
func MaskAndPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskAndPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskAndPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzAndPs: Compute the bitwise AND of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDPS'. Intrinsic: '_mm256_maskz_and_ps'.
// Requires AVX512DQ.
func MaskzAndPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzAndPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzAndPs(k uint8, a [8]float32, b [8]float32) [8]float32


// AndSi256: Compute the bitwise AND of 256 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := (a[255:0] AND b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPAND'. Intrinsic: '_mm256_and_si256'.
// Requires AVX2.
func AndSi256(a M256i, b M256i) M256i {
	return M256i(andSi256([32]byte(a), [32]byte(b)))
}

func andSi256(a [32]byte, b [32]byte) [32]byte


// MaskAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_mask_andnot_epi32'.
// Requires AVX512F.
func MaskAndnotEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndnotEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndnotEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnotEpi32: Compute the bitwise AND NOT of packed 32-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (NOT a[i+31:i]) AND b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDND'. Intrinsic: '_mm256_maskz_andnot_epi32'.
// Requires AVX512F.
func MaskzAndnotEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndnotEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndnotEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_mask_andnot_epi64'.
// Requires AVX512F.
func MaskAndnotEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskAndnotEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskAndnotEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzAndnotEpi64: Compute the bitwise AND NOT of packed 64-bit integers in
// 'a' and 'b', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (NOT a[i+63:i]) AND b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDNQ'. Intrinsic: '_mm256_maskz_andnot_epi64'.
// Requires AVX512F.
func MaskzAndnotEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzAndnotEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzAndnotEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// AndnotPd: Compute the bitwise AND NOT of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm256_andnot_pd'.
// Requires AVX.
func AndnotPd(a M256d, b M256d) M256d {
	return M256d(andnotPd([4]float64(a), [4]float64(b)))
}

func andnotPd(a [4]float64, b [4]float64) [4]float64


// MaskAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm256_mask_andnot_pd'.
// Requires AVX512DQ.
func MaskAndnotPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskAndnotPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskAndnotPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzAndnotPd: Compute the bitwise AND NOT of packed double-precision
// (64-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ((NOT a[i+63:i]) AND b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPD'. Intrinsic: '_mm256_maskz_andnot_pd'.
// Requires AVX512DQ.
func MaskzAndnotPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzAndnotPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzAndnotPd(k uint8, a [4]float64, b [4]float64) [4]float64


// AndnotPs: Compute the bitwise AND NOT of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm256_andnot_ps'.
// Requires AVX.
func AndnotPs(a M256, b M256) M256 {
	return M256(andnotPs([8]float32(a), [8]float32(b)))
}

func andnotPs(a [8]float32, b [8]float32) [8]float32


// MaskAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm256_mask_andnot_ps'.
// Requires AVX512DQ.
func MaskAndnotPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskAndnotPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskAndnotPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzAndnotPs: Compute the bitwise AND NOT of packed single-precision
// (32-bit) floating-point elements in 'a' and 'b', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ((NOT a[i+31:i]) AND b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VANDNPS'. Intrinsic: '_mm256_maskz_andnot_ps'.
// Requires AVX512DQ.
func MaskzAndnotPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzAndnotPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzAndnotPs(k uint8, a [8]float32, b [8]float32) [8]float32


// AndnotSi256: Compute the bitwise AND NOT of 256 bits (representing integer
// data) in 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := ((NOT a[255:0]) AND b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPANDN'. Intrinsic: '_mm256_andnot_si256'.
// Requires AVX2.
func AndnotSi256(a M256i, b M256i) M256i {
	return M256i(andnotSi256([32]byte(a), [32]byte(b)))
}

func andnotSi256(a [32]byte, b [32]byte) [32]byte


// AsinPd: Compute the inverse sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ASIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asin_pd'.
// Requires AVX.
func AsinPd(a M256d) M256d {
	return M256d(asinPd([4]float64(a)))
}

func asinPd(a [4]float64) [4]float64


// AsinPs: Compute the inverse sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ASIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asin_ps'.
// Requires AVX.
func AsinPs(a M256) M256 {
	return M256(asinPs([8]float32(a)))
}

func asinPs(a [8]float32) [8]float32


// AsinhPd: Compute the inverse hyperbolic sine of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ASINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asinh_pd'.
// Requires AVX.
func AsinhPd(a M256d) M256d {
	return M256d(asinhPd([4]float64(a)))
}

func asinhPd(a [4]float64) [4]float64


// AsinhPs: Compute the inverse hyperbolic sine of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ASINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_asinh_ps'.
// Requires AVX.
func AsinhPs(a M256) M256 {
	return M256(asinhPs([8]float32(a)))
}

func asinhPs(a [8]float32) [8]float32


// AtanPd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan_pd'.
// Requires AVX.
func AtanPd(a M256d) M256d {
	return M256d(atanPd([4]float64(a)))
}

func atanPd(a [4]float64) [4]float64


// AtanPs: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan_ps'.
// Requires AVX.
func AtanPs(a M256) M256 {
	return M256(atanPs([8]float32(a)))
}

func atanPs(a [8]float32) [8]float32


// Atan2Pd: Compute the inverse tangent of packed double-precision (64-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ATAN(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan2_pd'.
// Requires AVX.
func Atan2Pd(a M256d, b M256d) M256d {
	return M256d(atan2Pd([4]float64(a), [4]float64(b)))
}

func atan2Pd(a [4]float64, b [4]float64) [4]float64


// Atan2Ps: Compute the inverse tangent of packed single-precision (32-bit)
// floating-point elements in 'a' divided by packed elements in 'b', and store
// the results in 'dst' expressed in radians. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ATAN(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atan2_ps'.
// Requires AVX.
func Atan2Ps(a M256, b M256) M256 {
	return M256(atan2Ps([8]float32(a), [8]float32(b)))
}

func atan2Ps(a [8]float32, b [8]float32) [8]float32


// AtanhPd: Compute the inverse hyperbolic tangent of packed double-precision
// (64-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ATANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atanh_pd'.
// Requires AVX.
func AtanhPd(a M256d) M256d {
	return M256d(atanhPd([4]float64(a)))
}

func atanhPd(a [4]float64) [4]float64


// AtanhPs: Compute the inverse hyperbolic tangent of packed single-precision
// (32-bit) floating-point elements in 'a' expressed in radians, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ATANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_atanh_ps'.
// Requires AVX.
func AtanhPs(a M256) M256 {
	return M256(atanhPs([8]float32(a)))
}

func atanhPs(a [8]float32) [8]float32


// AvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm256_avg_epu16'.
// Requires AVX2.
func AvgEpu16(a M256i, b M256i) M256i {
	return M256i(avgEpu16([32]byte(a), [32]byte(b)))
}

func avgEpu16(a [32]byte, b [32]byte) [32]byte


// MaskAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm256_mask_avg_epu16'.
// Requires AVX512BW.
func MaskAvgEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskAvgEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskAvgEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzAvgEpu16: Average packed unsigned 16-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := (a[i+15:i] + b[i+15:i] + 1) >> 1
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGW'. Intrinsic: '_mm256_maskz_avg_epu16'.
// Requires AVX512BW.
func MaskzAvgEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzAvgEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzAvgEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// AvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm256_avg_epu8'.
// Requires AVX2.
func AvgEpu8(a M256i, b M256i) M256i {
	return M256i(avgEpu8([32]byte(a), [32]byte(b)))
}

func avgEpu8(a [32]byte, b [32]byte) [32]byte


// MaskAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm256_mask_avg_epu8'.
// Requires AVX512BW.
func MaskAvgEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskAvgEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskAvgEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzAvgEpu8: Average packed unsigned 8-bit integers in 'a' and 'b', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := (a[i+7:i] + b[i+7:i] + 1) >> 1
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPAVGB'. Intrinsic: '_mm256_maskz_avg_epu8'.
// Requires AVX512BW.
func MaskzAvgEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzAvgEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzAvgEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// BlendEpi16: Blend packed 16-bit integers from 'a' and 'b' using control mask
// 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[j%8]
//				dst[i+15:i] := b[i+15:i]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDW'. Intrinsic: '_mm256_blend_epi16'.
// Requires AVX2.
func BlendEpi16(a M256i, b M256i, imm8 int) M256i {
	return M256i(blendEpi16([32]byte(a), [32]byte(b), imm8))
}

func blendEpi16(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskBlendEpi16: Blend packed 16-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := b[i+15:i]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMW'. Intrinsic: '_mm256_mask_blend_epi16'.
// Requires AVX512BW.
func MaskBlendEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskBlendEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskBlendEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// BlendEpi32: Blend packed 32-bit integers from 'a' and 'b' using control mask
// 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[j%8]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDD'. Intrinsic: '_mm256_blend_epi32'.
// Requires AVX2.
func BlendEpi32(a M256i, b M256i, imm8 int) M256i {
	return M256i(blendEpi32([32]byte(a), [32]byte(b), imm8))
}

func blendEpi32(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskBlendEpi32: Blend packed 32-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMD'. Intrinsic: '_mm256_mask_blend_epi32'.
// Requires AVX512F.
func MaskBlendEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskBlendEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskBlendEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlendEpi64: Blend packed 64-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMQ'. Intrinsic: '_mm256_mask_blend_epi64'.
// Requires AVX512F.
func MaskBlendEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskBlendEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskBlendEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskBlendEpi8: Blend packed 8-bit integers from 'a' and 'b' using control
// mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := b[i+7:i]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDMB'. Intrinsic: '_mm256_mask_blend_epi8'.
// Requires AVX512BW.
func MaskBlendEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskBlendEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskBlendEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// BlendPd: Blend packed double-precision (64-bit) floating-point elements from
// 'a' and 'b' using control mask 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[j%8]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDPD'. Intrinsic: '_mm256_blend_pd'.
// Requires AVX.
func BlendPd(a M256d, b M256d, imm8 int) M256d {
	return M256d(blendPd([4]float64(a), [4]float64(b), imm8))
}

func blendPd(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskBlendPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPD'. Intrinsic: '_mm256_mask_blend_pd'.
// Requires AVX512F.
func MaskBlendPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskBlendPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskBlendPd(k uint8, a [4]float64, b [4]float64) [4]float64


// BlendPs: Blend packed single-precision (32-bit) floating-point elements from
// 'a' and 'b' using control mask 'imm8', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[j%8]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDPS'. Intrinsic: '_mm256_blend_ps'.
// Requires AVX.
func BlendPs(a M256, b M256, imm8 int) M256 {
	return M256(blendPs([8]float32(a), [8]float32(b), imm8))
}

func blendPs(a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskBlendPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using control mask 'k', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDMPS'. Intrinsic: '_mm256_mask_blend_ps'.
// Requires AVX512F.
func MaskBlendPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskBlendPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskBlendPs(k uint8, a [8]float32, b [8]float32) [8]float32


// BlendvEpi8: Blend packed 8-bit integers from 'a' and 'b' using 'mask', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF mask[i+7]
//				dst[i+7:i] := b[i+7:i]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBLENDVB'. Intrinsic: '_mm256_blendv_epi8'.
// Requires AVX2.
func BlendvEpi8(a M256i, b M256i, mask M256i) M256i {
	return M256i(blendvEpi8([32]byte(a), [32]byte(b), [32]byte(mask)))
}

func blendvEpi8(a [32]byte, b [32]byte, mask [32]byte) [32]byte


// BlendvPd: Blend packed double-precision (64-bit) floating-point elements
// from 'a' and 'b' using 'mask', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := b[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDVPD'. Intrinsic: '_mm256_blendv_pd'.
// Requires AVX.
func BlendvPd(a M256d, b M256d, mask M256d) M256d {
	return M256d(blendvPd([4]float64(a), [4]float64(b), [4]float64(mask)))
}

func blendvPd(a [4]float64, b [4]float64, mask [4]float64) [4]float64


// BlendvPs: Blend packed single-precision (32-bit) floating-point elements
// from 'a' and 'b' using 'mask', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := b[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBLENDVPS'. Intrinsic: '_mm256_blendv_ps'.
// Requires AVX.
func BlendvPs(a M256, b M256, mask M256) M256 {
	return M256(blendvPs([8]float32(a), [8]float32(b), [8]float32(mask)))
}

func blendvPs(a [8]float32, b [8]float32, mask [8]float32) [8]float32


// BroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm256_broadcast_f32x2'.
// Requires AVX512DQ.
func BroadcastF32x2(a M128) M256 {
	return M256(broadcastF32x2([4]float32(a)))
}

func broadcastF32x2(a [4]float32) [8]float32


// MaskBroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm256_mask_broadcast_f32x2'.
// Requires AVX512DQ.
func MaskBroadcastF32x2(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastF32x2([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastF32x2(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastF32x2: Broadcast the lower 2 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X2'. Intrinsic: '_mm256_maskz_broadcast_f32x2'.
// Requires AVX512DQ.
func MaskzBroadcastF32x2(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastF32x2(uint8(k), [4]float32(a)))
}

func maskzBroadcastF32x2(k uint8, a [4]float32) [8]float32


// BroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_broadcast_f32x4'.
// Requires AVX512F.
func BroadcastF32x4(a M128) M256 {
	return M256(broadcastF32x4([4]float32(a)))
}

func broadcastF32x4(a [4]float32) [8]float32


// MaskBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_mask_broadcast_f32x4'.
// Requires AVX512F.
func MaskBroadcastF32x4(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastF32x4([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastF32x4(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastF32x4: Broadcast the 4 packed single-precision (32-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF32X4'. Intrinsic: '_mm256_maskz_broadcast_f32x4'.
// Requires AVX512F.
func MaskzBroadcastF32x4(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastF32x4(uint8(k), [4]float32(a)))
}

func maskzBroadcastF32x4(k uint8, a [4]float32) [8]float32


// BroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm256_broadcast_f64x2'.
// Requires AVX512DQ.
func BroadcastF64x2(a M128d) M256d {
	return M256d(broadcastF64x2([2]float64(a)))
}

func broadcastF64x2(a [2]float64) [4]float64


// MaskBroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm256_mask_broadcast_f64x2'.
// Requires AVX512DQ.
func MaskBroadcastF64x2(src M256d, k Mmask8, a M128d) M256d {
	return M256d(maskBroadcastF64x2([4]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastF64x2(src [4]float64, k uint8, a [2]float64) [4]float64


// MaskzBroadcastF64x2: Broadcast the 2 packed double-precision (64-bit)
// floating-point elements from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF64X2'. Intrinsic: '_mm256_maskz_broadcast_f64x2'.
// Requires AVX512DQ.
func MaskzBroadcastF64x2(k Mmask8, a M128d) M256d {
	return M256d(maskzBroadcastF64x2(uint8(k), [2]float64(a)))
}

func maskzBroadcastF64x2(k uint8, a [2]float64) [4]float64


// BroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a' to all
// elements of "dst. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm256_broadcast_i32x2'.
// Requires AVX512DQ.
func BroadcastI32x2(a M128i) M256i {
	return M256i(broadcastI32x2([16]byte(a)))
}

func broadcastI32x2(a [16]byte) [32]byte


// MaskBroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a' to
// all elements of 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm256_mask_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskBroadcastI32x2(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastI32x2([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI32x2(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastI32x2: Broadcast the lower 2 packed 32-bit integers from 'a'
// to all elements of 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 2)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X2'. Intrinsic: '_mm256_maskz_broadcast_i32x2'.
// Requires AVX512DQ.
func MaskzBroadcastI32x2(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastI32x2(uint8(k), [16]byte(a)))
}

func maskzBroadcastI32x2(k uint8, a [16]byte) [32]byte


// BroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			dst[i+31:i] := a[n+31:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_broadcast_i32x4'.
// Requires AVX512F.
func BroadcastI32x4(a M128i) M256i {
	return M256i(broadcastI32x4([16]byte(a)))
}

func broadcastI32x4(a [16]byte) [32]byte


// MaskBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := src[n+31:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_mask_broadcast_i32x4'.
// Requires AVX512F.
func MaskBroadcastI32x4(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastI32x4([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI32x4(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastI32x4: Broadcast the 4 packed 32-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			n := (j mod 4)*32
//			IF k[j]
//				dst[i+31:i] := a[n+31:n]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI32X4'. Intrinsic: '_mm256_maskz_broadcast_i32x4'.
// Requires AVX512F.
func MaskzBroadcastI32x4(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastI32x4(uint8(k), [16]byte(a)))
}

func maskzBroadcastI32x4(k uint8, a [16]byte) [32]byte


// BroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			dst[i+63:i] := a[n+63:n]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm256_broadcast_i64x2'.
// Requires AVX512DQ.
func BroadcastI64x2(a M128i) M256i {
	return M256i(broadcastI64x2([16]byte(a)))
}

func broadcastI64x2(a [16]byte) [32]byte


// MaskBroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := src[n+63:n]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm256_mask_broadcast_i64x2'.
// Requires AVX512DQ.
func MaskBroadcastI64x2(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastI64x2([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastI64x2(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastI64x2: Broadcast the 2 packed 64-bit integers from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			n := (j mod 2)*64
//			IF k[j]
//				dst[i+63:i] := a[n+63:n]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI64X2'. Intrinsic: '_mm256_maskz_broadcast_i64x2'.
// Requires AVX512DQ.
func MaskzBroadcastI64x2(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastI64x2(uint8(k), [16]byte(a)))
}

func maskzBroadcastI64x2(k uint8, a [16]byte) [32]byte


// BroadcastPd: Broadcast 128 bits from memory (composed of 2 packed
// double-precision (64-bit) floating-point elements) to all elements of 'dst'. 
//
//		tmp[127:0] = MEM[mem_addr+127:mem_addr]
//		dst[127:0] := tmp[127:0]
//		dst[255:128] := tmp[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF128'. Intrinsic: '_mm256_broadcast_pd'.
// Requires AVX.
func BroadcastPd(mem_addr M128dConst) M256d {
	return M256d(broadcastPd(mem_addr))
}

func broadcastPd(mem_addr M128dConst) [4]float64


// BroadcastPs: Broadcast 128 bits from memory (composed of 4 packed
// single-precision (32-bit) floating-point elements) to all elements of 'dst'. 
//
//		tmp[127:0] = MEM[mem_addr+127:mem_addr]
//		dst[127:0] := tmp[127:0]
//		dst[255:128] := tmp[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTF128'. Intrinsic: '_mm256_broadcast_ps'.
// Requires AVX.
func BroadcastPs(mem_addr M128Const) M256 {
	return M256(broadcastPs(mem_addr))
}

func broadcastPs(mem_addr M128Const) [8]float32


// BroadcastSd: Broadcast a double-precision (64-bit) floating-point element
// from memory to all elements of 'dst'. 
//
//		tmp[63:0] = MEM[mem_addr+63:mem_addr]
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := tmp[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_broadcast_sd'.
// Requires AVX.
func BroadcastSd(mem_addr float64) M256d {
	return M256d(broadcastSd(mem_addr))
}

func broadcastSd(mem_addr float64) [4]float64


// BroadcastSs: Broadcast a single-precision (32-bit) floating-point element
// from memory to all elements of 'dst'. 
//
//		tmp[31:0] = MEM[mem_addr+31:mem_addr]
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := tmp[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_broadcast_ss'.
// Requires AVX.
func BroadcastSs(mem_addr float32) M256 {
	return M256(broadcastSs(mem_addr))
}

func broadcastSs(mem_addr float32) [8]float32


// BroadcastbEpi8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_broadcastb_epi8'.
// Requires AVX2.
func BroadcastbEpi8(a M128i) M256i {
	return M256i(broadcastbEpi8([16]byte(a)))
}

func broadcastbEpi8(a [16]byte) [32]byte


// MaskBroadcastbEpi8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_mask_broadcastb_epi8'.
// Requires AVX512BW.
func MaskBroadcastbEpi8(src M256i, k Mmask32, a M128i) M256i {
	return M256i(maskBroadcastbEpi8([32]byte(src), uint32(k), [16]byte(a)))
}

func maskBroadcastbEpi8(src [32]byte, k uint32, a [16]byte) [32]byte


// MaskzBroadcastbEpi8: Broadcast the low packed 8-bit integer from 'a' to all
// elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_maskz_broadcastb_epi8'.
// Requires AVX512BW.
func MaskzBroadcastbEpi8(k Mmask32, a M128i) M256i {
	return M256i(maskzBroadcastbEpi8(uint32(k), [16]byte(a)))
}

func maskzBroadcastbEpi8(k uint32, a [16]byte) [32]byte


// BroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_broadcastd_epi32'.
// Requires AVX2.
func BroadcastdEpi32(a M128i) M256i {
	return M256i(broadcastdEpi32([16]byte(a)))
}

func broadcastdEpi32(a [16]byte) [32]byte


// MaskBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_broadcastd_epi32'.
// Requires AVX512F.
func MaskBroadcastdEpi32(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastdEpi32([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastdEpi32(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastdEpi32: Broadcast the low packed 32-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_broadcastd_epi32'.
// Requires AVX512F.
func MaskzBroadcastdEpi32(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastdEpi32(uint8(k), [16]byte(a)))
}

func maskzBroadcastdEpi32(k uint8, a [16]byte) [32]byte


// BroadcastmbEpi64: Broadcast the low 8-bits from input mask 'k' to all 64-bit
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ZeroExtend(k[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTMB2Q'. Intrinsic: '_mm256_broadcastmb_epi64'.
// Requires AVX512CD.
func BroadcastmbEpi64(k Mmask8) M256i {
	return M256i(broadcastmbEpi64(uint8(k)))
}

func broadcastmbEpi64(k uint8) [32]byte


// BroadcastmwEpi32: Broadcast the low 16-bits from input mask 'k' to all
// 32-bit elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ZeroExtend(k[15:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTMW2D'. Intrinsic: '_mm256_broadcastmw_epi32'.
// Requires AVX512CD.
func BroadcastmwEpi32(k Mmask16) M256i {
	return M256i(broadcastmwEpi32(uint16(k)))
}

func broadcastmwEpi32(k uint16) [32]byte


// BroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_broadcastq_epi64'.
// Requires AVX2.
func BroadcastqEpi64(a M128i) M256i {
	return M256i(broadcastqEpi64([16]byte(a)))
}

func broadcastqEpi64(a [16]byte) [32]byte


// MaskBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_broadcastq_epi64'.
// Requires AVX512F.
func MaskBroadcastqEpi64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskBroadcastqEpi64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskBroadcastqEpi64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzBroadcastqEpi64: Broadcast the low packed 64-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_broadcastq_epi64'.
// Requires AVX512F.
func MaskzBroadcastqEpi64(k Mmask8, a M128i) M256i {
	return M256i(maskzBroadcastqEpi64(uint8(k), [16]byte(a)))
}

func maskzBroadcastqEpi64(k uint8, a [16]byte) [32]byte


// BroadcastsdPd: Broadcast the low double-precision (64-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_broadcastsd_pd'.
// Requires AVX2.
func BroadcastsdPd(a M128d) M256d {
	return M256d(broadcastsdPd([2]float64(a)))
}

func broadcastsdPd(a [2]float64) [4]float64


// MaskBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_mask_broadcastsd_pd'.
// Requires AVX512F.
func MaskBroadcastsdPd(src M256d, k Mmask8, a M128d) M256d {
	return M256d(maskBroadcastsdPd([4]float64(src), uint8(k), [2]float64(a)))
}

func maskBroadcastsdPd(src [4]float64, k uint8, a [2]float64) [4]float64


// MaskzBroadcastsdPd: Broadcast the low double-precision (64-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSD'. Intrinsic: '_mm256_maskz_broadcastsd_pd'.
// Requires AVX512F.
func MaskzBroadcastsdPd(k Mmask8, a M128d) M256d {
	return M256d(maskzBroadcastsdPd(uint8(k), [2]float64(a)))
}

func maskzBroadcastsdPd(k uint8, a [2]float64) [4]float64


// Broadcastsi128Si256: Broadcast 128 bits of integer data from 'a' to all
// 128-bit lanes in 'dst'. 
//
//		dst[127:0] := a[127:0]
//		dst[255:128] := a[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTI128'. Intrinsic: '_mm256_broadcastsi128_si256'.
// Requires AVX2.
func Broadcastsi128Si256(a M128i) M256i {
	return M256i(broadcastsi128Si256([16]byte(a)))
}

func broadcastsi128Si256(a [16]byte) [32]byte


// BroadcastssPs: Broadcast the low single-precision (32-bit) floating-point
// element from 'a' to all elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_broadcastss_ps'.
// Requires AVX2.
func BroadcastssPs(a M128) M256 {
	return M256(broadcastssPs([4]float32(a)))
}

func broadcastssPs(a [4]float32) [8]float32


// MaskBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_mask_broadcastss_ps'.
// Requires AVX512F.
func MaskBroadcastssPs(src M256, k Mmask8, a M128) M256 {
	return M256(maskBroadcastssPs([8]float32(src), uint8(k), [4]float32(a)))
}

func maskBroadcastssPs(src [8]float32, k uint8, a [4]float32) [8]float32


// MaskzBroadcastssPs: Broadcast the low single-precision (32-bit)
// floating-point element from 'a' to all elements of 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VBROADCASTSS'. Intrinsic: '_mm256_maskz_broadcastss_ps'.
// Requires AVX512F.
func MaskzBroadcastssPs(k Mmask8, a M128) M256 {
	return M256(maskzBroadcastssPs(uint8(k), [4]float32(a)))
}

func maskzBroadcastssPs(k uint8, a [4]float32) [8]float32


// BroadcastwEpi16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_broadcastw_epi16'.
// Requires AVX2.
func BroadcastwEpi16(a M128i) M256i {
	return M256i(broadcastwEpi16([16]byte(a)))
}

func broadcastwEpi16(a [16]byte) [32]byte


// MaskBroadcastwEpi16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_mask_broadcastw_epi16'.
// Requires AVX512BW.
func MaskBroadcastwEpi16(src M256i, k Mmask16, a M128i) M256i {
	return M256i(maskBroadcastwEpi16([32]byte(src), uint16(k), [16]byte(a)))
}

func maskBroadcastwEpi16(src [32]byte, k uint16, a [16]byte) [32]byte


// MaskzBroadcastwEpi16: Broadcast the low packed 16-bit integer from 'a' to
// all elements of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_maskz_broadcastw_epi16'.
// Requires AVX512BW.
func MaskzBroadcastwEpi16(k Mmask16, a M128i) M256i {
	return M256i(maskzBroadcastwEpi16(uint16(k), [16]byte(a)))
}

func maskzBroadcastwEpi16(k uint16, a [16]byte) [32]byte


// BslliEpi128: Shift 128-bit lanes in 'a' left by 'imm8' bytes while shifting
// in zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] << (tmp*8)
//		dst[255:128] := a[255:128] << (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLDQ'. Intrinsic: '_mm256_bslli_epi128'.
// Requires AVX2.
func BslliEpi128(a M256i, imm8 int) M256i {
	return M256i(bslliEpi128([32]byte(a), imm8))
}

func bslliEpi128(a [32]byte, imm8 int) [32]byte


// BsrliEpi128: Shift 128-bit lanes in 'a' right by 'imm8' bytes while shifting
// in zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] >> (tmp*8)
//		dst[255:128] := a[255:128] >> (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLDQ'. Intrinsic: '_mm256_bsrli_epi128'.
// Requires AVX2.
func BsrliEpi128(a M256i, imm8 int) M256i {
	return M256i(bsrliEpi128([32]byte(a), imm8))
}

func bsrliEpi128(a [32]byte, imm8 int) [32]byte


// CastpdPs: Cast vector of type __m256d to type __m256.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd_ps'.
// Requires AVX.
func CastpdPs(a M256d) M256 {
	return M256(castpdPs([4]float64(a)))
}

func castpdPs(a [4]float64) [8]float32


// CastpdSi256: Casts vector of type __m256d to type __m256i. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd_si256'.
// Requires AVX.
func CastpdSi256(a M256d) M256i {
	return M256i(castpdSi256([4]float64(a)))
}

func castpdSi256(a [4]float64) [32]byte


// Castpd128Pd256: Casts vector of type __m128d to type __m256d; the upper 128
// bits of the result are undefined. This intrinsic is only used for
// compilation and does not generate any instructions, thus it has zero
// latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd128_pd256'.
// Requires AVX.
func Castpd128Pd256(a M128d) M256d {
	return M256d(castpd128Pd256([2]float64(a)))
}

func castpd128Pd256(a [2]float64) [4]float64


// Castpd256Pd128: Casts vector of type __m256d to type __m128d. This intrinsic
// is only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castpd256_pd128'.
// Requires AVX.
func Castpd256Pd128(a M256d) M128d {
	return M128d(castpd256Pd128([4]float64(a)))
}

func castpd256Pd128(a [4]float64) [2]float64


// CastpsPd: Cast vector of type __m256 to type __m256d.
// 	This intrinsic is only used for compilation and does not generate any
// instructions, thus it has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps_pd'.
// Requires AVX.
func CastpsPd(a M256) M256d {
	return M256d(castpsPd([8]float32(a)))
}

func castpsPd(a [8]float32) [4]float64


// CastpsSi256: Casts vector of type __m256 to type __m256i. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps_si256'.
// Requires AVX.
func CastpsSi256(a M256) M256i {
	return M256i(castpsSi256([8]float32(a)))
}

func castpsSi256(a [8]float32) [32]byte


// Castps128Ps256: Casts vector of type __m128 to type __m256; the upper 128
// bits of the result are undefined. This intrinsic is only used for
// compilation and does not generate any instructions, thus it has zero
// latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps128_ps256'.
// Requires AVX.
func Castps128Ps256(a M128) M256 {
	return M256(castps128Ps256([4]float32(a)))
}

func castps128Ps256(a [4]float32) [8]float32


// Castps256Ps128: Casts vector of type __m256 to type __m128. This intrinsic
// is only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castps256_ps128'.
// Requires AVX.
func Castps256Ps128(a M256) M128 {
	return M128(castps256Ps128([8]float32(a)))
}

func castps256Ps128(a [8]float32) [4]float32


// Castsi128Si256: Casts vector of type __m128i to type __m256i; the upper 128
// bits of the result are undefined. This intrinsic is only used for
// compilation and does not generate any instructions, thus it has zero
// latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi128_si256'.
// Requires AVX.
func Castsi128Si256(a M128i) M256i {
	return M256i(castsi128Si256([16]byte(a)))
}

func castsi128Si256(a [16]byte) [32]byte


// Castsi256Pd: Casts vector of type __m256i to type __m256d. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi256_pd'.
// Requires AVX.
func Castsi256Pd(a M256i) M256d {
	return M256d(castsi256Pd([32]byte(a)))
}

func castsi256Pd(a [32]byte) [4]float64


// Castsi256Ps: Casts vector of type __m256i to type __m256. This intrinsic is
// only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi256_ps'.
// Requires AVX.
func Castsi256Ps(a M256i) M256 {
	return M256(castsi256Ps([32]byte(a)))
}

func castsi256Ps(a [32]byte) [8]float32


// Castsi256Si128: Casts vector of type __m256i to type __m128i. This intrinsic
// is only used for compilation and does not generate any instructions, thus it
// has zero latency. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_castsi256_si128'.
// Requires AVX.
func Castsi256Si128(a M256i) M128i {
	return M128i(castsi256Si128([32]byte(a)))
}

func castsi256Si128(a [32]byte) [16]byte


// CbrtPd: Compute the cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cbrt_pd'.
// Requires AVX.
func CbrtPd(a M256d) M256d {
	return M256d(cbrtPd([4]float64(a)))
}

func cbrtPd(a [4]float64) [4]float64


// CbrtPs: Compute the cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cbrt_ps'.
// Requires AVX.
func CbrtPs(a M256) M256 {
	return M256(cbrtPs([8]float32(a)))
}

func cbrtPs(a [8]float32) [8]float32


// CdfnormPd: Compute the cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorm_pd'.
// Requires AVX.
func CdfnormPd(a M256d) M256d {
	return M256d(cdfnormPd([4]float64(a)))
}

func cdfnormPd(a [4]float64) [4]float64


// CdfnormPs: Compute the cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorm_ps'.
// Requires AVX.
func CdfnormPs(a M256) M256 {
	return M256(cdfnormPs([8]float32(a)))
}

func cdfnormPs(a [8]float32) [8]float32


// CdfnorminvPd: Compute the inverse cumulative distribution function of packed
// double-precision (64-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := InverseCDFNormal(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorminv_pd'.
// Requires AVX.
func CdfnorminvPd(a M256d) M256d {
	return M256d(cdfnorminvPd([4]float64(a)))
}

func cdfnorminvPd(a [4]float64) [4]float64


// CdfnorminvPs: Compute the inverse cumulative distribution function of packed
// single-precision (32-bit) floating-point elements in 'a' using the normal
// distribution, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := InverseCDFNormal(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cdfnorminv_ps'.
// Requires AVX.
func CdfnorminvPs(a M256) M256 {
	return M256(cdfnorminvPs([8]float32(a)))
}

func cdfnorminvPs(a [8]float32) [8]float32


// CeilPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPD'. Intrinsic: '_mm256_ceil_pd'.
// Requires AVX.
func CeilPd(a M256d) M256d {
	return M256d(ceilPd([4]float64(a)))
}

func ceilPd(a [4]float64) [4]float64


// CeilPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm256_ceil_ps'.
// Requires AVX.
func CeilPs(a M256) M256 {
	return M256(ceilPs([8]float32(a)))
}

func ceilPs(a [8]float32) [8]float32


// CexpPs: Compute the exponential value of 'e' raised to the power of packed
// complex single-precision (32-bit) floating-point elements in 'a', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cexp_ps'.
// Requires AVX.
func CexpPs(a M256) M256 {
	return M256(cexpPs([8]float32(a)))
}

func cexpPs(a [8]float32) [8]float32


// ClogPs: Compute the natural logarithm of packed complex single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_clog_ps'.
// Requires AVX.
func ClogPs(a M256) M256 {
	return M256(clogPs([8]float32(a)))
}

func clogPs(a [8]float32) [8]float32


// CmpEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmp_epi16_mask'.
// Requires AVX512BW.
func CmpEpi16Mask(a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(cmpEpi16Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpi16Mask(a [32]byte, b [32]byte, imm8 int) uint16


// MaskCmpEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmp_epi16_mask'.
// Requires AVX512BW.
func MaskCmpEpi16Mask(k1 Mmask16, a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(maskCmpEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpi16Mask(k1 uint16, a [32]byte, b [32]byte, imm8 int) uint16


// CmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmp_epi32_mask'.
// Requires AVX512F.
func CmpEpi32Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi32Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpi32Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmp_epi32_mask'.
// Requires AVX512F.
func MaskCmpEpi32Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpi32Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmp_epi64_mask'.
// Requires AVX512F.
func CmpEpi64Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpi64Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpi64Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmp_epi64_mask'.
// Requires AVX512F.
func MaskCmpEpi64Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpi64Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmp_epi8_mask'.
// Requires AVX512BW.
func CmpEpi8Mask(a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(cmpEpi8Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpi8Mask(a [32]byte, b [32]byte, imm8 int) uint32


// MaskCmpEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' based on the
// comparison operand specified by 'imm8', and store the results in mask vector
// 'k1' using zeromask 'k' (elements are zeroed out when the corresponding mask
// bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmp_epi8_mask'.
// Requires AVX512BW.
func MaskCmpEpi8Mask(k1 Mmask32, a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(maskCmpEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpi8Mask(k1 uint32, a [32]byte, b [32]byte, imm8 int) uint32


// CmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmp_epu16_mask'.
// Requires AVX512BW.
func CmpEpu16Mask(a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(cmpEpu16Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu16Mask(a [32]byte, b [32]byte, imm8 int) uint16


// MaskCmpEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] OP b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmp_epu16_mask'.
// Requires AVX512BW.
func MaskCmpEpu16Mask(k1 Mmask16, a M256i, b M256i, imm8 int) Mmask16 {
	return Mmask16(maskCmpEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu16Mask(k1 uint16, a [32]byte, b [32]byte, imm8 int) uint16


// CmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmp_epu32_mask'.
// Requires AVX512F.
func CmpEpu32Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu32Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu32Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmp_epu32_mask'.
// Requires AVX512F.
func MaskCmpEpu32Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu32Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmp_epu64_mask'.
// Requires AVX512F.
func CmpEpu64Mask(a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(cmpEpu64Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu64Mask(a [32]byte, b [32]byte, imm8 uint8) uint8


// MaskCmpEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// based on the comparison operand specified by 'imm8', and store the results
// in mask vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmp_epu64_mask'.
// Requires AVX512F.
func MaskCmpEpu64Mask(k1 Mmask8, a M256i, b M256i, imm8 uint8) Mmask8 {
	return Mmask8(maskCmpEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu64Mask(k1 uint8, a [32]byte, b [32]byte, imm8 uint8) uint8


// CmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based on
// the comparison operand specified by 'imm8', and store the results in mask
// vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmp_epu8_mask'.
// Requires AVX512BW.
func CmpEpu8Mask(a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(cmpEpu8Mask([32]byte(a), [32]byte(b), imm8))
}

func cmpEpu8Mask(a [32]byte, b [32]byte, imm8 int) uint32


// MaskCmpEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' based
// on the comparison operand specified by 'imm8', and store the results in mask
// vector 'k1' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _MM_CMPINT_EQ
//		1: OP := _MM_CMPINT_LT
//		2: OP := _MM_CMPINT_LE
//		3: OP := _MM_CMPINT_FALSE
//		4: OP := _MM_CMPINT_NEQ
//		5: OP := _MM_CMPINT_NLT
//		6: OP := _MM_CMPINT_NLE
//		7: OP := _MM_CMPINT_TRUE
//		ESAC
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] OP b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmp_epu8_mask'.
// Requires AVX512BW.
func MaskCmpEpu8Mask(k1 Mmask32, a M256i, b M256i, imm8 int) Mmask32 {
	return Mmask32(maskCmpEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b), imm8))
}

func maskCmpEpu8Mask(k1 uint32, a [32]byte, b [32]byte, imm8 int) uint32


// CmpPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b' based on the comparison operand specified by 'imm8', and store
// the results in 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] OP b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_cmp_pd'.
// Requires AVX.
func CmpPd(a M256d, b M256d, imm8 int) M256d {
	return M256d(cmpPd([4]float64(a), [4]float64(b), imm8))
}

func cmpPd(a [4]float64, b [4]float64, imm8 int) [4]float64


// CmpPdMask: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := (a[i+63:i] OP b[i+63:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_cmp_pd_mask'.
// Requires AVX512F.
func CmpPdMask(a M256d, b M256d, imm8 int) Mmask8 {
	return Mmask8(cmpPdMask([4]float64(a), [4]float64(b), imm8))
}

func cmpPdMask(a [4]float64, b [4]float64, imm8 int) uint8


// MaskCmpPdMask: Compare packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] OP b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VCMPPD'. Intrinsic: '_mm256_mask_cmp_pd_mask'.
// Requires AVX512F.
func MaskCmpPdMask(k1 Mmask8, a M256d, b M256d, imm8 int) Mmask8 {
	return Mmask8(maskCmpPdMask(uint8(k1), [4]float64(a), [4]float64(b), imm8))
}

func maskCmpPdMask(k1 uint8, a [4]float64, b [4]float64, imm8 int) uint8


// CmpPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b' based on the comparison operand specified by 'imm8', and store
// the results in 'dst'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] OP b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_cmp_ps'.
// Requires AVX.
func CmpPs(a M256, b M256, imm8 int) M256 {
	return M256(cmpPs([8]float32(a), [8]float32(b), imm8))
}

func cmpPs(a [8]float32, b [8]float32, imm8 int) [8]float32


// CmpPsMask: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' based on the comparison operand specified by 'imm8', and
// store the results in mask vector 'k'. 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := (a[i+31:i] OP b[i+31:i]) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_cmp_ps_mask'.
// Requires AVX512F.
func CmpPsMask(a M256, b M256, imm8 int) Mmask8 {
	return Mmask8(cmpPsMask([8]float32(a), [8]float32(b), imm8))
}

func cmpPsMask(a [8]float32, b [8]float32, imm8 int) uint8


// MaskCmpPsMask: Compare packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' based on the comparison operand specified by 'imm8',
// and store the results in mask vector 'k' using zeromask 'k1' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		CASE (imm8[7:0]) OF
//		0: OP := _CMP_EQ_OQ
//		1: OP := _CMP_LT_OS
//		2: OP := _CMP_LE_OS
//		3: OP := _CMP_UNORD_Q 
//		4: OP := _CMP_NEQ_UQ
//		5: OP := _CMP_NLT_US
//		6: OP := _CMP_NLE_US
//		7: OP := _CMP_ORD_Q
//		8: OP := _CMP_EQ_UQ
//		9: OP := _CMP_NGE_US
//		10: OP := _CMP_NGT_US
//		11: OP := _CMP_FALSE_OQ
//		12: OP := _CMP_NEQ_OQ
//		13: OP := _CMP_GE_OS
//		14: OP := _CMP_GT_OS
//		15: OP := _CMP_TRUE_UQ
//		16: OP := _CMP_EQ_OS
//		17: OP := _CMP_LT_OQ
//		18: OP := _CMP_LE_OQ
//		19: OP := _CMP_UNORD_S
//		20: OP := _CMP_NEQ_US
//		21: OP := _CMP_NLT_UQ
//		22: OP := _CMP_NLE_UQ
//		23: OP := _CMP_ORD_S
//		24: OP := _CMP_EQ_US
//		25: OP := _CMP_NGE_UQ 
//		26: OP := _CMP_NGT_UQ 
//		27: OP := _CMP_FALSE_OS 
//		28: OP := _CMP_NEQ_OS 
//		29: OP := _CMP_GE_OQ
//		30: OP := _CMP_GT_OQ
//		31: OP := _CMP_TRUE_US
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] OP b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VCMPPS'. Intrinsic: '_mm256_mask_cmp_ps_mask'.
// Requires AVX512F.
func MaskCmpPsMask(k1 Mmask8, a M256, b M256, imm8 int) Mmask8 {
	return Mmask8(maskCmpPsMask(uint8(k1), [8]float32(a), [8]float32(b), imm8))
}

func maskCmpPsMask(k1 uint8, a [8]float32, b [8]float32, imm8 int) uint8


// CmpeqEpi16: Compare packed 16-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := ( a[i+15:i] == b[i+15:i] ) ? 0xFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQW'. Intrinsic: '_mm256_cmpeq_epi16'.
// Requires AVX2.
func CmpeqEpi16(a M256i, b M256i) M256i {
	return M256i(cmpeqEpi16([32]byte(a), [32]byte(b)))
}

func cmpeqEpi16(a [32]byte, b [32]byte) [32]byte


// CmpeqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpeq_epi16_mask'.
// Requires AVX512BW.
func CmpeqEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpeqEpi16Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpeqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpeq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpeqEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpeqEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpeqEpi32: Compare packed 32-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] == b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQD'. Intrinsic: '_mm256_cmpeq_epi32'.
// Requires AVX2.
func CmpeqEpi32(a M256i, b M256i) M256i {
	return M256i(cmpeqEpi32([32]byte(a), [32]byte(b)))
}

func cmpeqEpi32(a [32]byte, b [32]byte) [32]byte


// CmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpeq_epi32_mask'.
// Requires AVX512F.
func CmpeqEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpi32Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpeq_epi32_mask'.
// Requires AVX512F.
func MaskCmpeqEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpi64: Compare packed 64-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] == b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQQ'. Intrinsic: '_mm256_cmpeq_epi64'.
// Requires AVX2.
func CmpeqEpi64(a M256i, b M256i) M256i {
	return M256i(cmpeqEpi64([32]byte(a), [32]byte(b)))
}

func cmpeqEpi64(a [32]byte, b [32]byte) [32]byte


// CmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpeq_epi64_mask'.
// Requires AVX512F.
func CmpeqEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpi64Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpeq_epi64_mask'.
// Requires AVX512F.
func MaskCmpeqEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpi8: Compare packed 8-bit integers in 'a' and 'b' for equality, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := ( a[i+7:i] == b[i+7:i] ) ? 0xFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPEQB'. Intrinsic: '_mm256_cmpeq_epi8'.
// Requires AVX2.
func CmpeqEpi8(a M256i, b M256i) M256i {
	return M256i(cmpeqEpi8([32]byte(a), [32]byte(b)))
}

func cmpeqEpi8(a [32]byte, b [32]byte) [32]byte


// CmpeqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for equality,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpeq_epi8_mask'.
// Requires AVX512BW.
func CmpeqEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpeqEpi8Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpi8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpeqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpeq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpeqEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpeqEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpeq_epu16_mask'.
// Requires AVX512BW.
func CmpeqEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpeqEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpeqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] == b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpeq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpeqEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpeq_epu32_mask'.
// Requires AVX512F.
func CmpeqEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] == b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpeq_epu32_mask'.
// Requires AVX512F.
func MaskCmpeqEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpeq_epu64_mask'.
// Requires AVX512F.
func CmpeqEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpeqEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpeqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for equality, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] == b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpeq_epu64_mask'.
// Requires AVX512F.
func MaskCmpeqEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpeqEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpeq_epu8_mask'.
// Requires AVX512BW.
func CmpeqEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpeqEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpeqEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpeqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// equality, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] == b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpeq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpeqEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpeqEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpeqEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpgeEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpge_epi16_mask'.
// Requires AVX512BW.
func CmpgeEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpgeEpi16Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpgeEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpge_epi16_mask'.
// Requires AVX512BW.
func MaskCmpgeEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpgeEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpge_epi32_mask'.
// Requires AVX512F.
func CmpgeEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpi32Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpge_epi32_mask'.
// Requires AVX512F.
func MaskCmpgeEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpge_epi64_mask'.
// Requires AVX512F.
func CmpgeEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpi64Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpge_epi64_mask'.
// Requires AVX512F.
func MaskCmpgeEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpge_epi8_mask'.
// Requires AVX512BW.
func CmpgeEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpgeEpi8Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpi8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpgeEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpge_epi8_mask'.
// Requires AVX512BW.
func MaskCmpgeEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpgeEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpge_epu16_mask'.
// Requires AVX512BW.
func CmpgeEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpgeEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpgeEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] >= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpge_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpgeEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpge_epu32_mask'.
// Requires AVX512F.
func CmpgeEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] >= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpge_epu32_mask'.
// Requires AVX512F.
func MaskCmpgeEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpge_epu64_mask'.
// Requires AVX512F.
func CmpgeEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgeEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgeEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] >= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpge_epu64_mask'.
// Requires AVX512F.
func MaskCmpgeEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgeEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpge_epu8_mask'.
// Requires AVX512BW.
func CmpgeEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpgeEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpgeEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpgeEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than-or-equal, and store the results in mask vector 'k' using
// zeromask 'k1' (elements are zeroed out when the corresponding mask bit is
// not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] >= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpge_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgeEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpgeEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgeEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpgtEpi16: Compare packed 16-bit integers in 'a' and 'b' for greater-than,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := ( a[i+15:i] > b[i+15:i] ) ? 0xFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTW'. Intrinsic: '_mm256_cmpgt_epi16'.
// Requires AVX2.
func CmpgtEpi16(a M256i, b M256i) M256i {
	return M256i(cmpgtEpi16([32]byte(a), [32]byte(b)))
}

func cmpgtEpi16(a [32]byte, b [32]byte) [32]byte


// CmpgtEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpgt_epi16_mask'.
// Requires AVX512BW.
func CmpgtEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpgtEpi16Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpgtEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpgt_epi16_mask'.
// Requires AVX512BW.
func MaskCmpgtEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpgtEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpgtEpi32: Compare packed 32-bit integers in 'a' and 'b' for greater-than,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ( a[i+31:i] > b[i+31:i] ) ? 0xFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTD'. Intrinsic: '_mm256_cmpgt_epi32'.
// Requires AVX2.
func CmpgtEpi32(a M256i, b M256i) M256i {
	return M256i(cmpgtEpi32([32]byte(a), [32]byte(b)))
}

func cmpgtEpi32(a [32]byte, b [32]byte) [32]byte


// CmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpgt_epi32_mask'.
// Requires AVX512F.
func CmpgtEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpi32Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpgt_epi32_mask'.
// Requires AVX512F.
func MaskCmpgtEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpi64: Compare packed 64-bit integers in 'a' and 'b' for greater-than,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ( a[i+63:i] > b[i+63:i] ) ? 0xFFFFFFFFFFFFFFFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTQ'. Intrinsic: '_mm256_cmpgt_epi64'.
// Requires AVX2.
func CmpgtEpi64(a M256i, b M256i) M256i {
	return M256i(cmpgtEpi64([32]byte(a), [32]byte(b)))
}

func cmpgtEpi64(a [32]byte, b [32]byte) [32]byte


// CmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpgt_epi64_mask'.
// Requires AVX512F.
func CmpgtEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpi64Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpgt_epi64_mask'.
// Requires AVX512F.
func MaskCmpgtEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpi8: Compare packed 8-bit integers in 'a' and 'b' for greater-than,
// and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := ( a[i+7:i] > b[i+7:i] ) ? 0xFF : 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCMPGTB'. Intrinsic: '_mm256_cmpgt_epi8'.
// Requires AVX2.
func CmpgtEpi8(a M256i, b M256i) M256i {
	return M256i(cmpgtEpi8([32]byte(a), [32]byte(b)))
}

func cmpgtEpi8(a [32]byte, b [32]byte) [32]byte


// CmpgtEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpgt_epi8_mask'.
// Requires AVX512BW.
func CmpgtEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpgtEpi8Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpi8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpgtEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpgt_epi8_mask'.
// Requires AVX512BW.
func MaskCmpgtEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpgtEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpgt_epu16_mask'.
// Requires AVX512BW.
func CmpgtEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpgtEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpgtEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] > b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpgt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpgtEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpgt_epu32_mask'.
// Requires AVX512F.
func CmpgtEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] > b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpgt_epu32_mask'.
// Requires AVX512F.
func MaskCmpgtEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpgt_epu64_mask'.
// Requires AVX512F.
func CmpgtEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpgtEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpgtEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for greater-than, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] > b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpgt_epu64_mask'.
// Requires AVX512F.
func MaskCmpgtEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpgtEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpgt_epu8_mask'.
// Requires AVX512BW.
func CmpgtEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpgtEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpgtEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpgtEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// greater-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] > b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpgt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpgtEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpgtEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpgtEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpleEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmple_epi16_mask'.
// Requires AVX512BW.
func CmpleEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpleEpi16Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpleEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmple_epi16_mask'.
// Requires AVX512BW.
func MaskCmpleEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpleEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmple_epi32_mask'.
// Requires AVX512F.
func CmpleEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpi32Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpleEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmple_epi32_mask'.
// Requires AVX512F.
func MaskCmpleEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmple_epi64_mask'.
// Requires AVX512F.
func CmpleEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpi64Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpleEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k1' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmple_epi64_mask'.
// Requires AVX512F.
func MaskCmpleEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmple_epi8_mask'.
// Requires AVX512BW.
func CmpleEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpleEpi8Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpi8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpleEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmple_epi8_mask'.
// Requires AVX512BW.
func MaskCmpleEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpleEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmple_epu16_mask'.
// Requires AVX512BW.
func CmpleEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpleEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpleEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] <= b[i+15:i] ) ? 1 : 0
//			ELSE 
//					k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmple_epu16_mask'.
// Requires AVX512BW.
func MaskCmpleEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpleEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmple_epu32_mask'.
// Requires AVX512F.
func CmpleEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpleEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] <= b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmple_epu32_mask'.
// Requires AVX512F.
func MaskCmpleEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmple_epu64_mask'.
// Requires AVX512F.
func CmpleEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpleEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpleEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than-or-equal, and store the results in mask vector 'k1' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] <= b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmple_epu64_mask'.
// Requires AVX512F.
func MaskCmpleEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpleEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmple_epu8_mask'.
// Requires AVX512BW.
func CmpleEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpleEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpleEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpleEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than-or-equal, and store the results in mask vector 'k' using zeromask
// 'k1' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] <= b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmple_epu8_mask'.
// Requires AVX512BW.
func MaskCmpleEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpleEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpleEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpltEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmplt_epi16_mask'.
// Requires AVX512BW.
func CmpltEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpltEpi16Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpltEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmplt_epi16_mask'.
// Requires AVX512BW.
func MaskCmpltEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpltEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmplt_epi32_mask'.
// Requires AVX512F.
func CmpltEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpi32Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpltEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmplt_epi32_mask'.
// Requires AVX512F.
func MaskCmpltEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmplt_epi64_mask'.
// Requires AVX512F.
func CmpltEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpi64Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpltEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmplt_epi64_mask'.
// Requires AVX512F.
func MaskCmpltEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for less-than,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmplt_epi8_mask'.
// Requires AVX512BW.
func CmpltEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpltEpi8Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpi8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpltEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmplt_epi8_mask'.
// Requires AVX512BW.
func MaskCmpltEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpltEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmplt_epu16_mask'.
// Requires AVX512BW.
func CmpltEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpltEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpltEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] < b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmplt_epu16_mask'.
// Requires AVX512BW.
func MaskCmpltEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpltEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmplt_epu32_mask'.
// Requires AVX512F.
func CmpltEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpltEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] < b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmplt_epu32_mask'.
// Requires AVX512F.
func MaskCmpltEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmplt_epu64_mask'.
// Requires AVX512F.
func CmpltEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpltEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpltEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for less-than, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] < b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmplt_epu64_mask'.
// Requires AVX512F.
func MaskCmpltEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpltEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmplt_epu8_mask'.
// Requires AVX512BW.
func CmpltEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpltEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpltEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpltEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// less-than, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] < b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmplt_epu8_mask'.
// Requires AVX512BW.
func MaskCmpltEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpltEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpltEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpneqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_cmpneq_epi16_mask'.
// Requires AVX512BW.
func CmpneqEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpneqEpi16Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpneqEpi16Mask: Compare packed 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPW'. Intrinsic: '_mm256_mask_cmpneq_epi16_mask'.
// Requires AVX512BW.
func MaskCmpneqEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpneqEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_cmpneq_epi32_mask'.
// Requires AVX512F.
func CmpneqEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpi32Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpi32Mask: Compare packed 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPD'. Intrinsic: '_mm256_mask_cmpneq_epi32_mask'.
// Requires AVX512F.
func MaskCmpneqEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_cmpneq_epi64_mask'.
// Requires AVX512F.
func CmpneqEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpi64Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpi64Mask: Compare packed 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPQ'. Intrinsic: '_mm256_mask_cmpneq_epi64_mask'.
// Requires AVX512F.
func MaskCmpneqEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for not-equal,
// and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_cmpneq_epi8_mask'.
// Requires AVX512BW.
func CmpneqEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpneqEpi8Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpi8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpneqEpi8Mask: Compare packed 8-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPB'. Intrinsic: '_mm256_mask_cmpneq_epi8_mask'.
// Requires AVX512BW.
func MaskCmpneqEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpneqEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// CmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_cmpneq_epu16_mask'.
// Requires AVX512BW.
func CmpneqEpu16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(cmpneqEpu16Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu16Mask(a [32]byte, b [32]byte) uint16


// MaskCmpneqEpu16Mask: Compare packed unsigned 16-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ( a[i+15:i] != b[i+15:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPCMPUW'. Intrinsic: '_mm256_mask_cmpneq_epu16_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskCmpneqEpu16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// CmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_cmpneq_epu32_mask'.
// Requires AVX512F.
func CmpneqEpu32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpu32Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu32Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpu32Mask: Compare packed unsigned 32-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ( a[i+31:i] != b[i+31:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPCMPUD'. Intrinsic: '_mm256_mask_cmpneq_epu32_mask'.
// Requires AVX512F.
func MaskCmpneqEpu32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpu32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_cmpneq_epu64_mask'.
// Requires AVX512F.
func CmpneqEpu64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(cmpneqEpu64Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu64Mask(a [32]byte, b [32]byte) uint8


// MaskCmpneqEpu64Mask: Compare packed unsigned 64-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k1' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ( a[i+63:i] != b[i+63:i] ) ? 1 : 0
//			ELSE 
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPCMPUQ'. Intrinsic: '_mm256_mask_cmpneq_epu64_mask'.
// Requires AVX512F.
func MaskCmpneqEpu64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskCmpneqEpu64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// CmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b' for
// not-equal, and store the results in mask vector 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_cmpneq_epu8_mask'.
// Requires AVX512BW.
func CmpneqEpu8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(cmpneqEpu8Mask([32]byte(a), [32]byte(b)))
}

func cmpneqEpu8Mask(a [32]byte, b [32]byte) uint32


// MaskCmpneqEpu8Mask: Compare packed unsigned 8-bit integers in 'a' and 'b'
// for not-equal, and store the results in mask vector 'k' using zeromask 'k1'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ( a[i+7:i] != b[i+7:i] ) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPCMPUB'. Intrinsic: '_mm256_mask_cmpneq_epu8_mask'.
// Requires AVX512BW.
func MaskCmpneqEpu8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskCmpneqEpu8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskCmpneqEpu8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// MaskCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_mask_compress_epi32'.
// Requires AVX512F.
func MaskCompressEpi32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskCompressEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskCompressEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzCompressEpi32: Contiguously store the active 32-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_maskz_compress_epi32'.
// Requires AVX512F.
func MaskzCompressEpi32(k Mmask8, a M256i) M256i {
	return M256i(maskzCompressEpi32(uint8(k), [32]byte(a)))
}

func maskzCompressEpi32(k uint8, a [32]byte) [32]byte


// MaskCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in writemask 'k') to 'dst', and pass
// through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_mask_compress_epi64'.
// Requires AVX512F.
func MaskCompressEpi64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskCompressEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskCompressEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzCompressEpi64: Contiguously store the active 64-bit integers in 'a'
// (those with their respective bit set in zeromask 'k') to 'dst', and set the
// remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_maskz_compress_epi64'.
// Requires AVX512F.
func MaskzCompressEpi64(k Mmask8, a M256i) M256i {
	return M256i(maskzCompressEpi64(uint8(k), [32]byte(a)))
}

func maskzCompressEpi64(k uint8, a [32]byte) [32]byte


// MaskCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_mask_compress_pd'.
// Requires AVX512F.
func MaskCompressPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskCompressPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskCompressPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzCompressPd: Contiguously store the active double-precision (64-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 64
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_maskz_compress_pd'.
// Requires AVX512F.
func MaskzCompressPd(k Mmask8, a M256d) M256d {
	return M256d(maskzCompressPd(uint8(k), [4]float64(a)))
}

func maskzCompressPd(k uint8, a [4]float64) [4]float64


// MaskCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// writemask 'k') to 'dst', and pass through the remaining elements from 'src'. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := src[255:m]
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_mask_compress_ps'.
// Requires AVX512F.
func MaskCompressPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskCompressPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskCompressPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzCompressPs: Contiguously store the active single-precision (32-bit)
// floating-point elements in 'a' (those with their respective bit set in
// zeromask 'k') to 'dst', and set the remaining elements to zero. 
//
//		size := 32
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//		dst[255:m] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_maskz_compress_ps'.
// Requires AVX512F.
func MaskzCompressPs(k Mmask8, a M256) M256 {
	return M256(maskzCompressPs(uint8(k), [8]float32(a)))
}

func maskzCompressPs(k uint8, a [8]float32) [8]float32


// MaskCompressstoreuEpi32: Contiguously store the active 32-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSD'. Intrinsic: '_mm256_mask_compressstoreu_epi32'.
// Requires AVX512F.
func MaskCompressstoreuEpi32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCompressstoreuEpi32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCompressstoreuEpi32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCompressstoreuEpi64: Contiguously store the active 64-bit integers in
// 'a' (those with their respective bit set in writemask 'k') to unaligned
// memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VPCOMPRESSQ'. Intrinsic: '_mm256_mask_compressstoreu_epi64'.
// Requires AVX512F.
func MaskCompressstoreuEpi64(base_addr uintptr, k Mmask8, a M256i)  {
	maskCompressstoreuEpi64(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCompressstoreuEpi64(base_addr uintptr, k uint8, a [32]byte) 


// MaskCompressstoreuPd: Contiguously store the active double-precision
// (64-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 64
//		m := base_addr
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[m+size-1:m] := a[i+63:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPD'. Intrinsic: '_mm256_mask_compressstoreu_pd'.
// Requires AVX512F.
func MaskCompressstoreuPd(base_addr uintptr, k Mmask8, a M256d)  {
	maskCompressstoreuPd(uintptr(base_addr), uint8(k), [4]float64(a))
}

func maskCompressstoreuPd(base_addr uintptr, k uint8, a [4]float64) 


// MaskCompressstoreuPs: Contiguously store the active single-precision
// (32-bit) floating-point elements in 'a' (those with their respective bit set
// in writemask 'k') to unaligned memory at 'base_addr'. 
//
//		size := 32
//		m := base_addr
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[m+size-1:m] := a[i+31:i]
//				m := m + size
//			FI
//		ENDFOR
//
// Instruction: 'VCOMPRESSPS'. Intrinsic: '_mm256_mask_compressstoreu_ps'.
// Requires AVX512F.
func MaskCompressstoreuPs(base_addr uintptr, k Mmask8, a M256)  {
	maskCompressstoreuPs(uintptr(base_addr), uint8(k), [8]float32(a))
}

func maskCompressstoreuPs(base_addr uintptr, k uint8, a [8]float32) 


// ConflictEpi32: Test each 32-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			FOR k := 0 to j-1
//				m := k*32
//				dst[i+k] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//			ENDFOR
//			dst[i+31:i+j] := 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm256_conflict_epi32'.
// Requires AVX512CD.
func ConflictEpi32(a M256i) M256i {
	return M256i(conflictEpi32([32]byte(a)))
}

func conflictEpi32(a [32]byte) [32]byte


// MaskConflictEpi32: Test each 32-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm256_mask_conflict_epi32'.
// Requires AVX512CD.
func MaskConflictEpi32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskConflictEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskConflictEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzConflictEpi32: Test each 32-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[i]
//				FOR l := 0 to j-1
//					m := l*32
//					dst[i+l] := (a[i+31:i] == a[m+31:m]) ? 1 : 0
//				ENDFOR
//				dst[i+31:i+j] := 0
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTD'. Intrinsic: '_mm256_maskz_conflict_epi32'.
// Requires AVX512CD.
func MaskzConflictEpi32(k Mmask8, a M256i) M256i {
	return M256i(maskzConflictEpi32(uint8(k), [32]byte(a)))
}

func maskzConflictEpi32(k uint8, a [32]byte) [32]byte


// ConflictEpi64: Test each 64-bit element of 'a' for equality with all other
// elements in 'a' closer to the least significant bit. Each element's
// comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			FOR k := 0 to j-1
//				m := k*64
//				dst[i+k] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//			ENDFOR
//			dst[i+63:i+j] := 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm256_conflict_epi64'.
// Requires AVX512CD.
func ConflictEpi64(a M256i) M256i {
	return M256i(conflictEpi64([32]byte(a)))
}

func conflictEpi64(a [32]byte) [32]byte


// MaskConflictEpi64: Test each 64-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). Each element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm256_mask_conflict_epi64'.
// Requires AVX512CD.
func MaskConflictEpi64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskConflictEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskConflictEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzConflictEpi64: Test each 64-bit element of 'a' for equality with all
// other elements in 'a' closer to the least significant bit using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). Each
// element's comparison forms a zero extended bit vector in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR l := 0 to j-1
//					m := l*64
//					dst[i+l] := (a[i+63:i] == a[m+63:m]) ? 1 : 0
//				ENDFOR
//				dst[i+63:i+j] := 0
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPCONFLICTQ'. Intrinsic: '_mm256_maskz_conflict_epi64'.
// Requires AVX512CD.
func MaskzConflictEpi64(k Mmask8, a M256i) M256i {
	return M256i(maskzConflictEpi64(uint8(k), [32]byte(a)))
}

func maskzConflictEpi64(k uint8, a [32]byte) [32]byte


// CosPd: Compute the cosine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cos_pd'.
// Requires AVX.
func CosPd(a M256d) M256d {
	return M256d(cosPd([4]float64(a)))
}

func cosPd(a [4]float64) [4]float64


// CosPs: Compute the cosine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cos_ps'.
// Requires AVX.
func CosPs(a M256) M256 {
	return M256(cosPs([8]float32(a)))
}

func cosPs(a [8]float32) [8]float32


// CosdPd: Compute the cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := COSD(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosd_pd'.
// Requires AVX.
func CosdPd(a M256d) M256d {
	return M256d(cosdPd([4]float64(a)))
}

func cosdPd(a [4]float64) [4]float64


// CosdPs: Compute the cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := COSD(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosd_ps'.
// Requires AVX.
func CosdPs(a M256) M256 {
	return M256(cosdPs([8]float32(a)))
}

func cosdPs(a [8]float32) [8]float32


// CoshPd: Compute the hyperbolic cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := COSH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosh_pd'.
// Requires AVX.
func CoshPd(a M256d) M256d {
	return M256d(coshPd([4]float64(a)))
}

func coshPd(a [4]float64) [4]float64


// CoshPs: Compute the hyperbolic cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := COSH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_cosh_ps'.
// Requires AVX.
func CoshPs(a M256) M256 {
	return M256(coshPs([8]float32(a)))
}

func coshPs(a [8]float32) [8]float32


// CsqrtPs: Compute the square root of packed complex single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_csqrt_ps'.
// Requires AVX.
func CsqrtPs(a M256) M256 {
	return M256(csqrtPs([8]float32(a)))
}

func csqrtPs(a [8]float32) [8]float32


// MaskCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvt_roundps_ph'.
// Requires AVX512F.
func MaskCvtRoundpsPh(src M128i, k Mmask8, a M256, rounding int) M128i {
	return M128i(maskCvtRoundpsPh([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtRoundpsPh(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// MaskzCvtRoundpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvt_roundps_ph'.
// Requires AVX512F.
func MaskzCvtRoundpsPh(k Mmask8, a M256, rounding int) M128i {
	return M128i(maskzCvtRoundpsPh(uint8(k), [8]float32(a), rounding))
}

func maskzCvtRoundpsPh(k uint8, a [8]float32, rounding int) [16]byte


// Cvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_cvtepi16_epi32'.
// Requires AVX2.
func Cvtepi16Epi32(a M128i) M256i {
	return M256i(cvtepi16Epi32([16]byte(a)))
}

func cvtepi16Epi32(a [16]byte) [32]byte


// MaskCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			l := j*16
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_mask_cvtepi16_epi32'.
// Requires AVX512F.
func MaskCvtepi16Epi32(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi16Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi16Epi32: Sign extend packed 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWD'. Intrinsic: '_mm256_maskz_cvtepi16_epi32'.
// Requires AVX512F.
func MaskzCvtepi16Epi32(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi16Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi32(k uint8, a [16]byte) [32]byte


// Cvtepi16Epi64: Sign extend packed 16-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := SignExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_cvtepi16_epi64'.
// Requires AVX2.
func Cvtepi16Epi64(a M128i) M256i {
	return M256i(cvtepi16Epi64([16]byte(a)))
}

func cvtepi16Epi64(a [16]byte) [32]byte


// MaskCvtepi16Epi64: Sign extend packed 16-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_mask_cvtepi16_epi64'.
// Requires AVX512F.
func MaskCvtepi16Epi64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi16Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi16Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi16Epi64: Sign extend packed 16-bit integers in the low 8 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXWQ'. Intrinsic: '_mm256_maskz_cvtepi16_epi64'.
// Requires AVX512F.
func MaskzCvtepi16Epi64(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi16Epi64(k uint8, a [16]byte) [32]byte


// Cvtepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_cvtepi16_epi8'.
// Requires AVX512BW.
func Cvtepi16Epi8(a M256i) M128i {
	return M128i(cvtepi16Epi8([32]byte(a)))
}

func cvtepi16Epi8(a [32]byte) [16]byte


// MaskCvtepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_mask_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskCvtepi16Epi8(src M128i, k Mmask16, a M256i) M128i {
	return M128i(maskCvtepi16Epi8([16]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtepi16Epi8(src [16]byte, k uint16, a [32]byte) [16]byte


// MaskzCvtepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_maskz_cvtepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtepi16Epi8(k Mmask16, a M256i) M128i {
	return M128i(maskzCvtepi16Epi8(uint16(k), [32]byte(a)))
}

func maskzCvtepi16Epi8(k uint16, a [32]byte) [16]byte


// MaskCvtepi16StoreuEpi8: Convert packed 16-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVWB'. Intrinsic: '_mm256_mask_cvtepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtepi16StoreuEpi8(base_addr uintptr, k Mmask16, a M256i)  {
	maskCvtepi16StoreuEpi8(uintptr(base_addr), uint16(k), [32]byte(a))
}

func maskCvtepi16StoreuEpi8(base_addr uintptr, k uint16, a [32]byte) 


// Cvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_cvtepi32_epi16'.
// Requires AVX512F.
func Cvtepi32Epi16(a M256i) M128i {
	return M128i(cvtepi32Epi16([32]byte(a)))
}

func cvtepi32Epi16(a [32]byte) [16]byte


// MaskCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_mask_cvtepi32_epi16'.
// Requires AVX512F.
func MaskCvtepi32Epi16(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi32Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_maskz_cvtepi32_epi16'.
// Requires AVX512F.
func MaskzCvtepi32Epi16(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi32Epi16(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Epi16(k uint8, a [32]byte) [16]byte


// Cvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed 64-bit
// integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := SignExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_cvtepi32_epi64'.
// Requires AVX2.
func Cvtepi32Epi64(a M128i) M256i {
	return M256i(cvtepi32Epi64([16]byte(a)))
}

func cvtepi32Epi64(a [16]byte) [32]byte


// MaskCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_mask_cvtepi32_epi64'.
// Requires AVX512F.
func MaskCvtepi32Epi64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi32Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi32Epi64: Sign extend packed 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXDQ'. Intrinsic: '_mm256_maskz_cvtepi32_epi64'.
// Requires AVX512F.
func MaskzCvtepi32Epi64(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi32Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Epi64(k uint8, a [16]byte) [32]byte


// Cvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_cvtepi32_epi8'.
// Requires AVX512F.
func Cvtepi32Epi8(a M256i) M128i {
	return M128i(cvtepi32Epi8([32]byte(a)))
}

func cvtepi32Epi8(a [32]byte) [16]byte


// MaskCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_mask_cvtepi32_epi8'.
// Requires AVX512F.
func MaskCvtepi32Epi8(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi32Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_maskz_cvtepi32_epi8'.
// Requires AVX512F.
func MaskzCvtepi32Epi8(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi32Epi8(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Epi8(k uint8, a [32]byte) [16]byte


// Cvtepi32Pd: Convert packed 32-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_cvtepi32_pd'.
// Requires AVX.
func Cvtepi32Pd(a M128i) M256d {
	return M256d(cvtepi32Pd([16]byte(a)))
}

func cvtepi32Pd(a [16]byte) [4]float64


// MaskCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := src[m+63:m]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_mask_cvtepi32_pd'.
// Requires AVX512F.
func MaskCvtepi32Pd(src M256d, k Mmask8, a M128i) M256d {
	return M256d(maskCvtepi32Pd([4]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepi32Pd(src [4]float64, k uint8, a [16]byte) [4]float64


// MaskzCvtepi32Pd: Convert packed 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[m+63:m] := Convert_Int32_To_FP64(a[i+31:i])
//			ELSE
//				dst[m+63:m] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PD'. Intrinsic: '_mm256_maskz_cvtepi32_pd'.
// Requires AVX512F.
func MaskzCvtepi32Pd(k Mmask8, a M128i) M256d {
	return M256d(maskzCvtepi32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepi32Pd(k uint8, a [16]byte) [4]float64


// Cvtepi32Ps: Convert packed 32-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_cvtepi32_ps'.
// Requires AVX.
func Cvtepi32Ps(a M256i) M256 {
	return M256(cvtepi32Ps([32]byte(a)))
}

func cvtepi32Ps(a [32]byte) [8]float32


// MaskCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_mask_cvtepi32_ps'.
// Requires AVX512F.
func MaskCvtepi32Ps(src M256, k Mmask8, a M256i) M256 {
	return M256(maskCvtepi32Ps([8]float32(src), uint8(k), [32]byte(a)))
}

func maskCvtepi32Ps(src [8]float32, k uint8, a [32]byte) [8]float32


// MaskzCvtepi32Ps: Convert packed 32-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_Int32_To_FP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTDQ2PS'. Intrinsic: '_mm256_maskz_cvtepi32_ps'.
// Requires AVX512F.
func MaskzCvtepi32Ps(k Mmask8, a M256i) M256 {
	return M256(maskzCvtepi32Ps(uint8(k), [32]byte(a)))
}

func maskzCvtepi32Ps(k uint8, a [32]byte) [8]float32


// MaskCvtepi32StoreuEpi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVDW'. Intrinsic: '_mm256_mask_cvtepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi32StoreuEpi16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi32StoreuEpi16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi32StoreuEpi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVDB'. Intrinsic: '_mm256_mask_cvtepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi32StoreuEpi8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi32StoreuEpi8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi32StoreuEpi8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Truncate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_cvtepi64_epi16'.
// Requires AVX512F.
func Cvtepi64Epi16(a M256i) M128i {
	return M128i(cvtepi64Epi16([32]byte(a)))
}

func cvtepi64Epi16(a [32]byte) [16]byte


// MaskCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_mask_cvtepi64_epi16'.
// Requires AVX512F.
func MaskCvtepi64Epi16(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi64Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Truncate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_maskz_cvtepi64_epi16'.
// Requires AVX512F.
func MaskzCvtepi64Epi16(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi64Epi16(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Epi16(k uint8, a [32]byte) [16]byte


// Cvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Truncate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_cvtepi64_epi32'.
// Requires AVX512F.
func Cvtepi64Epi32(a M256i) M128i {
	return M128i(cvtepi64Epi32([32]byte(a)))
}

func cvtepi64Epi32(a [32]byte) [16]byte


// MaskCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_mask_cvtepi64_epi32'.
// Requires AVX512F.
func MaskCvtepi64Epi32(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi64Epi32([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Epi32(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Truncate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_maskz_cvtepi64_epi32'.
// Requires AVX512F.
func MaskzCvtepi64Epi32(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi64Epi32(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Epi32(k uint8, a [32]byte) [16]byte


// Cvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit integers
// with truncation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Truncate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_cvtepi64_epi8'.
// Requires AVX512F.
func Cvtepi64Epi8(a M256i) M128i {
	return M128i(cvtepi64Epi8([32]byte(a)))
}

func cvtepi64Epi8(a [32]byte) [16]byte


// MaskCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_mask_cvtepi64_epi8'.
// Requires AVX512F.
func MaskCvtepi64Epi8(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtepi64Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with truncation, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Truncate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_maskz_cvtepi64_epi8'.
// Requires AVX512F.
func MaskzCvtepi64Epi8(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtepi64Epi8(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Epi8(k uint8, a [32]byte) [16]byte


// Cvtepi64Pd: Convert packed 64-bit integers in 'a' to packed double-precision
// (64-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm256_cvtepi64_pd'.
// Requires AVX512DQ.
func Cvtepi64Pd(a M256i) M256d {
	return M256d(cvtepi64Pd([32]byte(a)))
}

func cvtepi64Pd(a [32]byte) [4]float64


// MaskCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm256_mask_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskCvtepi64Pd(src M256d, k Mmask8, a M256i) M256d {
	return M256d(maskCvtepi64Pd([4]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Pd(src [4]float64, k uint8, a [32]byte) [4]float64


// MaskzCvtepi64Pd: Convert packed 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_Int64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTQQ2PD'. Intrinsic: '_mm256_maskz_cvtepi64_pd'.
// Requires AVX512DQ.
func MaskzCvtepi64Pd(k Mmask8, a M256i) M256d {
	return M256d(maskzCvtepi64Pd(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Pd(k uint8, a [32]byte) [4]float64


// Cvtepi64Ps: Convert packed 64-bit integers in 'a' to packed single-precision
// (32-bit) floating-point elements, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm256_cvtepi64_ps'.
// Requires AVX512DQ.
func Cvtepi64Ps(a M256i) M128 {
	return M128(cvtepi64Ps([32]byte(a)))
}

func cvtepi64Ps(a [32]byte) [4]float32


// MaskCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm256_mask_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskCvtepi64Ps(src M128, k Mmask8, a M256i) M128 {
	return M128(maskCvtepi64Ps([4]float32(src), uint8(k), [32]byte(a)))
}

func maskCvtepi64Ps(src [4]float32, k uint8, a [32]byte) [4]float32


// MaskzCvtepi64Ps: Convert packed 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := Convert_Int64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTQQ2PS'. Intrinsic: '_mm256_maskz_cvtepi64_ps'.
// Requires AVX512DQ.
func MaskzCvtepi64Ps(k Mmask8, a M256i) M128 {
	return M128(maskzCvtepi64Ps(uint8(k), [32]byte(a)))
}

func maskzCvtepi64Ps(k uint8, a [32]byte) [4]float32


// MaskCvtepi64StoreuEpi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Truncate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVQW'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64StoreuEpi16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64StoreuEpi16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi64StoreuEpi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Truncate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQD'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64StoreuEpi32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64StoreuEpi32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtepi64StoreuEpi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with truncation, and store the active results (those with
// their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Truncate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVQB'. Intrinsic: '_mm256_mask_cvtepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtepi64StoreuEpi8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtepi64StoreuEpi8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtepi8Epi16: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			dst[l+15:l] := SignExtend(a[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm256_cvtepi8_epi16'.
// Requires AVX2.
func Cvtepi8Epi16(a M128i) M256i {
	return M256i(cvtepi8Epi16([16]byte(a)))
}

func cvtepi8Epi16(a [16]byte) [32]byte


// MaskCvtepi8Epi16: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm256_mask_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskCvtepi8Epi16(src M256i, k Mmask16, a M128i) M256i {
	return M256i(maskCvtepi8Epi16([32]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepi8Epi16(src [32]byte, k uint16, a [16]byte) [32]byte


// MaskzCvtepi8Epi16: Sign extend packed 8-bit integers in 'a' to packed 16-bit
// integers, and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := SignExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBW'. Intrinsic: '_mm256_maskz_cvtepi8_epi16'.
// Requires AVX512BW.
func MaskzCvtepi8Epi16(k Mmask16, a M128i) M256i {
	return M256i(maskzCvtepi8Epi16(uint16(k), [16]byte(a)))
}

func maskzCvtepi8Epi16(k uint16, a [16]byte) [32]byte


// Cvtepi8Epi32: Sign extend packed 8-bit integers in 'a' to packed 32-bit
// integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_cvtepi8_epi32'.
// Requires AVX2.
func Cvtepi8Epi32(a M128i) M256i {
	return M256i(cvtepi8Epi32([16]byte(a)))
}

func cvtepi8Epi32(a [16]byte) [32]byte


// MaskCvtepi8Epi32: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_mask_cvtepi8_epi32'.
// Requires AVX512F.
func MaskCvtepi8Epi32(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi8Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi8Epi32: Sign extend packed 8-bit integers in the low 8 bytes of
// 'a' to packed 32-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBD'. Intrinsic: '_mm256_maskz_cvtepi8_epi32'.
// Requires AVX512F.
func MaskzCvtepi8Epi32(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi8Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi32(k uint8, a [16]byte) [32]byte


// Cvtepi8Epi64: Sign extend packed 8-bit integers in the low 8 bytes of 'a' to
// packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := SignExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_cvtepi8_epi64'.
// Requires AVX2.
func Cvtepi8Epi64(a M128i) M256i {
	return M256i(cvtepi8Epi64([16]byte(a)))
}

func cvtepi8Epi64(a [16]byte) [32]byte


// MaskCvtepi8Epi64: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_mask_cvtepi8_epi64'.
// Requires AVX512F.
func MaskCvtepi8Epi64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepi8Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepi8Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepi8Epi64: Sign extend packed 8-bit integers in the low 4 bytes of
// 'a' to packed 64-bit integers, and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVSXBQ'. Intrinsic: '_mm256_maskz_cvtepi8_epi64'.
// Requires AVX512F.
func MaskzCvtepi8Epi64(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepi8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepi8Epi64(k uint8, a [16]byte) [32]byte


// Cvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[i+31:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_cvtepu16_epi32'.
// Requires AVX2.
func Cvtepu16Epi32(a M128i) M256i {
	return M256i(cvtepu16Epi32([16]byte(a)))
}

func cvtepu16Epi32(a [16]byte) [32]byte


// MaskCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_mask_cvtepu16_epi32'.
// Requires AVX512F.
func MaskCvtepu16Epi32(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu16Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu16Epi32: Zero extend packed unsigned 16-bit integers in 'a' to
// packed 32-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWD'. Intrinsic: '_mm256_maskz_cvtepu16_epi32'.
// Requires AVX512F.
func MaskzCvtepu16Epi32(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu16Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi32(k uint8, a [16]byte) [32]byte


// Cvtepu16Epi64: Zero extend packed unsigned 16-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[i+63:i] := ZeroExtend(a[k+15:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_cvtepu16_epi64'.
// Requires AVX2.
func Cvtepu16Epi64(a M128i) M256i {
	return M256i(cvtepu16Epi64([16]byte(a)))
}

func cvtepu16Epi64(a [16]byte) [32]byte


// MaskCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_mask_cvtepu16_epi64'.
// Requires AVX512F.
func MaskCvtepu16Epi64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu16Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu16Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu16Epi64: Zero extend packed unsigned 16-bit integers in the low 8
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+15:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXWQ'. Intrinsic: '_mm256_maskz_cvtepu16_epi64'.
// Requires AVX512F.
func MaskzCvtepu16Epi64(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu16Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu16Epi64(k uint8, a [16]byte) [32]byte


// Cvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to packed
// 64-bit integers, and store the results in 'dst'. 
//
//		FOR j:= 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := ZeroExtend(a[k+31:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_cvtepu32_epi64'.
// Requires AVX2.
func Cvtepu32Epi64(a M128i) M256i {
	return M256i(cvtepu32Epi64([16]byte(a)))
}

func cvtepu32Epi64(a [16]byte) [32]byte


// MaskCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_mask_cvtepu32_epi64'.
// Requires AVX512F.
func MaskCvtepu32Epi64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu32Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu32Epi64: Zero extend packed unsigned 32-bit integers in 'a' to
// packed 64-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+31:l])
//			ELSE 
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXDQ'. Intrinsic: '_mm256_maskz_cvtepu32_epi64'.
// Requires AVX512F.
func MaskzCvtepu32Epi64(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu32Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Epi64(k uint8, a [16]byte) [32]byte


// Cvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_cvtepu32_pd'.
// Requires AVX512F.
func Cvtepu32Pd(a M128i) M256d {
	return M256d(cvtepu32Pd([16]byte(a)))
}

func cvtepu32Pd(a [16]byte) [4]float64


// MaskCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_mask_cvtepu32_pd'.
// Requires AVX512F.
func MaskCvtepu32Pd(src M256d, k Mmask8, a M128i) M256d {
	return M256d(maskCvtepu32Pd([4]float64(src), uint8(k), [16]byte(a)))
}

func maskCvtepu32Pd(src [4]float64, k uint8, a [16]byte) [4]float64


// MaskzCvtepu32Pd: Convert packed unsigned 32-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedIntegerTo_FP64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUDQ2PD'. Intrinsic: '_mm256_maskz_cvtepu32_pd'.
// Requires AVX512F.
func MaskzCvtepu32Pd(k Mmask8, a M128i) M256d {
	return M256d(maskzCvtepu32Pd(uint8(k), [16]byte(a)))
}

func maskzCvtepu32Pd(k uint8, a [16]byte) [4]float64


// Cvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm256_cvtepu64_pd'.
// Requires AVX512DQ.
func Cvtepu64Pd(a M256i) M256d {
	return M256d(cvtepu64Pd([32]byte(a)))
}

func cvtepu64Pd(a [32]byte) [4]float64


// MaskCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm256_mask_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskCvtepu64Pd(src M256d, k Mmask8, a M256i) M256d {
	return M256d(maskCvtepu64Pd([4]float64(src), uint8(k), [32]byte(a)))
}

func maskCvtepu64Pd(src [4]float64, k uint8, a [32]byte) [4]float64


// MaskzCvtepu64Pd: Convert packed unsigned 64-bit integers in 'a' to packed
// double-precision (64-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertUnsignedInt64_To_FP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTUQQ2PD'. Intrinsic: '_mm256_maskz_cvtepu64_pd'.
// Requires AVX512DQ.
func MaskzCvtepu64Pd(k Mmask8, a M256i) M256d {
	return M256d(maskzCvtepu64Pd(uint8(k), [32]byte(a)))
}

func maskzCvtepu64Pd(k uint8, a [32]byte) [4]float64


// Cvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm256_cvtepu64_ps'.
// Requires AVX512DQ.
func Cvtepu64Ps(a M256i) M128 {
	return M128(cvtepu64Ps([32]byte(a)))
}

func cvtepu64Ps(a [32]byte) [4]float32


// MaskCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm256_mask_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskCvtepu64Ps(src M128, k Mmask8, a M256i) M128 {
	return M128(maskCvtepu64Ps([4]float32(src), uint8(k), [32]byte(a)))
}

func maskCvtepu64Ps(src [4]float32, k uint8, a [32]byte) [4]float32


// MaskzCvtepu64Ps: Convert packed unsigned 64-bit integers in 'a' to packed
// single-precision (32-bit) floating-point elements, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[l+31:l] := ConvertUnsignedInt64_To_FP32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTUQQ2PS'. Intrinsic: '_mm256_maskz_cvtepu64_ps'.
// Requires AVX512DQ.
func MaskzCvtepu64Ps(k Mmask8, a M256i) M128 {
	return M128(maskzCvtepu64Ps(uint8(k), [32]byte(a)))
}

func maskzCvtepu64Ps(k uint8, a [32]byte) [4]float32


// Cvtepu8Epi16: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 16-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			dst[l+15:l] := ZeroExtend(a[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm256_cvtepu8_epi16'.
// Requires AVX2.
func Cvtepu8Epi16(a M128i) M256i {
	return M256i(cvtepu8Epi16([16]byte(a)))
}

func cvtepu8Epi16(a [16]byte) [32]byte


// MaskCvtepu8Epi16: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 16-bit integers, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm256_mask_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskCvtepu8Epi16(src M256i, k Mmask16, a M128i) M256i {
	return M256i(maskCvtepu8Epi16([32]byte(src), uint16(k), [16]byte(a)))
}

func maskCvtepu8Epi16(src [32]byte, k uint16, a [16]byte) [32]byte


// MaskzCvtepu8Epi16: Zero extend packed unsigned 8-bit integers in 'a' to
// packed 16-bit integers, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*8
//			l := j*16
//			IF k[j]
//				dst[l+15:l] := ZeroExtend(a[i+7:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBW'. Intrinsic: '_mm256_maskz_cvtepu8_epi16'.
// Requires AVX512BW.
func MaskzCvtepu8Epi16(k Mmask16, a M128i) M256i {
	return M256i(maskzCvtepu8Epi16(uint16(k), [16]byte(a)))
}

func maskzCvtepu8Epi16(k uint16, a [16]byte) [32]byte


// Cvtepu8Epi32: Zero extend packed unsigned 8-bit integers in 'a' to packed
// 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[i+31:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_cvtepu8_epi32'.
// Requires AVX2.
func Cvtepu8Epi32(a M128i) M256i {
	return M256i(cvtepu8Epi32([16]byte(a)))
}

func cvtepu8Epi32(a [16]byte) [32]byte


// MaskCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_mask_cvtepu8_epi32'.
// Requires AVX512F.
func MaskCvtepu8Epi32(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu8Epi32([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi32(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu8Epi32: Zero extend packed unsigned 8-bit integers in the low 8
// bytes of 'a' to packed 32-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBD'. Intrinsic: '_mm256_maskz_cvtepu8_epi32'.
// Requires AVX512F.
func MaskzCvtepu8Epi32(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu8Epi32(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi32(k uint8, a [16]byte) [32]byte


// Cvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 8 byte
// sof 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[i+63:i] := ZeroExtend(a[k+7:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_cvtepu8_epi64'.
// Requires AVX2.
func Cvtepu8Epi64(a M128i) M256i {
	return M256i(cvtepu8Epi64([16]byte(a)))
}

func cvtepu8Epi64(a [16]byte) [32]byte


// MaskCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_mask_cvtepu8_epi64'.
// Requires AVX512F.
func MaskCvtepu8Epi64(src M256i, k Mmask8, a M128i) M256i {
	return M256i(maskCvtepu8Epi64([32]byte(src), uint8(k), [16]byte(a)))
}

func maskCvtepu8Epi64(src [32]byte, k uint8, a [16]byte) [32]byte


// MaskzCvtepu8Epi64: Zero extend packed unsigned 8-bit integers in the low 4
// bytes of 'a' to packed 64-bit integers, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[l+7:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVZXBQ'. Intrinsic: '_mm256_maskz_cvtepu8_epi64'.
// Requires AVX512F.
func MaskzCvtepu8Epi64(k Mmask8, a M128i) M256i {
	return M256i(maskzCvtepu8Epi64(uint8(k), [16]byte(a)))
}

func maskzCvtepu8Epi64(k uint8, a [16]byte) [32]byte


// CvtpdEpi32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_cvtpd_epi32'.
// Requires AVX.
func CvtpdEpi32(a M256d) M128i {
	return M128i(cvtpdEpi32([4]float64(a)))
}

func cvtpdEpi32(a [4]float64) [16]byte


// MaskCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_mask_cvtpd_epi32'.
// Requires AVX512F.
func MaskCvtpdEpi32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvtpdEpi32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpi32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvtpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2DQ'. Intrinsic: '_mm256_maskz_cvtpd_epi32'.
// Requires AVX512F.
func MaskzCvtpdEpi32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvtpdEpi32(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpi32(k uint8, a [4]float64) [16]byte


// CvtpdEpi64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm256_cvtpd_epi64'.
// Requires AVX512DQ.
func CvtpdEpi64(a M256d) M256i {
	return M256i(cvtpdEpi64([4]float64(a)))
}

func cvtpdEpi64(a [4]float64) [32]byte


// MaskCvtpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm256_mask_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskCvtpdEpi64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvtpdEpi64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpi64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvtpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2QQ'. Intrinsic: '_mm256_maskz_cvtpd_epi64'.
// Requires AVX512DQ.
func MaskzCvtpdEpi64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvtpdEpi64(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpi64(k uint8, a [4]float64) [32]byte


// CvtpdEpu32: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_cvtpd_epu32'.
// Requires AVX512F.
func CvtpdEpu32(a M256d) M128i {
	return M128i(cvtpdEpu32([4]float64(a)))
}

func cvtpdEpu32(a [4]float64) [16]byte


// MaskCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_mask_cvtpd_epu32'.
// Requires AVX512F.
func MaskCvtpdEpu32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvtpdEpu32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpu32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvtpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2UDQ'. Intrinsic: '_mm256_maskz_cvtpd_epu32'.
// Requires AVX512F.
func MaskzCvtpdEpu32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvtpdEpu32(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpu32(k uint8, a [4]float64) [16]byte


// CvtpdEpu64: Convert packed double-precision (64-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm256_cvtpd_epu64'.
// Requires AVX512DQ.
func CvtpdEpu64(a M256d) M256i {
	return M256i(cvtpdEpu64([4]float64(a)))
}

func cvtpdEpu64(a [4]float64) [32]byte


// MaskCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm256_mask_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskCvtpdEpu64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvtpdEpu64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvtpdEpu64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvtpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPD2UQQ'. Intrinsic: '_mm256_maskz_cvtpd_epu64'.
// Requires AVX512DQ.
func MaskzCvtpdEpu64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvtpdEpu64(uint8(k), [4]float64(a)))
}

func maskzCvtpdEpu64(k uint8, a [4]float64) [32]byte


// CvtpdPs: Convert packed double-precision (64-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_FP32(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_cvtpd_ps'.
// Requires AVX.
func CvtpdPs(a M256d) M128 {
	return M128(cvtpdPs([4]float64(a)))
}

func cvtpdPs(a [4]float64) [4]float32


// MaskCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_mask_cvtpd_ps'.
// Requires AVX512F.
func MaskCvtpdPs(src M128, k Mmask8, a M256d) M128 {
	return M128(maskCvtpdPs([4]float32(src), uint8(k), [4]float64(a)))
}

func maskCvtpdPs(src [4]float32, k uint8, a [4]float64) [4]float32


// MaskzCvtpdPs: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed single-precision (32-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_FP32(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPD2PS'. Intrinsic: '_mm256_maskz_cvtpd_ps'.
// Requires AVX512F.
func MaskzCvtpdPs(k Mmask8, a M256d) M128 {
	return M128(maskzCvtpdPs(uint8(k), [4]float64(a)))
}

func maskzCvtpdPs(k uint8, a [4]float64) [4]float32


// CvtphPs: Convert packed half-precision (16-bit) floating-point elements in
// 'a' to packed single-precision (32-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_cvtph_ps'.
// Requires FP16C.
func CvtphPs(a M128i) M256 {
	return M256(cvtphPs([16]byte(a)))
}

func cvtphPs(a [16]byte) [8]float32


// MaskCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_mask_cvtph_ps'.
// Requires AVX512F.
func MaskCvtphPs(src M256, k Mmask8, a M128i) M256 {
	return M256(maskCvtphPs([8]float32(src), uint8(k), [16]byte(a)))
}

func maskCvtphPs(src [8]float32, k uint8, a [16]byte) [8]float32


// MaskzCvtphPs: Convert packed half-precision (16-bit) floating-point elements
// in 'a' to packed single-precision (32-bit) floating-point elements, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			m := j*16
//			IF k[j]
//				dst[i+31:i] := Convert_FP16_To_FP32(a[m+15:m])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPH2PS'. Intrinsic: '_mm256_maskz_cvtph_ps'.
// Requires AVX512F.
func MaskzCvtphPs(k Mmask8, a M128i) M256 {
	return M256(maskzCvtphPs(uint8(k), [16]byte(a)))
}

func maskzCvtphPs(k uint8, a [16]byte) [8]float32


// CvtpsEpi32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_cvtps_epi32'.
// Requires AVX.
func CvtpsEpi32(a M256) M256i {
	return M256i(cvtpsEpi32([8]float32(a)))
}

func cvtpsEpi32(a [8]float32) [32]byte


// MaskCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_mask_cvtps_epi32'.
// Requires AVX512F.
func MaskCvtpsEpi32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvtpsEpi32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtpsEpi32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvtpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2DQ'. Intrinsic: '_mm256_maskz_cvtps_epi32'.
// Requires AVX512F.
func MaskzCvtpsEpi32(k Mmask8, a M256) M256i {
	return M256i(maskzCvtpsEpi32(uint8(k), [8]float32(a)))
}

func maskzCvtpsEpi32(k uint8, a [8]float32) [32]byte


// CvtpsEpi64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm256_cvtps_epi64'.
// Requires AVX512DQ.
func CvtpsEpi64(a M128) M256i {
	return M256i(cvtpsEpi64([4]float32(a)))
}

func cvtpsEpi64(a [4]float32) [32]byte


// MaskCvtpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm256_mask_cvtps_epi64'.
// Requires AVX512DQ.
func MaskCvtpsEpi64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvtpsEpi64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpi64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvtpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2QQ'. Intrinsic: '_mm256_maskz_cvtps_epi64'.
// Requires AVX512DQ.
func MaskzCvtpsEpi64(k Mmask8, a M128) M256i {
	return M256i(maskzCvtpsEpi64(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpi64(k uint8, a [4]float32) [32]byte


// CvtpsEpu32: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 32-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_cvtps_epu32'.
// Requires AVX512F.
func CvtpsEpu32(a M256) M256i {
	return M256i(cvtpsEpu32([8]float32(a)))
}

func cvtpsEpu32(a [8]float32) [32]byte


// MaskCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_mask_cvtps_epu32'.
// Requires AVX512F.
func MaskCvtpsEpu32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvtpsEpu32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvtpsEpu32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvtpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_UnsignedInt32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UDQ'. Intrinsic: '_mm256_maskz_cvtps_epu32'.
// Requires AVX512F.
func MaskzCvtpsEpu32(k Mmask8, a M256) M256i {
	return M256i(maskzCvtpsEpu32(uint8(k), [8]float32(a)))
}

func maskzCvtpsEpu32(k uint8, a [8]float32) [32]byte


// CvtpsEpu64: Convert packed single-precision (32-bit) floating-point elements
// in 'a' to packed unsigned 64-bit integers, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm256_cvtps_epu64'.
// Requires AVX512DQ.
func CvtpsEpu64(a M128) M256i {
	return M256i(cvtpsEpu64([4]float32(a)))
}

func cvtpsEpu64(a [4]float32) [32]byte


// MaskCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm256_mask_cvtps_epu64'.
// Requires AVX512DQ.
func MaskCvtpsEpu64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvtpsEpu64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvtpsEpu64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvtpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2UQQ'. Intrinsic: '_mm256_maskz_cvtps_epu64'.
// Requires AVX512DQ.
func MaskzCvtpsEpu64(k Mmask8, a M128) M256i {
	return M256i(maskzCvtpsEpu64(uint8(k), [4]float32(a)))
}

func maskzCvtpsEpu64(k uint8, a [4]float32) [32]byte


// CvtpsPd: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed double-precision (64-bit) floating-point elements, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[i+63:i] := Convert_FP32_To_FP64(a[k+31:k])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTPS2PD'. Intrinsic: '_mm256_cvtps_pd'.
// Requires AVX.
func CvtpsPd(a M128) M256d {
	return M256d(cvtpsPd([4]float32(a)))
}

func cvtpsPd(a [4]float32) [4]float64


// CvtpsPh: Convert packed single-precision (32-bit) floating-point elements in
// 'a' to packed half-precision (16-bit) floating-point elements, and store the
// results in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_cvtps_ph'.
// Requires FP16C.
func CvtpsPh(a M256, rounding int) M128i {
	return M128i(cvtpsPh([8]float32(a), rounding))
}

func cvtpsPh(a [8]float32, rounding int) [16]byte


// MaskCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_mask_cvtps_ph'.
// Requires AVX512F.
func MaskCvtpsPh(src M128i, k Mmask8, a M256, rounding int) M128i {
	return M128i(maskCvtpsPh([16]byte(src), uint8(k), [8]float32(a), rounding))
}

func maskCvtpsPh(src [16]byte, k uint8, a [8]float32, rounding int) [16]byte


// MaskzCvtpsPh: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed half-precision (16-bit) floating-point elements,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set).
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := 16*j
//			l := 32*j
//			IF k[j]
//				dst[i+15:i] := Convert_FP32_To_FP16FP(a[l+31:l])
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTPS2PH'. Intrinsic: '_mm256_maskz_cvtps_ph'.
// Requires AVX512F.
func MaskzCvtpsPh(k Mmask8, a M256, rounding int) M128i {
	return M128i(maskzCvtpsPh(uint8(k), [8]float32(a), rounding))
}

func maskzCvtpsPh(k uint8, a [8]float32, rounding int) [16]byte


// Cvtsepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_cvtsepi16_epi8'.
// Requires AVX512BW.
func Cvtsepi16Epi8(a M256i) M128i {
	return M128i(cvtsepi16Epi8([32]byte(a)))
}

func cvtsepi16Epi8(a [32]byte) [16]byte


// MaskCvtsepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_mask_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskCvtsepi16Epi8(src M128i, k Mmask16, a M256i) M128i {
	return M128i(maskCvtsepi16Epi8([16]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtsepi16Epi8(src [16]byte, k uint16, a [32]byte) [16]byte


// MaskzCvtsepi16Epi8: Convert packed 16-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_maskz_cvtsepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtsepi16Epi8(k Mmask16, a M256i) M128i {
	return M128i(maskzCvtsepi16Epi8(uint16(k), [32]byte(a)))
}

func maskzCvtsepi16Epi8(k uint16, a [32]byte) [16]byte


// MaskCvtsepi16StoreuEpi8: Convert packed 16-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSWB'. Intrinsic: '_mm256_mask_cvtsepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtsepi16StoreuEpi8(base_addr uintptr, k Mmask16, a M256i)  {
	maskCvtsepi16StoreuEpi8(uintptr(base_addr), uint16(k), [32]byte(a))
}

func maskCvtsepi16StoreuEpi8(base_addr uintptr, k uint16, a [32]byte) 


// Cvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_cvtsepi32_epi16'.
// Requires AVX512F.
func Cvtsepi32Epi16(a M256i) M128i {
	return M128i(cvtsepi32Epi16([32]byte(a)))
}

func cvtsepi32Epi16(a [32]byte) [16]byte


// MaskCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_mask_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskCvtsepi32Epi16(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi32Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi32Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi32Epi16: Convert packed 32-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_maskz_cvtsepi32_epi16'.
// Requires AVX512F.
func MaskzCvtsepi32Epi16(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi32Epi16(uint8(k), [32]byte(a)))
}

func maskzCvtsepi32Epi16(k uint8, a [32]byte) [16]byte


// Cvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_cvtsepi32_epi8'.
// Requires AVX512F.
func Cvtsepi32Epi8(a M256i) M128i {
	return M128i(cvtsepi32Epi8([32]byte(a)))
}

func cvtsepi32Epi8(a [32]byte) [16]byte


// MaskCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_mask_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskCvtsepi32Epi8(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi32Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi32Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi32Epi8: Convert packed 32-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_maskz_cvtsepi32_epi8'.
// Requires AVX512F.
func MaskzCvtsepi32Epi8(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi32Epi8(uint8(k), [32]byte(a)))
}

func maskzCvtsepi32Epi8(k uint8, a [32]byte) [16]byte


// MaskCvtsepi32StoreuEpi16: Convert packed 32-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSDW'. Intrinsic: '_mm256_mask_cvtsepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi32StoreuEpi16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi32StoreuEpi16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi32StoreuEpi8: Convert packed 32-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSDB'. Intrinsic: '_mm256_mask_cvtsepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi32StoreuEpi8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi32StoreuEpi8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi32StoreuEpi8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_Int64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_cvtsepi64_epi16'.
// Requires AVX512F.
func Cvtsepi64Epi16(a M256i) M128i {
	return M128i(cvtsepi64Epi16([32]byte(a)))
}

func cvtsepi64Epi16(a [32]byte) [16]byte


// MaskCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_mask_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskCvtsepi64Epi16(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi64Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi64Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi64Epi16: Convert packed 64-bit integers in 'a' to packed 16-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_Int64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_maskz_cvtsepi64_epi16'.
// Requires AVX512F.
func MaskzCvtsepi64Epi16(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi64Epi16(uint8(k), [32]byte(a)))
}

func maskzCvtsepi64Epi16(k uint8, a [32]byte) [16]byte


// Cvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_Int64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_cvtsepi64_epi32'.
// Requires AVX512F.
func Cvtsepi64Epi32(a M256i) M128i {
	return M128i(cvtsepi64Epi32([32]byte(a)))
}

func cvtsepi64Epi32(a [32]byte) [16]byte


// MaskCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_mask_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskCvtsepi64Epi32(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi64Epi32([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi64Epi32(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi64Epi32: Convert packed 64-bit integers in 'a' to packed 32-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_Int64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_maskz_cvtsepi64_epi32'.
// Requires AVX512F.
func MaskzCvtsepi64Epi32(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi64Epi32(uint8(k), [32]byte(a)))
}

func maskzCvtsepi64Epi32(k uint8, a [32]byte) [16]byte


// Cvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_Int64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_cvtsepi64_epi8'.
// Requires AVX512F.
func Cvtsepi64Epi8(a M256i) M128i {
	return M128i(cvtsepi64Epi8([32]byte(a)))
}

func cvtsepi64Epi8(a [32]byte) [16]byte


// MaskCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_mask_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskCvtsepi64Epi8(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtsepi64Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtsepi64Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtsepi64Epi8: Convert packed 64-bit integers in 'a' to packed 8-bit
// integers with signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_Int64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_maskz_cvtsepi64_epi8'.
// Requires AVX512F.
func MaskzCvtsepi64Epi8(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtsepi64Epi8(uint8(k), [32]byte(a)))
}

func maskzCvtsepi64Epi8(k uint8, a [32]byte) [16]byte


// MaskCvtsepi64StoreuEpi16: Convert packed 64-bit integers in 'a' to packed
// 16-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_Int64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVSQW'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64StoreuEpi16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64StoreuEpi16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi64StoreuEpi32: Convert packed 64-bit integers in 'a' to packed
// 32-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_Int64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVSQD'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64StoreuEpi32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64StoreuEpi32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtsepi64StoreuEpi8: Convert packed 64-bit integers in 'a' to packed
// 8-bit integers with signed saturation, and store the active results (those
// with their respective bit set in writemask 'k') to unaligned memory at
// 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_Int64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVSQB'. Intrinsic: '_mm256_mask_cvtsepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtsepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtsepi64StoreuEpi8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtsepi64StoreuEpi8(base_addr uintptr, k uint8, a [32]byte) 


// CvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_cvttpd_epi32'.
// Requires AVX.
func CvttpdEpi32(a M256d) M128i {
	return M128i(cvttpdEpi32([4]float64(a)))
}

func cvttpdEpi32(a [4]float64) [16]byte


// MaskCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_mask_cvttpd_epi32'.
// Requires AVX512F.
func MaskCvttpdEpi32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvttpdEpi32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpi32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvttpdEpi32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_Int32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2DQ'. Intrinsic: '_mm256_maskz_cvttpd_epi32'.
// Requires AVX512F.
func MaskzCvttpdEpi32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvttpdEpi32(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpi32(k uint8, a [4]float64) [16]byte


// CvttpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm256_cvttpd_epi64'.
// Requires AVX512DQ.
func CvttpdEpi64(a M256d) M256i {
	return M256i(cvttpdEpi64([4]float64(a)))
}

func cvttpdEpi64(a [4]float64) [32]byte


// MaskCvttpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm256_mask_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskCvttpdEpi64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvttpdEpi64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpi64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvttpdEpi64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_Int64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2QQ'. Intrinsic: '_mm256_maskz_cvttpd_epi64'.
// Requires AVX512DQ.
func MaskzCvttpdEpi64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvttpdEpi64(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpi64(k uint8, a [4]float64) [32]byte


// CvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 32*j
//			k := 64*j
//			dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[k+63:k])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_cvttpd_epu32'.
// Requires AVX512F.
func CvttpdEpu32(a M256d) M128i {
	return M128i(cvttpdEpu32([4]float64(a)))
}

func cvttpdEpu32(a [4]float64) [16]byte


// MaskCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_mask_cvttpd_epu32'.
// Requires AVX512F.
func MaskCvttpdEpu32(src M128i, k Mmask8, a M256d) M128i {
	return M128i(maskCvttpdEpu32([16]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpu32(src [16]byte, k uint8, a [4]float64) [16]byte


// MaskzCvttpdEpu32: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 32*j
//			l := 64*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[l+63:l])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VCVTTPD2UDQ'. Intrinsic: '_mm256_maskz_cvttpd_epu32'.
// Requires AVX512F.
func MaskzCvttpdEpu32(k Mmask8, a M256d) M128i {
	return M128i(maskzCvttpdEpu32(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpu32(k uint8, a [4]float64) [16]byte


// CvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm256_cvttpd_epu64'.
// Requires AVX512DQ.
func CvttpdEpu64(a M256d) M256i {
	return M256i(cvttpdEpu64([4]float64(a)))
}

func cvttpdEpu64(a [4]float64) [32]byte


// MaskCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm256_mask_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskCvttpdEpu64(src M256i, k Mmask8, a M256d) M256i {
	return M256i(maskCvttpdEpu64([32]byte(src), uint8(k), [4]float64(a)))
}

func maskCvttpdEpu64(src [32]byte, k uint8, a [4]float64) [32]byte


// MaskzCvttpdEpu64: Convert packed double-precision (64-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := Convert_FP64_To_UnsignedInt64_Truncate(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPD2UQQ'. Intrinsic: '_mm256_maskz_cvttpd_epu64'.
// Requires AVX512DQ.
func MaskzCvttpdEpu64(k Mmask8, a M256d) M256i {
	return M256i(maskzCvttpdEpu64(uint8(k), [4]float64(a)))
}

func maskzCvttpdEpu64(k uint8, a [4]float64) [32]byte


// CvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_cvttps_epi32'.
// Requires AVX.
func CvttpsEpi32(a M256) M256i {
	return M256i(cvttpsEpi32([8]float32(a)))
}

func cvttpsEpi32(a [8]float32) [32]byte


// MaskCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_Int32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_mask_cvttps_epi32'.
// Requires AVX512F.
func MaskCvttpsEpi32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvttpsEpi32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttpsEpi32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvttpsEpi32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 32-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*i
//			IF k[j]
//				dst[i+31:i] := Convert_FP32_To_IntegerTruncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2DQ'. Intrinsic: '_mm256_maskz_cvttps_epi32'.
// Requires AVX512F.
func MaskzCvttpsEpi32(k Mmask8, a M256) M256i {
	return M256i(maskzCvttpsEpi32(uint8(k), [8]float32(a)))
}

func maskzCvttpsEpi32(k uint8, a [8]float32) [32]byte


// CvttpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm256_cvttps_epi64'.
// Requires AVX512DQ.
func CvttpsEpi64(a M128) M256i {
	return M256i(cvttpsEpi64([4]float32(a)))
}

func cvttpsEpi64(a [4]float32) [32]byte


// MaskCvttpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm256_mask_cvttps_epi64'.
// Requires AVX512DQ.
func MaskCvttpsEpi64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvttpsEpi64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpi64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvttpsEpi64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed 64-bit integers with truncation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_Int64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2QQ'. Intrinsic: '_mm256_maskz_cvttps_epi64'.
// Requires AVX512DQ.
func MaskzCvttpsEpi64(k Mmask8, a M128) M256i {
	return M256i(maskzCvttpsEpi64(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpi64(k uint8, a [4]float32) [32]byte


// CvttpsEpu32: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := Convert_FP32_To_UnsignedInt32_Truncate(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_cvttps_epu32'.
// Requires AVX512F.
func CvttpsEpu32(a M256) M256i {
	return M256i(cvttpsEpu32([8]float32(a)))
}

func cvttpsEpu32(a [8]float32) [32]byte


// MaskCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_mask_cvttps_epu32'.
// Requires AVX512F.
func MaskCvttpsEpu32(src M256i, k Mmask8, a M256) M256i {
	return M256i(maskCvttpsEpu32([32]byte(src), uint8(k), [8]float32(a)))
}

func maskCvttpsEpu32(src [32]byte, k uint8, a [8]float32) [32]byte


// MaskzCvttpsEpu32: Convert packed double-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 32-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := Convert_FP64_To_UnsignedInt32_Truncate(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UDQ'. Intrinsic: '_mm256_maskz_cvttps_epu32'.
// Requires AVX512F.
func MaskzCvttpsEpu32(k Mmask8, a M256) M256i {
	return M256i(maskzCvttpsEpu32(uint8(k), [8]float32(a)))
}

func maskzCvttpsEpu32(k uint8, a [8]float32) [32]byte


// CvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm256_cvttps_epu64'.
// Requires AVX512DQ.
func CvttpsEpu64(a M128) M256i {
	return M256i(cvttpsEpu64([4]float32(a)))
}

func cvttpsEpu64(a [4]float32) [32]byte


// MaskCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm256_mask_cvttps_epu64'.
// Requires AVX512DQ.
func MaskCvttpsEpu64(src M256i, k Mmask8, a M128) M256i {
	return M256i(maskCvttpsEpu64([32]byte(src), uint8(k), [4]float32(a)))
}

func maskCvttpsEpu64(src [32]byte, k uint8, a [4]float32) [32]byte


// MaskzCvttpsEpu64: Convert packed single-precision (32-bit) floating-point
// elements in 'a' to packed unsigned 64-bit integers with truncation, and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				dst[i+63:i] := Convert_FP32_To_UnsignedInt64_Truncate(a[l+31:l])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VCVTTPS2UQQ'. Intrinsic: '_mm256_maskz_cvttps_epu64'.
// Requires AVX512DQ.
func MaskzCvttpsEpu64(k Mmask8, a M128) M256i {
	return M256i(maskzCvttpsEpu64(uint8(k), [4]float32(a)))
}

func maskzCvttpsEpu64(k uint8, a [4]float32) [32]byte


// Cvtusepi16Epi8: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_cvtusepi16_epi8'.
// Requires AVX512BW.
func Cvtusepi16Epi8(a M256i) M128i {
	return M128i(cvtusepi16Epi8([32]byte(a)))
}

func cvtusepi16Epi8(a [32]byte) [16]byte


// MaskCvtusepi16Epi8: Convert packed unsigned 16-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_mask_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskCvtusepi16Epi8(src M128i, k Mmask16, a M256i) M128i {
	return M128i(maskCvtusepi16Epi8([16]byte(src), uint16(k), [32]byte(a)))
}

func maskCvtusepi16Epi8(src [16]byte, k uint16, a [32]byte) [16]byte


// MaskzCvtusepi16Epi8: Convert packed unsigned 16-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_maskz_cvtusepi16_epi8'.
// Requires AVX512BW.
func MaskzCvtusepi16Epi8(k Mmask16, a M256i) M128i {
	return M128i(maskzCvtusepi16Epi8(uint16(k), [32]byte(a)))
}

func maskzCvtusepi16Epi8(k uint16, a [32]byte) [16]byte


// MaskCvtusepi16StoreuEpi8: Convert packed unsigned 16-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt16_To_Int8(a[i+15:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSWB'. Intrinsic: '_mm256_mask_cvtusepi16_storeu_epi8'.
// Requires AVX512BW.
func MaskCvtusepi16StoreuEpi8(base_addr uintptr, k Mmask16, a M256i)  {
	maskCvtusepi16StoreuEpi8(uintptr(base_addr), uint16(k), [32]byte(a))
}

func maskCvtusepi16StoreuEpi8(base_addr uintptr, k uint16, a [32]byte) 


// Cvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_cvtusepi32_epi16'.
// Requires AVX512F.
func Cvtusepi32Epi16(a M256i) M128i {
	return M128i(cvtusepi32Epi16([32]byte(a)))
}

func cvtusepi32Epi16(a [32]byte) [16]byte


// MaskCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_mask_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskCvtusepi32Epi16(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi32Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi32Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi32Epi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_maskz_cvtusepi32_epi16'.
// Requires AVX512F.
func MaskzCvtusepi32Epi16(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi32Epi16(uint8(k), [32]byte(a)))
}

func maskzCvtusepi32Epi16(k uint8, a [32]byte) [16]byte


// Cvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_cvtusepi32_epi8'.
// Requires AVX512F.
func Cvtusepi32Epi8(a M256i) M128i {
	return M128i(cvtusepi32Epi8([32]byte(a)))
}

func cvtusepi32Epi8(a [32]byte) [16]byte


// MaskCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_mask_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskCvtusepi32Epi8(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi32Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi32Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi32Epi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_maskz_cvtusepi32_epi8'.
// Requires AVX512F.
func MaskzCvtusepi32Epi8(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi32Epi8(uint8(k), [32]byte(a)))
}

func maskzCvtusepi32Epi8(k uint8, a [32]byte) [16]byte


// MaskCvtusepi32StoreuEpi16: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt32_To_Int16(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSDW'. Intrinsic: '_mm256_mask_cvtusepi32_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi32StoreuEpi16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi32StoreuEpi16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi32StoreuEpi8: Convert packed unsigned 32-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt32_To_Int8(a[i+31:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSDB'. Intrinsic: '_mm256_mask_cvtusepi32_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi32StoreuEpi8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi32StoreuEpi8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi32StoreuEpi8(base_addr uintptr, k uint8, a [32]byte) 


// Cvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 16-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 16*j
//			dst[k+15:k] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_cvtusepi64_epi16'.
// Requires AVX512F.
func Cvtusepi64Epi16(a M256i) M128i {
	return M128i(cvtusepi64Epi16([32]byte(a)))
}

func cvtusepi64Epi16(a [32]byte) [16]byte


// MaskCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := src[l+15:l]
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_mask_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskCvtusepi64Epi16(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi64Epi16([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi64Epi16(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi64Epi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				dst[l+15:l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			ELSE
//				dst[l+15:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_maskz_cvtusepi64_epi16'.
// Requires AVX512F.
func MaskzCvtusepi64Epi16(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi64Epi16(uint8(k), [32]byte(a)))
}

func maskzCvtusepi64Epi16(k uint8, a [32]byte) [16]byte


// Cvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 32-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 32*j
//			dst[k+31:k] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_cvtusepi64_epi32'.
// Requires AVX512F.
func Cvtusepi64Epi32(a M256i) M128i {
	return M128i(cvtusepi64Epi32([32]byte(a)))
}

func cvtusepi64Epi32(a [32]byte) [16]byte


// MaskCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := src[l+31:l]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_mask_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskCvtusepi64Epi32(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi64Epi32([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi64Epi32(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi64Epi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				dst[l+31:l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			ELSE
//				dst[l+31:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_maskz_cvtusepi64_epi32'.
// Requires AVX512F.
func MaskzCvtusepi64Epi32(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi64Epi32(uint8(k), [32]byte(a)))
}

func maskzCvtusepi64Epi32(k uint8, a [32]byte) [16]byte


// Cvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			k := 8*j
//			dst[k+7:k] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_cvtusepi64_epi8'.
// Requires AVX512F.
func Cvtusepi64Epi8(a M256i) M128i {
	return M128i(cvtusepi64Epi8([32]byte(a)))
}

func cvtusepi64Epi8(a [32]byte) [16]byte


// MaskCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to packed
// unsigned 8-bit integers with unsigned saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := src[l+7:l]
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_mask_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskCvtusepi64Epi8(src M128i, k Mmask8, a M256i) M128i {
	return M128i(maskCvtusepi64Epi8([16]byte(src), uint8(k), [32]byte(a)))
}

func maskCvtusepi64Epi8(src [16]byte, k uint8, a [32]byte) [16]byte


// MaskzCvtusepi64Epi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				dst[l+7:l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			ELSE
//				dst[l+7:l] := 0
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_maskz_cvtusepi64_epi8'.
// Requires AVX512F.
func MaskzCvtusepi64Epi8(k Mmask8, a M256i) M128i {
	return M128i(maskzCvtusepi64Epi8(uint8(k), [32]byte(a)))
}

func maskzCvtusepi64Epi8(k uint8, a [32]byte) [16]byte


// MaskCvtusepi64StoreuEpi16: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 16-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 16*j
//			IF k[j]
//				MEM[base_addr+l+15:base_addr+l] := Saturate_UnsignedInt64_To_Int16(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:64] := 0
//
// Instruction: 'VPMOVUSQW'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi16'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi16(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64StoreuEpi16(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64StoreuEpi16(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi64StoreuEpi32: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 32-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 32*j
//			IF k[j]
//				MEM[base_addr+l+31:base_addr+l] := Saturate_UnsignedInt64_To_Int32(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPMOVUSQD'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi32'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi32(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64StoreuEpi32(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64StoreuEpi32(base_addr uintptr, k uint8, a [32]byte) 


// MaskCvtusepi64StoreuEpi8: Convert packed unsigned 64-bit integers in 'a' to
// packed unsigned 8-bit integers with unsigned saturation, and store the
// active results (those with their respective bit set in writemask 'k') to
// unaligned memory at 'base_addr'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			l := 8*j
//			IF k[j]
//				MEM[base_addr+l+7:base_addr+l] := Saturate_UnsignedInt64_To_Int8(a[i+63:i])
//			FI
//		ENDFOR
//		dst[MAX:32] := 0
//
// Instruction: 'VPMOVUSQB'. Intrinsic: '_mm256_mask_cvtusepi64_storeu_epi8'.
// Requires AVX512F.
func MaskCvtusepi64StoreuEpi8(base_addr uintptr, k Mmask8, a M256i)  {
	maskCvtusepi64StoreuEpi8(uintptr(base_addr), uint8(k), [32]byte(a))
}

func maskCvtusepi64StoreuEpi8(base_addr uintptr, k uint8, a [32]byte) 


// DbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets of
// unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst'.
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm256_dbsad_epu8'.
// Requires AVX512BW.
func DbsadEpu8(a M256i, b M256i, imm8 int) M256i {
	return M256i(dbsadEpu8([32]byte(a), [32]byte(b), imm8))
}

func dbsadEpu8(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskDbsadEpu8: Compute the sum of absolute differences (SADs) of quadruplets
// of unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 3
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm256_mask_dbsad_epu8'.
// Requires AVX512BW.
func MaskDbsadEpu8(src M256i, k Mmask16, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskDbsadEpu8([32]byte(src), uint16(k), [32]byte(a), [32]byte(b), imm8))
}

func maskDbsadEpu8(src [32]byte, k uint16, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzDbsadEpu8: Compute the sum of absolute differences (SADs) of
// quadruplets of unsigned 8-bit integers in 'a' compared to those in 'b', and
// store the 16-bit results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set).
// 	Four SADs are performed on four 8-bit quadruplets for each 64-bit lane. The
// first two SADs use the lower 8-bit quadruplet of the lane from 'a', and the
// last two SADs use the uppper 8-bit quadruplet of the lane from 'a'.
// Quadruplets from 'b' are selected from within 128-bit lanes according to the
// control in 'imm8', and each SAD in each 64-bit lane uses the selected
// quadruplet at 8-bit offsets. 
//
//		FOR j := 0 to 1
//			i := j*128
//			tmp[i+31:i] := select(b[i+127:i], imm8[1:0])
//			tmp[i+63:i+32] := select(b[i+127:i], imm8[3:2])
//			tmp[i+95:i+64] := select(b[i+127:i], imm8[5:4])
//			tmp[i+127:i+96] := select(b[i+127:i], imm8[7:6])
//		ENDFOR
//		
//		FOR j := 0 to 3
//			i := j*64
//			tmp_dst[i+15:i] := ABS(a[i+7:i] - tmp[i+7:i]) + ABS(a[i+15:i+8] - tmp[i+15:i+8])
//						 + ABS(a[i+23:i+16] - tmp[i+23:i+16]) + ABS(a[i+31:i+24] - tmp[i+31:i+24])
//			
//			tmp_dst[i+31:i+16] := ABS(a[i+7:i] - tmp[i+15:i+8]) + ABS(a[i+15:i+8] - tmp[i+23:i+16])
//						 + ABS(a[i+23:i+16] - tmp[i+31:i+24]) + ABS(a[i+31:i+24] - tmp[i+39:i+32])
//			
//			tmp_dst[i+47:i+32] := ABS(a[i+39:i+32] - tmp[i+23:i+16]) + ABS(a[i+47:i+40] - tmp[i+31:i+24])
//						 + ABS(a[i+55:i+48] - tmp[i+39:i+32]) + ABS(a[i+63:i+56] - tmp[i+47:i+40])
//			
//			tmp_dst[i+63:i+48] := ABS(a[i+39:i+32] - tmp[i+31:i+24]) + ABS(a[i+47:i+40] - tmp[i+39:i+32])
//						 + ABS(a[i+55:i+48] - tmp[i+47:i+40]) + ABS(a[i+63:i+56] - tmp[i+55:i+48])
//		ENDFOR
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDBPSADBW'. Intrinsic: '_mm256_maskz_dbsad_epu8'.
// Requires AVX512BW.
func MaskzDbsadEpu8(k Mmask16, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzDbsadEpu8(uint16(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzDbsadEpu8(k uint16, a [32]byte, b [32]byte, imm8 int) [32]byte


// DivEpi16: Divide packed 16-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi16'.
// Requires AVX.
func DivEpi16(a M256i, b M256i) M256i {
	return M256i(divEpi16([32]byte(a), [32]byte(b)))
}

func divEpi16(a [32]byte, b [32]byte) [32]byte


// DivEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi32'.
// Requires AVX.
func DivEpi32(a M256i, b M256i) M256i {
	return M256i(divEpi32([32]byte(a), [32]byte(b)))
}

func divEpi32(a [32]byte, b [32]byte) [32]byte


// DivEpi64: Divide packed 64-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi64'.
// Requires AVX.
func DivEpi64(a M256i, b M256i) M256i {
	return M256i(divEpi64([32]byte(a), [32]byte(b)))
}

func divEpi64(a [32]byte, b [32]byte) [32]byte


// DivEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epi8'.
// Requires AVX.
func DivEpi8(a M256i, b M256i) M256i {
	return M256i(divEpi8([32]byte(a), [32]byte(b)))
}

func divEpi8(a [32]byte, b [32]byte) [32]byte


// DivEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := TRUNCATE(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu16'.
// Requires AVX.
func DivEpu16(a M256i, b M256i) M256i {
	return M256i(divEpu16([32]byte(a), [32]byte(b)))
}

func divEpu16(a [32]byte, b [32]byte) [32]byte


// DivEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu32'.
// Requires AVX.
func DivEpu32(a M256i, b M256i) M256i {
	return M256i(divEpu32([32]byte(a), [32]byte(b)))
}

func divEpu32(a [32]byte, b [32]byte) [32]byte


// DivEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := TRUNCATE(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu64'.
// Requires AVX.
func DivEpu64(a M256i, b M256i) M256i {
	return M256i(divEpu64([32]byte(a), [32]byte(b)))
}

func divEpu64(a [32]byte, b [32]byte) [32]byte


// DivEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := TRUNCATE(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_div_epu8'.
// Requires AVX.
func DivEpu8(a M256i, b M256i) M256i {
	return M256i(divEpu8([32]byte(a), [32]byte(b)))
}

func divEpu8(a [32]byte, b [32]byte) [32]byte


// DivPd: Divide packed double-precision (64-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := a[i+63:i] / b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_div_pd'.
// Requires AVX.
func DivPd(a M256d, b M256d) M256d {
	return M256d(divPd([4]float64(a), [4]float64(b)))
}

func divPd(a [4]float64, b [4]float64) [4]float64


// MaskDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_mask_div_pd'.
// Requires AVX512F.
func MaskDivPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskDivPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskDivPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzDivPd: Divide packed double-precision (64-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := 64*j
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] / b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPD'. Intrinsic: '_mm256_maskz_div_pd'.
// Requires AVX512F.
func MaskzDivPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzDivPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzDivPd(k uint8, a [4]float64, b [4]float64) [4]float64


// DivPs: Divide packed single-precision (32-bit) floating-point elements in
// 'a' by packed elements in 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := a[i+31:i] / b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_div_ps'.
// Requires AVX.
func DivPs(a M256, b M256) M256 {
	return M256(divPs([8]float32(a), [8]float32(b)))
}

func divPs(a [8]float32, b [8]float32) [8]float32


// MaskDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_mask_div_ps'.
// Requires AVX512F.
func MaskDivPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskDivPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskDivPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzDivPs: Divide packed single-precision (32-bit) floating-point elements
// in 'a' by packed elements in 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := 32*j
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] / b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VDIVPS'. Intrinsic: '_mm256_maskz_div_ps'.
// Requires AVX512F.
func MaskzDivPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzDivPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzDivPs(k uint8, a [8]float32, b [8]float32) [8]float32


// DpPs: Conditionally multiply the packed single-precision (32-bit)
// floating-point elements in 'a' and 'b' using the high 4 bits in 'imm8', sum
// the four products, and conditionally store the sum in 'dst' using the low 4
// bits of 'imm8'. 
//
//		DP(a[127:0], b[127:0], imm8[7:0]) {
//			FOR j := 0 to 3
//				i := j*32
//				IF imm8[(4+j)%8]
//					temp[i+31:i] := a[i+31:i] * b[i+31:i]
//				ELSE
//					temp[i+31:i] := 0
//				FI
//			ENDFOR
//			
//			sum[31:0] := (temp[127:96] + temp[95:64]) + (temp[63:32] + temp[31:0])
//			
//			FOR j := 0 to 3
//				i := j*32
//				IF imm8[j%8]
//					tmpdst[i+31:i] := sum[31:0]
//				ELSE
//					tmpdst[i+31:i] := 0
//				FI
//			ENDFOR
//			RETURN tmpdst[127:0]
//		}
//		
//		dst[127:0] := DP(a[127:0], b[127:0], imm8[7:0])
//		dst[255:128] := DP(a[255:128], b[255:128], imm8[7:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VDPPS'. Intrinsic: '_mm256_dp_ps'.
// Requires AVX.
func DpPs(a M256, b M256, imm8 int) M256 {
	return M256(dpPs([8]float32(a), [8]float32(b), imm8))
}

func dpPs(a [8]float32, b [8]float32, imm8 int) [8]float32


// ErfPd: Compute the error function of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erf_pd'.
// Requires AVX.
func ErfPd(a M256d) M256d {
	return M256d(erfPd([4]float64(a)))
}

func erfPd(a [4]float64) [4]float64


// ErfPs: Compute the error function of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erf_ps'.
// Requires AVX.
func ErfPs(a M256) M256 {
	return M256(erfPs([8]float32(a)))
}

func erfPs(a [8]float32) [8]float32


// ErfcPd: Compute the complementary error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 1.0 - ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfc_pd'.
// Requires AVX.
func ErfcPd(a M256d) M256d {
	return M256d(erfcPd([4]float64(a)))
}

func erfcPd(a [4]float64) [4]float64


// ErfcPs: Compute the complementary error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 1.0 - ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfc_ps'.
// Requires AVX.
func ErfcPs(a M256) M256 {
	return M256(erfcPs([8]float32(a)))
}

func erfcPs(a [8]float32) [8]float32


// ErfcinvPd: Compute the inverse complementary error function of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 1.0 / (1.0 - ERF(a[i+63:i]))
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfcinv_pd'.
// Requires AVX.
func ErfcinvPd(a M256d) M256d {
	return M256d(erfcinvPd([4]float64(a)))
}

func erfcinvPd(a [4]float64) [4]float64


// ErfcinvPs: Compute the inverse complementary error function of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 1.0 / (1.0 - ERF(a[i+31:i]))
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfcinv_ps'.
// Requires AVX.
func ErfcinvPs(a M256) M256 {
	return M256(erfcinvPs([8]float32(a)))
}

func erfcinvPs(a [8]float32) [8]float32


// ErfinvPd: Compute the inverse error function of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 1.0 / ERF(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfinv_pd'.
// Requires AVX.
func ErfinvPd(a M256d) M256d {
	return M256d(erfinvPd([4]float64(a)))
}

func erfinvPd(a [4]float64) [4]float64


// ErfinvPs: Compute the inverse error function of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 1.0 / ERF(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_erfinv_ps'.
// Requires AVX.
func ErfinvPs(a M256) M256 {
	return M256(erfinvPs([8]float32(a)))
}

func erfinvPs(a [8]float32) [8]float32


// ExpPd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp_pd'.
// Requires AVX.
func ExpPd(a M256d) M256d {
	return M256d(expPd([4]float64(a)))
}

func expPd(a [4]float64) [4]float64


// ExpPs: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp_ps'.
// Requires AVX.
func ExpPs(a M256) M256 {
	return M256(expPs([8]float32(a)))
}

func expPs(a [8]float32) [8]float32


// Exp10Pd: Compute the exponential value of 10 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 10^(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp10_pd'.
// Requires AVX.
func Exp10Pd(a M256d) M256d {
	return M256d(exp10Pd([4]float64(a)))
}

func exp10Pd(a [4]float64) [4]float64


// Exp10Ps: Compute the exponential value of 10 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 10^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp10_ps'.
// Requires AVX.
func Exp10Ps(a M256) M256 {
	return M256(exp10Ps([8]float32(a)))
}

func exp10Ps(a [8]float32) [8]float32


// Exp2Pd: Compute the exponential value of 2 raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := 2^(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp2_pd'.
// Requires AVX.
func Exp2Pd(a M256d) M256d {
	return M256d(exp2Pd([4]float64(a)))
}

func exp2Pd(a [4]float64) [4]float64


// Exp2Ps: Compute the exponential value of 2 raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := 2^(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_exp2_ps'.
// Requires AVX.
func Exp2Ps(a M256) M256 {
	return M256(exp2Ps([8]float32(a)))
}

func exp2Ps(a [8]float32) [8]float32


// MaskExpandEpi32: Load contiguous active 32-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_mask_expand_epi32'.
// Requires AVX512F.
func MaskExpandEpi32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskExpandEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskExpandEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzExpandEpi32: Load contiguous active 32-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_maskz_expand_epi32'.
// Requires AVX512F.
func MaskzExpandEpi32(k Mmask8, a M256i) M256i {
	return M256i(maskzExpandEpi32(uint8(k), [32]byte(a)))
}

func maskzExpandEpi32(k uint8, a [32]byte) [32]byte


// MaskExpandEpi64: Load contiguous active 64-bit integers from 'a' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_mask_expand_epi64'.
// Requires AVX512F.
func MaskExpandEpi64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskExpandEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskExpandEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzExpandEpi64: Load contiguous active 64-bit integers from 'a' (those
// with their respective bit set in mask 'k'), and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_maskz_expand_epi64'.
// Requires AVX512F.
func MaskzExpandEpi64(k Mmask8, a M256i) M256i {
	return M256i(maskzExpandEpi64(uint8(k), [32]byte(a)))
}

func maskzExpandEpi64(k uint8, a [32]byte) [32]byte


// MaskExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_mask_expand_pd'.
// Requires AVX512F.
func MaskExpandPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskExpandPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskExpandPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzExpandPd: Load contiguous active double-precision (64-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[m+63:m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_maskz_expand_pd'.
// Requires AVX512F.
func MaskzExpandPd(k Mmask8, a M256d) M256d {
	return M256d(maskzExpandPd(uint8(k), [4]float64(a)))
}

func maskzExpandPd(k uint8, a [4]float64) [4]float64


// MaskExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_mask_expand_ps'.
// Requires AVX512F.
func MaskExpandPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskExpandPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskExpandPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzExpandPs: Load contiguous active single-precision (32-bit)
// floating-point elements from 'a' (those with their respective bit set in
// mask 'k'), and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[m+31:m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_maskz_expand_ps'.
// Requires AVX512F.
func MaskzExpandPs(k Mmask8, a M256) M256 {
	return M256(maskzExpandPs(uint8(k), [8]float32(a)))
}

func maskzExpandPs(k uint8, a [8]float32) [8]float32


// MaskExpandloaduEpi32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_mask_expandloadu_epi32'.
// Requires AVX512F.
func MaskExpandloaduEpi32(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskExpandloaduEpi32([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi32(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzExpandloaduEpi32: Load contiguous active 32-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDD'. Intrinsic: '_mm256_maskz_expandloadu_epi32'.
// Requires AVX512F.
func MaskzExpandloaduEpi32(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzExpandloaduEpi32(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi32(k uint8, mem_addr uintptr) [32]byte


// MaskExpandloaduEpi64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_mask_expandloadu_epi64'.
// Requires AVX512F.
func MaskExpandloaduEpi64(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskExpandloaduEpi64([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduEpi64(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzExpandloaduEpi64: Load contiguous active 64-bit integers from unaligned
// memory at 'mem_addr' (those with their respective bit set in mask 'k'), and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPEXPANDQ'. Intrinsic: '_mm256_maskz_expandloadu_epi64'.
// Requires AVX512F.
func MaskzExpandloaduEpi64(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzExpandloaduEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduEpi64(k uint8, mem_addr uintptr) [32]byte


// MaskExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_mask_expandloadu_pd'.
// Requires AVX512F.
func MaskExpandloaduPd(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskExpandloaduPd([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPd(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzExpandloaduPd: Load contiguous active double-precision (64-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+m+63:mem_addr+m]
//				m := m + 64
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPD'. Intrinsic: '_mm256_maskz_expandloadu_pd'.
// Requires AVX512F.
func MaskzExpandloaduPd(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzExpandloaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPd(k uint8, mem_addr uintptr) [4]float64


// MaskExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_mask_expandloadu_ps'.
// Requires AVX512F.
func MaskExpandloaduPs(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskExpandloaduPs([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskExpandloaduPs(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzExpandloaduPs: Load contiguous active single-precision (32-bit)
// floating-point elements from unaligned memory at 'mem_addr' (those with
// their respective bit set in mask 'k'), and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		m := 0
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+m+31:mem_addr+m]
//				m := m + 32
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VEXPANDPS'. Intrinsic: '_mm256_maskz_expandloadu_ps'.
// Requires AVX512F.
func MaskzExpandloaduPs(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzExpandloaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzExpandloaduPs(k uint8, mem_addr uintptr) [8]float32


// Expm1Pd: Compute the exponential value of 'e' raised to the power of packed
// double-precision (64-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := e^(a[i+63:i]) - 1.0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_expm1_pd'.
// Requires AVX.
func Expm1Pd(a M256d) M256d {
	return M256d(expm1Pd([4]float64(a)))
}

func expm1Pd(a [4]float64) [4]float64


// Expm1Ps: Compute the exponential value of 'e' raised to the power of packed
// single-precision (32-bit) floating-point elements in 'a', subtract one from
// each element, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := e^(a[i+31:i]) - 1.0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_expm1_ps'.
// Requires AVX.
func Expm1Ps(a M256) M256 {
	return M256(expm1Ps([8]float32(a)))
}

func expm1Ps(a [8]float32) [8]float32


// ExtractEpi16: Extract a 16-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[15:0] := (a[255:0] >> (index * 16))[15:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi16'.
// Requires AVX.
func ExtractEpi16(a M256i, index int) int16 {
	return int16(extractEpi16([32]byte(a), index))
}

func extractEpi16(a [32]byte, index int) int16


// ExtractEpi32: Extract a 32-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[31:0] := (a[255:0] >> (index * 32))[31:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi32'.
// Requires AVX.
func ExtractEpi32(a M256i, index int) int32 {
	return int32(extractEpi32([32]byte(a), index))
}

func extractEpi32(a [32]byte, index int) int32


// ExtractEpi64: Extract a 64-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[63:0] := (a[255:0] >> (index * 64))[63:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi64'.
// Requires AVX.
func ExtractEpi64(a M256i, index int) int64 {
	return int64(extractEpi64([32]byte(a), index))
}

func extractEpi64(a [32]byte, index int) int64


// ExtractEpi8: Extract an 8-bit integer from 'a', selected with 'index', and
// store the result in 'dst'. 
//
//		dst[7:0] := (a[255:0] >> (index * 8))[7:0]
//
// Instruction: '...'. Intrinsic: '_mm256_extract_epi8'.
// Requires AVX.
func ExtractEpi8(a M256i, index int) int8 {
	return int8(extractEpi8([32]byte(a), index))
}

func extractEpi8(a [32]byte, index int) int8


// Extractf128Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF128'. Intrinsic: '_mm256_extractf128_pd'.
// Requires AVX.
func Extractf128Pd(a M256d, imm8 int) M128d {
	return M128d(extractf128Pd([4]float64(a), imm8))
}

func extractf128Pd(a [4]float64, imm8 int) [2]float64


// Extractf128Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF128'. Intrinsic: '_mm256_extractf128_ps'.
// Requires AVX.
func Extractf128Ps(a M256, imm8 int) M128 {
	return M128(extractf128Ps([8]float32(a), imm8))
}

func extractf128Ps(a [8]float32, imm8 int) [4]float32


// Extractf128Si256: Extract 128 bits (composed of integer data) from 'a',
// selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF128'. Intrinsic: '_mm256_extractf128_si256'.
// Requires AVX.
func Extractf128Si256(a M256i, imm8 int) M128i {
	return M128i(extractf128Si256([32]byte(a), imm8))
}

func extractf128Si256(a [32]byte, imm8 int) [16]byte


// Extractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_extractf32x4_ps'.
// Requires AVX512F.
func Extractf32x4Ps(a M256, imm8 int) M128 {
	return M128(extractf32x4Ps([8]float32(a), imm8))
}

func extractf32x4Ps(a [8]float32, imm8 int) [4]float32


// MaskExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_mask_extractf32x4_ps'.
// Requires AVX512F.
func MaskExtractf32x4Ps(src M128, k Mmask8, a M256, imm8 int) M128 {
	return M128(maskExtractf32x4Ps([4]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskExtractf32x4Ps(src [4]float32, k uint8, a [8]float32, imm8 int) [4]float32


// MaskzExtractf32x4Ps: Extract 128 bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF32X4'. Intrinsic: '_mm256_maskz_extractf32x4_ps'.
// Requires AVX512F.
func MaskzExtractf32x4Ps(k Mmask8, a M256, imm8 int) M128 {
	return M128(maskzExtractf32x4Ps(uint8(k), [8]float32(a), imm8))
}

func maskzExtractf32x4Ps(k uint8, a [8]float32, imm8 int) [4]float32


// Extractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm256_extractf64x2_pd'.
// Requires AVX512DQ.
func Extractf64x2Pd(a M256d, imm8 int) M128d {
	return M128d(extractf64x2Pd([4]float64(a), imm8))
}

func extractf64x2Pd(a [4]float64, imm8 int) [2]float64


// MaskExtractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm256_mask_extractf64x2_pd'.
// Requires AVX512DQ.
func MaskExtractf64x2Pd(src M128d, k Mmask8, a M256d, imm8 int) M128d {
	return M128d(maskExtractf64x2Pd([2]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskExtractf64x2Pd(src [2]float64, k uint8, a [4]float64, imm8 int) [2]float64


// MaskzExtractf64x2Pd: Extract 128 bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) from 'a', selected with 'imm8', and store
// the results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTF64X2'. Intrinsic: '_mm256_maskz_extractf64x2_pd'.
// Requires AVX512DQ.
func MaskzExtractf64x2Pd(k Mmask8, a M256d, imm8 int) M128d {
	return M128d(maskzExtractf64x2Pd(uint8(k), [4]float64(a), imm8))
}

func maskzExtractf64x2Pd(k uint8, a [4]float64, imm8 int) [2]float64


// Extracti128Si256: Extract 128 bits (composed of integer data) from 'a',
// selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI128'. Intrinsic: '_mm256_extracti128_si256'.
// Requires AVX2.
func Extracti128Si256(a M256i, imm8 int) M128i {
	return M128i(extracti128Si256([32]byte(a), imm8))
}

func extracti128Si256(a [32]byte, imm8 int) [16]byte


// Extracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_extracti32x4_epi32'.
// Requires AVX512F.
func Extracti32x4Epi32(a M256i, imm8 int) M128i {
	return M128i(extracti32x4Epi32([32]byte(a), imm8))
}

func extracti32x4Epi32(a [32]byte, imm8 int) [16]byte


// MaskExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_mask_extracti32x4_epi32'.
// Requires AVX512F.
func MaskExtracti32x4Epi32(src M128i, k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskExtracti32x4Epi32([16]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskExtracti32x4Epi32(src [16]byte, k uint8, a [32]byte, imm8 int) [16]byte


// MaskzExtracti32x4Epi32: Extract 128 bits (composed of 4 packed 32-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		FOR j := 0 to 3
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI32X4'. Intrinsic: '_mm256_maskz_extracti32x4_epi32'.
// Requires AVX512F.
func MaskzExtracti32x4Epi32(k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskzExtracti32x4Epi32(uint8(k), [32]byte(a), imm8))
}

func maskzExtracti32x4Epi32(k uint8, a [32]byte, imm8 int) [16]byte


// Extracti64x2Epi64: Extract 128 bits (composed of 2 packed 64-bit integers)
// from 'a', selected with 'imm8', and store the result in 'dst'. 
//
//		CASE imm8[7:0] of
//		0: dst[127:0] := a[127:0]
//		1: dst[127:0] := a[255:128]
//		ESAC
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm256_extracti64x2_epi64'.
// Requires AVX512DQ.
func Extracti64x2Epi64(a M256i, imm8 int) M128i {
	return M128i(extracti64x2Epi64([32]byte(a), imm8))
}

func extracti64x2Epi64(a [32]byte, imm8 int) [16]byte


// MaskExtracti64x2Epi64: Extract 128 bits (composed of 2 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm256_mask_extracti64x2_epi64'.
// Requires AVX512DQ.
func MaskExtracti64x2Epi64(src M128i, k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskExtracti64x2Epi64([16]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskExtracti64x2Epi64(src [16]byte, k uint8, a [32]byte, imm8 int) [16]byte


// MaskzExtracti64x2Epi64: Extract 128 bits (composed of 2 packed 64-bit
// integers) from 'a', selected with 'imm8', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		CASE imm8[7:0] of
//		0: tmp[127:0] := a[127:0]
//		1: tmp[127:0] := a[255:128]
//		ESAC
//		
//		FOR j := 0 to 1
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VEXTRACTI64X2'. Intrinsic: '_mm256_maskz_extracti64x2_epi64'.
// Requires AVX512DQ.
func MaskzExtracti64x2Epi64(k Mmask8, a M256i, imm8 int) M128i {
	return M128i(maskzExtracti64x2Epi64(uint8(k), [32]byte(a), imm8))
}

func maskzExtracti64x2Epi64(k uint8, a [32]byte, imm8 int) [16]byte


// FixupimmPd: Fix up packed double-precision (64-bit) floating-point elements
// in 'a' and 'b' using packed 64-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? –INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 1⁄2
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_fixupimm_pd'.
// Requires AVX512F.
func FixupimmPd(a M256d, b M256d, c M256i, imm8 int) M256d {
	return M256d(fixupimmPd([4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func fixupimmPd(a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// MaskFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? –INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 1⁄2
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_mask_fixupimm_pd'.
// Requires AVX512F.
func MaskFixupimmPd(a M256d, k Mmask8, b M256d, c M256i, imm8 int) M256d {
	return M256d(maskFixupimmPd([4]float64(a), uint8(k), [4]float64(b), [32]byte(c), imm8))
}

func maskFixupimmPd(a [4]float64, k uint8, b [4]float64, c [32]byte, imm8 int) [4]float64


// MaskzFixupimmPd: Fix up packed double-precision (64-bit) floating-point
// elements in 'a' and 'b' using packed 64-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN := 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[63:0], src2[63:0], src3[63:0], imm8[7:0]){
//			tsrc[63:0] := ((src2[62:52] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[63:0]
//			CASE(tsrc[63:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[63:0] := src1[63:0]
//			1 : dest[63:0] := tsrc[63:0]
//			2 : dest[63:0] := QNaN(tsrc[63:0])
//			3 : dest[63:0] := QNAN_Indefinite
//			4 : dest[63:0] := -INF
//			5 : dest[63:0] := +INF
//			6 : dest[63:0] := tsrc.sign? –INF : +INF
//			7 : dest[63:0] := -0
//			8 : dest[63:0] := +0
//			9 : dest[63:0] := -1
//			10: dest[63:0] := +1
//			11: dest[63:0] := 1⁄2
//			12: dest[63:0] := 90.0
//			13: dest[63:0] := PI/2
//			14: dest[63:0] := MAX_FLOAT
//			15: dest[63:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := FIXUPIMMPD(a[i+63:i], b[i+63:i], c[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPD'. Intrinsic: '_mm256_maskz_fixupimm_pd'.
// Requires AVX512F.
func MaskzFixupimmPd(k Mmask8, a M256d, b M256d, c M256i, imm8 int) M256d {
	return M256d(maskzFixupimmPd(uint8(k), [4]float64(a), [4]float64(b), [32]byte(c), imm8))
}

func maskzFixupimmPd(k uint8, a [4]float64, b [4]float64, c [32]byte, imm8 int) [4]float64


// FixupimmPs: Fix up packed single-precision (32-bit) floating-point elements
// in 'a' and 'b' using packed 32-bit integers in 'c', and store the results in
// 'dst'. 'imm8' is used to set the required flags reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? –INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 1⁄2
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_fixupimm_ps'.
// Requires AVX512F.
func FixupimmPs(a M256, b M256, c M256i, imm8 int) M256 {
	return M256(fixupimmPs([8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func fixupimmPs(a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// MaskFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? –INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 1⁄2
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_mask_fixupimm_ps'.
// Requires AVX512F.
func MaskFixupimmPs(a M256, k Mmask8, b M256, c M256i, imm8 int) M256 {
	return M256(maskFixupimmPs([8]float32(a), uint8(k), [8]float32(b), [32]byte(c), imm8))
}

func maskFixupimmPs(a [8]float32, k uint8, b [8]float32, c [32]byte, imm8 int) [8]float32


// MaskzFixupimmPs: Fix up packed single-precision (32-bit) floating-point
// elements in 'a' and 'b' using packed 32-bit integers in 'c', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'imm8' is used to set the required flags
// reporting. 
//
//		enum TOKEN_TYPE {
//			QNAN_TOKEN := 0, 
//			SNAN_TOKEN L= 1, 
//			ZERO_VALUE_TOKEN := 2, 
//			ONE_VALUE_TOKEN := 3, 
//			NEG_INF_TOKEN := 4, 
//			POS_INF_TOKEN := 5, 
//			NEG_VALUE_TOKEN := 6, 
//			POS_VALUE_TOKEN := 7
//		}
//		FIXUPIMMPD(src1[31:0], src2[31:0], src3[31:0], imm8[7:0]){
//			tsrc[31:0] := ((src2[30:23] == 0) AND (MXCSR.DAZ == 1)) ? 0.0 : src2[31:0]
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			QNAN_TOKEN:j := 0
//			SNAN_TOKEN:j := 1
//			ZERO_VALUE_TOKEN: j := 2
//			ONE_VALUE_TOKEN: j := 3
//			NEG_INF_TOKEN: j := 4
//			POS_INF_TOKEN: j := 5
//			NEG_VALUE_TOKEN: j := 6
//			POS_VALUE_TOKEN: j := 7
//			ESAC
//			
//			token_response[3:0] := src3[3+4*j:4*j]
//			
//			CASE(token_response[3:0]) of
//			0 : dest[31:0] := src1[31:0]
//			1 : dest[31:0] := tsrc[31:0]
//			2 : dest[31:0] := QNaN(tsrc[31:0])
//			3 : dest[31:0] := QNAN_Indefinite
//			4 : dest[31:0] := -INF
//			5 : dest[31:0] := +INF
//			6 : dest[31:0] := tsrc.sign? –INF : +INF
//			7 : dest[31:0] := -0
//			8 : dest[31:0] := +0
//			9 : dest[31:0] := -1
//			10: dest[31:0] := +1
//			11: dest[31:0] := 1⁄2
//			12: dest[31:0] := 90.0
//			13: dest[31:0] := PI/2
//			14: dest[31:0] := MAX_FLOAT
//			15: dest[31:0] := -MAX_FLOAT
//			ESAC
//			
//			CASE(tsrc[31:0] of TOKEN_TYPE)
//			ZERO_VALUE_TOKEN: if imm8[0] then set #ZE
//			ZERO_VALUE_TOKEN: if imm8[1] then set #IE
//			ONE_VALUE_TOKEN: if imm8[2] then set #ZE
//			ONE_VALUE_TOKEN: if imm8[3] then set #IE
//			SNAN_TOKEN: if imm8[4] then set #IE
//			NEG_INF_TOKEN: if imm8[5] then set #IE
//			NEG_VALUE_TOKEN: if imm8[6] then set #IE
//			POS_INF_TOKEN: if imm8[7] then set #IE
//			ESAC
//			RETURN dest[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := FIXUPIMMPD(a[i+31:i], b[i+31:i], c[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFIXUPIMMPS'. Intrinsic: '_mm256_maskz_fixupimm_ps'.
// Requires AVX512F.
func MaskzFixupimmPs(k Mmask8, a M256, b M256, c M256i, imm8 int) M256 {
	return M256(maskzFixupimmPs(uint8(k), [8]float32(a), [8]float32(b), [32]byte(c), imm8))
}

func maskzFixupimmPs(k uint8, a [8]float32, b [8]float32, c [32]byte, imm8 int) [8]float32


// FloorPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPD'. Intrinsic: '_mm256_floor_pd'.
// Requires AVX.
func FloorPd(a M256d) M256d {
	return M256d(floorPd([4]float64(a)))
}

func floorPd(a [4]float64) [4]float64


// FloorPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm256_floor_ps'.
// Requires AVX.
func FloorPs(a M256) M256 {
	return M256(floorPs([8]float32(a)))
}

func floorPs(a [8]float32) [8]float32


// FmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_fmadd_pd'.
// Requires FMA.
func FmaddPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmaddPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmaddPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask_fmadd_pd'.
// Requires AVX512F.
func MaskFmaddPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_mask3_fmadd_pd'.
// Requires AVX512F.
func Mask3FmaddPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PD, VFMADD213PD, VFMADD231PD'. Intrinsic: '_mm256_maskz_fmadd_pd'.
// Requires AVX512F.
func MaskzFmaddPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the intermediate result to packed elements in 'c', and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_fmadd_ps'.
// Requires FMA.
func FmaddPs(a M256, b M256, c M256) M256 {
	return M256(fmaddPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmaddPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask_fmadd_ps'.
// Requires AVX512F.
func MaskFmaddPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_mask3_fmadd_ps'.
// Requires AVX512F.
func Mask3FmaddPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the intermediate result to packed elements in
// 'c', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADD132PS, VFMADD213PS, VFMADD231PS'. Intrinsic: '_mm256_maskz_fmadd_ps'.
// Requires AVX512F.
func MaskzFmaddPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_fmaddsub_pd'.
// Requires FMA.
func FmaddsubPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmaddsubPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmaddsubPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask_fmaddsub_pd'.
// Requires AVX512F.
func MaskFmaddsubPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmaddsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmaddsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_mask3_fmaddsub_pd'.
// Requires AVX512F.
func Mask3FmaddsubPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmaddsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmaddsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmaddsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PD, VFMADDSUB213PD, VFMADDSUB231PD'. Intrinsic: '_mm256_maskz_fmaddsub_pd'.
// Requires AVX512F.
func MaskzFmaddsubPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmaddsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmaddsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_fmaddsub_ps'.
// Requires FMA.
func FmaddsubPs(a M256, b M256, c M256) M256 {
	return M256(fmaddsubPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmaddsubPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask_fmaddsub_ps'.
// Requires AVX512F.
func MaskFmaddsubPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmaddsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmaddsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_mask3_fmaddsub_ps'.
// Requires AVX512F.
func Mask3FmaddsubPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmaddsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmaddsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmaddsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively add and subtract packed elements in
// 'c' to/from the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMADDSUB132PS, VFMADDSUB213PS, VFMADDSUB231PS'. Intrinsic: '_mm256_maskz_fmaddsub_ps'.
// Requires AVX512F.
func MaskzFmaddsubPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmaddsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmaddsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_fmsub_pd'.
// Requires FMA.
func FmsubPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmsubPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmsubPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask_fmsub_pd'.
// Requires AVX512F.
func MaskFmsubPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_mask3_fmsub_pd'.
// Requires AVX512F.
func Mask3FmsubPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PD, VFMSUB213PD, VFMSUB231PD'. Intrinsic: '_mm256_maskz_fmsub_pd'.
// Requires AVX512F.
func MaskzFmsubPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the intermediate
// result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_fmsub_ps'.
// Requires FMA.
func FmsubPs(a M256, b M256, c M256) M256 {
	return M256(fmsubPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmsubPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask_fmsub_ps'.
// Requires AVX512F.
func MaskFmsubPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_mask3_fmsub_ps'.
// Requires AVX512F.
func Mask3FmsubPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUB132PS, VFMSUB213PS, VFMSUB231PS'. Intrinsic: '_mm256_maskz_fmsub_ps'.
// Requires AVX512F.
func MaskzFmsubPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF (j is even) 
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_fmsubadd_pd'.
// Requires FMA.
func FmsubaddPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fmsubaddPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fmsubaddPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask_fmsubadd_pd'.
// Requires AVX512F.
func MaskFmsubaddPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFmsubaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFmsubaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_mask3_fmsubadd_pd'.
// Requires AVX512F.
func Mask3FmsubaddPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FmsubaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FmsubaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFmsubaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF (j is even) 
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) + c[i+63:i]
//				ELSE
//					dst[i+63:i] := (a[i+63:i] * b[i+63:i]) - c[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PD, VFMSUBADD213PD, VFMSUBADD231PD'. Intrinsic: '_mm256_maskz_fmsubadd_pd'.
// Requires AVX512F.
func MaskzFmsubaddPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFmsubaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFmsubaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF (j is even) 
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_fmsubadd_ps'.
// Requires FMA.
func FmsubaddPs(a M256, b M256, c M256) M256 {
	return M256(fmsubaddPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fmsubaddPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask_fmsubadd_ps'.
// Requires AVX512F.
func MaskFmsubaddPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFmsubaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFmsubaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'c' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_mask3_fmsubadd_ps'.
// Requires AVX512F.
func Mask3FmsubaddPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FmsubaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FmsubaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFmsubaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', alternatively subtract and add packed elements in
// 'c' from/to the intermediate result, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF (j is even) 
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) + c[i+31:i]
//				ELSE
//					dst[i+31:i] := (a[i+31:i] * b[i+31:i]) - c[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VFMSUBADD132PS, VFMSUBADD213PS, VFMSUBADD231PS'. Intrinsic: '_mm256_maskz_fmsubadd_ps'.
// Requires AVX512F.
func MaskzFmsubaddPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFmsubaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFmsubaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FnmaddPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_fnmadd_pd'.
// Requires FMA.
func FnmaddPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fnmaddPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fnmaddPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask_fnmadd_pd'.
// Requires AVX512F.
func MaskFnmaddPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFnmaddPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFnmaddPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_mask3_fnmadd_pd'.
// Requires AVX512F.
func Mask3FnmaddPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FnmaddPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FnmaddPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFnmaddPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) + c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PD, VFNMADD213PD, VFNMADD231PD'. Intrinsic: '_mm256_maskz_fnmadd_pd'.
// Requires AVX512F.
func MaskzFnmaddPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFnmaddPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFnmaddPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FnmaddPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', add the negated intermediate result to packed elements in
// 'c', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			a[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_fnmadd_ps'.
// Requires FMA.
func FnmaddPs(a M256, b M256, c M256) M256 {
	return M256(fnmaddPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fnmaddPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask_fnmadd_ps'.
// Requires AVX512F.
func MaskFnmaddPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFnmaddPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFnmaddPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_mask3_fnmadd_ps'.
// Requires AVX512F.
func Mask3FnmaddPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FnmaddPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FnmaddPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFnmaddPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', add the negated intermediate result to packed
// elements in 'c', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) + c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMADD132PS, VFNMADD213PS, VFNMADD231PS'. Intrinsic: '_mm256_maskz_fnmadd_ps'.
// Requires AVX512F.
func MaskzFnmaddPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFnmaddPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFnmaddPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FnmsubPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_fnmsub_pd'.
// Requires FMA.
func FnmsubPd(a M256d, b M256d, c M256d) M256d {
	return M256d(fnmsubPd([4]float64(a), [4]float64(b), [4]float64(c)))
}

func fnmsubPd(a [4]float64, b [4]float64, c [4]float64) [4]float64


// MaskFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask_fnmsub_pd'.
// Requires AVX512F.
func MaskFnmsubPd(a M256d, k Mmask8, b M256d, c M256d) M256d {
	return M256d(maskFnmsubPd([4]float64(a), uint8(k), [4]float64(b), [4]float64(c)))
}

func maskFnmsubPd(a [4]float64, k uint8, b [4]float64, c [4]float64) [4]float64


// Mask3FnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := c[i+63:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_mask3_fnmsub_pd'.
// Requires AVX512F.
func Mask3FnmsubPd(a M256d, b M256d, c M256d, k Mmask8) M256d {
	return M256d(mask3FnmsubPd([4]float64(a), [4]float64(b), [4]float64(c), uint8(k)))
}

func mask3FnmsubPd(a [4]float64, b [4]float64, c [4]float64, k uint8) [4]float64


// MaskzFnmsubPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := -(a[i+63:i] * b[i+63:i]) - c[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PD, VFNMSUB213PD, VFNMSUB231PD'. Intrinsic: '_mm256_maskz_fnmsub_pd'.
// Requires AVX512F.
func MaskzFnmsubPd(k Mmask8, a M256d, b M256d, c M256d) M256d {
	return M256d(maskzFnmsubPd(uint8(k), [4]float64(a), [4]float64(b), [4]float64(c)))
}

func maskzFnmsubPd(k uint8, a [4]float64, b [4]float64, c [4]float64) [4]float64


// FnmsubPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_fnmsub_ps'.
// Requires FMA.
func FnmsubPs(a M256, b M256, c M256) M256 {
	return M256(fnmsubPs([8]float32(a), [8]float32(b), [8]float32(c)))
}

func fnmsubPs(a [8]float32, b [8]float32, c [8]float32) [8]float32


// MaskFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask_fnmsub_ps'.
// Requires AVX512F.
func MaskFnmsubPs(a M256, k Mmask8, b M256, c M256) M256 {
	return M256(maskFnmsubPs([8]float32(a), uint8(k), [8]float32(b), [8]float32(c)))
}

func maskFnmsubPs(a [8]float32, k uint8, b [8]float32, c [8]float32) [8]float32


// Mask3FnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'c' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := c[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_mask3_fnmsub_ps'.
// Requires AVX512F.
func Mask3FnmsubPs(a M256, b M256, c M256, k Mmask8) M256 {
	return M256(mask3FnmsubPs([8]float32(a), [8]float32(b), [8]float32(c), uint8(k)))
}

func mask3FnmsubPs(a [8]float32, b [8]float32, c [8]float32, k uint8) [8]float32


// MaskzFnmsubPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', subtract packed elements in 'c' from the negated
// intermediate result, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := -(a[i+31:i] * b[i+31:i]) - c[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VFNMSUB132PS, VFNMSUB213PS, VFNMSUB231PS'. Intrinsic: '_mm256_maskz_fnmsub_ps'.
// Requires AVX512F.
func MaskzFnmsubPs(k Mmask8, a M256, b M256, c M256) M256 {
	return M256(maskzFnmsubPs(uint8(k), [8]float32(a), [8]float32(b), [8]float32(c)))
}

func maskzFnmsubPs(k uint8, a [8]float32, b [8]float32, c [8]float32) [8]float32


// FpclassPdMask: Test packed double-precision (64-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm256_fpclass_pd_mask'.
// Requires AVX512DQ.
func FpclassPdMask(a M256d, imm8 int) Mmask8 {
	return Mmask8(fpclassPdMask([4]float64(a), imm8))
}

func fpclassPdMask(a [4]float64, imm8 int) uint8


// MaskFpclassPdMask: Test packed double-precision (64-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := CheckFPClass_FP64(a[i+63:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VFPCLASSPD'. Intrinsic: '_mm256_mask_fpclass_pd_mask'.
// Requires AVX512DQ.
func MaskFpclassPdMask(k1 Mmask8, a M256d, imm8 int) Mmask8 {
	return Mmask8(maskFpclassPdMask(uint8(k1), [4]float64(a), imm8))
}

func maskFpclassPdMask(k1 uint8, a [4]float64, imm8 int) uint8


// FpclassPsMask: Test packed single-precision (32-bit) floating-point elements
// in 'a' for special categories specified by 'imm8', and store the results in
// mask vector 'k'.
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm256_fpclass_ps_mask'.
// Requires AVX512DQ.
func FpclassPsMask(a M256, imm8 int) Mmask8 {
	return Mmask8(fpclassPsMask([8]float32(a), imm8))
}

func fpclassPsMask(a [8]float32, imm8 int) uint8


// MaskFpclassPsMask: Test packed single-precision (32-bit) floating-point
// elements in 'a' for special categories specified by 'imm8', and store the
// results in mask vector 'k' using zeromask 'k1' (elements are zeroed out when
// the corresponding mask bit is not set).
// 	'imm' can be a combination of:
//     0x01 // QNaN
//     0x02 // Positive Zero
//     0x04 // Negative Zero
//     0x08 // Positive Infinity
//     0x10 // Negative Infinity
//     0x20 // Denormal
//     0x40 // Negative
//     0x80 // SNaN 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := CheckFPClass_FP32(a[i+31:i], imm8[7:0])
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VFPCLASSPS'. Intrinsic: '_mm256_mask_fpclass_ps_mask'.
// Requires AVX512DQ.
func MaskFpclassPsMask(k1 Mmask8, a M256, imm8 int) Mmask8 {
	return Mmask8(maskFpclassPsMask(uint8(k1), [8]float32(a), imm8))
}

func maskFpclassPsMask(k1 uint8, a [8]float32, imm8 int) uint8


// GetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_getexp_pd'.
// Requires AVX512F.
func GetexpPd(a M256d) M256d {
	return M256d(getexpPd([4]float64(a)))
}

func getexpPd(a [4]float64) [4]float64


// MaskGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_mask_getexp_pd'.
// Requires AVX512F.
func MaskGetexpPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskGetexpPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskGetexpPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzGetexpPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision (64-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPD'. Intrinsic: '_mm256_maskz_getexp_pd'.
// Requires AVX512F.
func MaskzGetexpPd(k Mmask8, a M256d) M256d {
	return M256d(maskzGetexpPd(uint8(k), [4]float64(a)))
}

func maskzGetexpPd(k uint8, a [4]float64) [4]float64


// GetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'.
// This intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_getexp_ps'.
// Requires AVX512F.
func GetexpPs(a M256) M256 {
	return M256(getexpPs([8]float32(a)))
}

func getexpPs(a [8]float32) [8]float32


// MaskGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). This intrinsic essentially calculates 'floor(log2(x))'
// for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_mask_getexp_ps'.
// Requires AVX512F.
func MaskGetexpPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskGetexpPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskGetexpPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzGetexpPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision (32-bit) floating-point
// number representing the integer exponent, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). This intrinsic essentially calculates 'floor(log2(x))' for each
// element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETEXPPS'. Intrinsic: '_mm256_maskz_getexp_ps'.
// Requires AVX512F.
func MaskzGetexpPs(k Mmask8, a M256) M256 {
	return M256(maskzGetexpPs(uint8(k), [8]float32(a)))
}

func maskzGetexpPs(k uint8, a [8]float32) [8]float32


// GetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '±(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_getmant_pd'.
// Requires AVX512F.
func GetmantPd(a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(getmantPd([4]float64(a), interv, sc))
}

func getmantPd(a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// MaskGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '±(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_mask_getmant_pd'.
// Requires AVX512F.
func MaskGetmantPd(src M256d, k Mmask8, a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(maskGetmantPd([4]float64(src), uint8(k), [4]float64(a), interv, sc))
}

func maskGetmantPd(src [4]float64, k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// MaskzGetmantPd: Normalize the mantissas of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '±(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := GetNormalizedMantissa(a[i+63:i], sc, interv)
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPD'. Intrinsic: '_mm256_maskz_getmant_pd'.
// Requires AVX512F.
func MaskzGetmantPd(k Mmask8, a M256d, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256d {
	return M256d(maskzGetmantPd(uint8(k), [4]float64(a), interv, sc))
}

func maskzGetmantPd(k uint8, a [4]float64, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [4]float64


// GetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. This
// intrinsic essentially calculates '±(2^k)*|x.significand|', where 'k'
// depends on the interval range defined by 'interv' and the sign depends on
// 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_getmant_ps'.
// Requires AVX512F.
func GetmantPs(a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(getmantPs([8]float32(a), interv, sc))
}

func getmantPs(a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// MaskGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). This intrinsic essentially calculates
// '±(2^k)*|x.significand|', where 'k' depends on the interval range defined
// by 'interv' and the sign depends on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_mask_getmant_ps'.
// Requires AVX512F.
func MaskGetmantPs(src M256, k Mmask8, a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(maskGetmantPs([8]float32(src), uint8(k), [8]float32(a), interv, sc))
}

func maskGetmantPs(src [8]float32, k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// MaskzGetmantPs: Normalize the mantissas of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). This intrinsic essentially calculates '±(2^k)*|x.significand|', where
// 'k' depends on the interval range defined by 'interv' and the sign depends
// on 'sc' and the source sign.
// 	The mantissa is normalized to the interval specified by 'interv', which can
// take the following values:
//     _MM_MANT_NORM_1_2     // interval [1, 2)
//     _MM_MANT_NORM_p5_2    // interval [0.5, 2)
//     _MM_MANT_NORM_p5_1    // interval [0.5, 1)
//     _MM_MANT_NORM_p75_1p5 // interval [0.75, 1.5)The sign is determined by 'sc' which can take the following values:
//     _MM_MANT_SIGN_src     // sign = sign(src)
//     _MM_MANT_SIGN_zero    // sign = 0
//     _MM_MANT_SIGN_nan     // dst = NaN if sign(src) = 1 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := GetNormalizedMantissa(a[i+31:i], sc, interv)
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGETMANTPS'. Intrinsic: '_mm256_maskz_getmant_ps'.
// Requires AVX512F.
func MaskzGetmantPs(k Mmask8, a M256, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) M256 {
	return M256(maskzGetmantPs(uint8(k), [8]float32(a), interv, sc))
}

func maskzGetmantPs(k uint8, a [8]float32, interv MMMANTISSANORMENUM, sc MMMANTISSASIGNENUM) [8]float32


// HaddEpi16: Horizontally add adjacent pairs of 16-bit integers in 'a' and
// 'b', and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0] := a[31:16] + a[15:0]
//		dst[31:16] := a[63:48] + a[47:32]
//		dst[47:32] := a[95:80] + a[79:64]
//		dst[63:48] := a[127:112] + a[111:96]
//		dst[79:64] := b[31:16] + b[15:0]
//		dst[95:80] := b[63:48] + b[47:32]
//		dst[111:96] := b[95:80] + b[79:64]
//		dst[127:112] := b[127:112] + b[111:96]
//		dst[143:128] := a[159:144] + a[143:128]
//		dst[159:144] := a[191:176] + a[175:160]
//		dst[175:160] := a[223:208] + a[207:192]
//		dst[191:176] := a[255:240] + a[239:224]
//		dst[207:192] := b[127:112] + b[143:128]
//		dst[223:208] := b[159:144] + b[175:160]
//		dst[239:224] := b[191:176] + b[207:192]
//		dst[255:240] := b[223:208] + b[239:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHADDW'. Intrinsic: '_mm256_hadd_epi16'.
// Requires AVX2.
func HaddEpi16(a M256i, b M256i) M256i {
	return M256i(haddEpi16([32]byte(a), [32]byte(b)))
}

func haddEpi16(a [32]byte, b [32]byte) [32]byte


// HaddEpi32: Horizontally add adjacent pairs of 32-bit integers in 'a' and
// 'b', and pack the signed 32-bit results in 'dst'. 
//
//		dst[31:0] := a[63:32] + a[31:0]
//		dst[63:32] := a[127:96] + a[95:64]
//		dst[95:64] := b[63:32] + b[31:0]
//		dst[127:96] := b[127:96] + b[95:64]
//		dst[159:128] := a[191:160] + a[159:128]
//		dst[191:160] := a[255:224] + a[223:192]
//		dst[223:192] := b[191:160] + b[159:128]
//		dst[255:224] := b[255:224] + b[223:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHADDD'. Intrinsic: '_mm256_hadd_epi32'.
// Requires AVX2.
func HaddEpi32(a M256i, b M256i) M256i {
	return M256i(haddEpi32([32]byte(a), [32]byte(b)))
}

func haddEpi32(a [32]byte, b [32]byte) [32]byte


// HaddPd: Horizontally add adjacent pairs of double-precision (64-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[63:0] := a[127:64] + a[63:0]
//		dst[127:64] := b[127:64] + b[63:0]
//		dst[191:128] := a[255:192] + a[191:128]
//		dst[255:192] := b[255:192] + b[191:128]
//		dst[MAX:256] := 0
//
// Instruction: 'VHADDPD'. Intrinsic: '_mm256_hadd_pd'.
// Requires AVX.
func HaddPd(a M256d, b M256d) M256d {
	return M256d(haddPd([4]float64(a), [4]float64(b)))
}

func haddPd(a [4]float64, b [4]float64) [4]float64


// HaddPs: Horizontally add adjacent pairs of single-precision (32-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[31:0] := a[63:32] + a[31:0]
//		dst[63:32] := a[127:96] + a[95:64]
//		dst[95:64] := b[63:32] + b[31:0]
//		dst[127:96] := b[127:96] + b[95:64]
//		dst[159:128] := a[191:160] + a[159:128]
//		dst[191:160] := a[255:224] + a[223:192]
//		dst[223:192] := b[191:160] + b[159:128]
//		dst[255:224] := b[255:224] + b[223:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VHADDPS'. Intrinsic: '_mm256_hadd_ps'.
// Requires AVX.
func HaddPs(a M256, b M256) M256 {
	return M256(haddPs([8]float32(a), [8]float32(b)))
}

func haddPs(a [8]float32, b [8]float32) [8]float32


// HaddsEpi16: Horizontally add adjacent pairs of 16-bit integers in 'a' and
// 'b' using saturation, and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0]= Saturate_To_Int16(a[31:16] + a[15:0])
//		dst[31:16] = Saturate_To_Int16(a[63:48] + a[47:32])
//		dst[47:32] = Saturate_To_Int16(a[95:80] + a[79:64])
//		dst[63:48] = Saturate_To_Int16(a[127:112] + a[111:96])
//		dst[79:64] = Saturate_To_Int16(b[31:16] + b[15:0])
//		dst[95:80] = Saturate_To_Int16(b[63:48] + b[47:32])
//		dst[111:96] = Saturate_To_Int16(b[95:80] + b[79:64])
//		dst[127:112] = Saturate_To_Int16(b[127:112] + b[111:96])
//		dst[143:128] = Saturate_To_Int16(a[159:144] + a[143:128])
//		dst[159:144] = Saturate_To_Int16(a[191:176] + a[175:160])
//		dst[175:160] = Saturate_To_Int16( a[223:208] + a[207:192])
//		dst[191:176] = Saturate_To_Int16(a[255:240] + a[239:224])
//		dst[207:192] = Saturate_To_Int16(b[127:112] + b[143:128])
//		dst[223:208] = Saturate_To_Int16(b[159:144] + b[175:160])
//		dst[239:224] = Saturate_To_Int16(b[191-160] + b[159-128])
//		dst[255:240] = Saturate_To_Int16(b[255:240] + b[239:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPHADDSW'. Intrinsic: '_mm256_hadds_epi16'.
// Requires AVX2.
func HaddsEpi16(a M256i, b M256i) M256i {
	return M256i(haddsEpi16([32]byte(a), [32]byte(b)))
}

func haddsEpi16(a [32]byte, b [32]byte) [32]byte


// HsubEpi16: Horizontally subtract adjacent pairs of 16-bit integers in 'a'
// and 'b', and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0] := a[15:0] - a[31:16]
//		dst[31:16] := a[47:32] - a[63:48]
//		dst[47:32] := a[79:64] - a[95:80]
//		dst[63:48] := a[111:96] - a[127:112]
//		dst[79:64] := b[15:0] - b[31:16]
//		dst[95:80] := b[47:32] - b[63:48]
//		dst[111:96] := b[79:64] - b[95:80]
//		dst[127:112] := b[111:96] - b[127:112]
//		dst[143:128] := a[143:128] - a[159:144]
//		dst[159:144] := a[175:160] - a[191:176]
//		dst[175:160] := a[207:192] - a[223:208]
//		dst[191:176] := a[239:224] - a[255:240]
//		dst[207:192] := b[143:128] - b[159:144]
//		dst[223:208] := b[175:160] - b[191:176]
//		dst[239:224] := b[207:192] - b[223:208]
//		dst[255:240] := b[239:224] - b[255:240]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHSUBW'. Intrinsic: '_mm256_hsub_epi16'.
// Requires AVX2.
func HsubEpi16(a M256i, b M256i) M256i {
	return M256i(hsubEpi16([32]byte(a), [32]byte(b)))
}

func hsubEpi16(a [32]byte, b [32]byte) [32]byte


// HsubEpi32: Horizontally subtract adjacent pairs of 32-bit integers in 'a'
// and 'b', and pack the signed 32-bit results in 'dst'. 
//
//		dst[31:0] := a[31:0] - a[63:32]
//		dst[63:32] := a[95:64] - a[127:96]
//		dst[95:64] := b[31:0] - b[63:32]
//		dst[127:96] := b[95:64] - b[127:96]
//		dst[159:128] := a[159:128] - a[191:160]
//		dst[191:160] := a[223:192] - a[255:224]
//		dst[223:192] := b[159:128] - b[191:160]
//		dst[255:224] := b[223:192] - b[255:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VPHSUBD'. Intrinsic: '_mm256_hsub_epi32'.
// Requires AVX2.
func HsubEpi32(a M256i, b M256i) M256i {
	return M256i(hsubEpi32([32]byte(a), [32]byte(b)))
}

func hsubEpi32(a [32]byte, b [32]byte) [32]byte


// HsubPd: Horizontally subtract adjacent pairs of double-precision (64-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[63:0] := a[63:0] - a[127:64]
//		dst[127:64] := b[63:0] - b[127:64]
//		dst[191:128] := a[191:128] - a[255:192]
//		dst[255:192] := b[191:128] - b[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VHSUBPD'. Intrinsic: '_mm256_hsub_pd'.
// Requires AVX.
func HsubPd(a M256d, b M256d) M256d {
	return M256d(hsubPd([4]float64(a), [4]float64(b)))
}

func hsubPd(a [4]float64, b [4]float64) [4]float64


// HsubPs: Horizontally add adjacent pairs of single-precision (32-bit)
// floating-point elements in 'a' and 'b', and pack the results in 'dst'. 
//
//		dst[31:0] := a[31:0] - a[63:32]
//		dst[63:32] := a[95:64] - a[127:96]
//		dst[95:64] := b[31:0] - b[63:32]
//		dst[127:96] := b[95:64] - b[127:96]
//		dst[159:128] := a[159:128] - a[191:160]
//		dst[191:160] := a[223:192] - a[255:224]
//		dst[223:192] := b[159:128] - b[191:160]
//		dst[255:224] := b[223:192] - b[255:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VHSUBPS'. Intrinsic: '_mm256_hsub_ps'.
// Requires AVX.
func HsubPs(a M256, b M256) M256 {
	return M256(hsubPs([8]float32(a), [8]float32(b)))
}

func hsubPs(a [8]float32, b [8]float32) [8]float32


// HsubsEpi16: Horizontally subtract adjacent pairs of 16-bit integers in 'a'
// and 'b' using saturation, and pack the signed 16-bit results in 'dst'. 
//
//		dst[15:0]= Saturate_To_Int16(a[15:0] - a[31:16])
//		dst[31:16] = Saturate_To_Int16(a[47:32] - a[63:48])
//		dst[47:32] = Saturate_To_Int16(a[79:64] - a[95:80])
//		dst[63:48] = Saturate_To_Int16(a[111:96] - a[127:112])
//		dst[79:64] = Saturate_To_Int16(b[15:0] - b[31:16])
//		dst[95:80] = Saturate_To_Int16(b[47:32] - b[63:48])
//		dst[111:96] = Saturate_To_Int16(b[79:64] - b[95:80])
//		dst[127:112] = Saturate_To_Int16(b[111:96] - b[127:112])
//		dst[143:128]= Saturate_To_Int16(a[143:128] - a[159:144])
//		dst[159:144] = Saturate_To_Int16(a[175:160] - a[191:176])
//		dst[175:160] = Saturate_To_Int16(a[207:192] - a[223:208])
//		dst[191:176] = Saturate_To_Int16(a[239:224] - a[255:240])
//		dst[207:192] = Saturate_To_Int16(b[143:128] - b[159:144])
//		dst[223:208] = Saturate_To_Int16(b[175:160] - b[191:176])
//		dst[239:224] = Saturate_To_Int16(b[207:192] - b[223:208])
//		dst[255:240] = Saturate_To_Int16(b[239:224] - b[255:240])
//		dst[MAX:256] := 0
//
// Instruction: 'VPHSUBSW'. Intrinsic: '_mm256_hsubs_epi16'.
// Requires AVX2.
func HsubsEpi16(a M256i, b M256i) M256i {
	return M256i(hsubsEpi16([32]byte(a), [32]byte(b)))
}

func hsubsEpi16(a [32]byte, b [32]byte) [32]byte


// HypotPd: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// double-precision (64-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i]^2 + b[i+63:i]^2)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_hypot_pd'.
// Requires AVX.
func HypotPd(a M256d, b M256d) M256d {
	return M256d(hypotPd([4]float64(a), [4]float64(b)))
}

func hypotPd(a [4]float64, b [4]float64) [4]float64


// HypotPs: Compute the length of the hypotenous of a right triangle, with the
// lengths of the other two sides of the triangle stored as packed
// single-precision (32-bit) floating-point elements in 'a' and 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i]^2 + b[i+31:i]^2)
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_hypot_ps'.
// Requires AVX.
func HypotPs(a M256, b M256) M256 {
	return M256(hypotPs([8]float32(a), [8]float32(b)))
}

func hypotPs(a [8]float32, b [8]float32) [8]float32


// I32gatherEpi32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm256_i32gather_epi32'.
// Requires AVX2.
func I32gatherEpi32(base_addr int, vindex M256i, scale int) M256i {
	return M256i(i32gatherEpi32(base_addr, [32]byte(vindex), scale))
}

func i32gatherEpi32(base_addr int, vindex [32]byte, scale int) [32]byte


// MaskI32gatherEpi32: Gather 32-bit integers from memory using 32-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm256_mask_i32gather_epi32'.
// Requires AVX2.
func MaskI32gatherEpi32(src M256i, base_addr int, vindex M256i, mask M256i, scale int) M256i {
	return M256i(maskI32gatherEpi32([32]byte(src), base_addr, [32]byte(vindex), [32]byte(mask), scale))
}

func maskI32gatherEpi32(src [32]byte, base_addr int, vindex [32]byte, mask [32]byte, scale int) [32]byte


// MmaskI32gatherEpi32: Gather 32-bit integers from memory using 32-bit
// indices. 32-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDD'. Intrinsic: '_mm256_mmask_i32gather_epi32'.
// Requires AVX512F.
func MmaskI32gatherEpi32(src M256i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI32gatherEpi32([32]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherEpi32(src [32]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [32]byte


// I32gatherEpi64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm256_i32gather_epi64'.
// Requires AVX2.
func I32gatherEpi64(base_addr int, vindex M128i, scale int) M256i {
	return M256i(i32gatherEpi64(base_addr, [16]byte(vindex), scale))
}

func i32gatherEpi64(base_addr int, vindex [16]byte, scale int) [32]byte


// MaskI32gatherEpi64: Gather 64-bit integers from memory using 32-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 32-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm256_mask_i32gather_epi64'.
// Requires AVX2.
func MaskI32gatherEpi64(src M256i, base_addr int, vindex M128i, mask M256i, scale int) M256i {
	return M256i(maskI32gatherEpi64([32]byte(src), base_addr, [16]byte(vindex), [32]byte(mask), scale))
}

func maskI32gatherEpi64(src [32]byte, base_addr int, vindex [16]byte, mask [32]byte, scale int) [32]byte


// MmaskI32gatherEpi64: Gather 64-bit integers from memory using 32-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERDQ'. Intrinsic: '_mm256_mmask_i32gather_epi64'.
// Requires AVX512F.
func MmaskI32gatherEpi64(src M256i, k Mmask8, vindex M128i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI32gatherEpi64([32]byte(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherEpi64(src [32]byte, k uint8, vindex [16]byte, base_addr uintptr, scale int) [32]byte


// I32gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm256_i32gather_pd'.
// Requires AVX2.
func I32gatherPd(base_addr float64, vindex M128i, scale int) M256d {
	return M256d(i32gatherPd(base_addr, [16]byte(vindex), scale))
}

func i32gatherPd(base_addr float64, vindex [16]byte, scale int) [4]float64


// MaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm256_mask_i32gather_pd'.
// Requires AVX2.
func MaskI32gatherPd(src M256d, base_addr float64, vindex M128i, mask M256d, scale int) M256d {
	return M256d(maskI32gatherPd([4]float64(src), base_addr, [16]byte(vindex), [4]float64(mask), scale))
}

func maskI32gatherPd(src [4]float64, base_addr float64, vindex [16]byte, mask [4]float64, scale int) [4]float64


// MmaskI32gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 32-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			m := j*32
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[m+31:m])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPD'. Intrinsic: '_mm256_mmask_i32gather_pd'.
// Requires AVX512F.
func MmaskI32gatherPd(src M256d, k Mmask8, vindex M128i, base_addr uintptr, scale int) M256d {
	return M256d(mmaskI32gatherPd([4]float64(src), uint8(k), [16]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPd(src [4]float64, k uint8, vindex [16]byte, base_addr uintptr, scale int) [4]float64


// I32gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm256_i32gather_ps'.
// Requires AVX2.
func I32gatherPs(base_addr float32, vindex M256i, scale int) M256 {
	return M256(i32gatherPs(base_addr, [32]byte(vindex), scale))
}

func i32gatherPs(base_addr float32, vindex [32]byte, scale int) [8]float32


// MaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm256_mask_i32gather_ps'.
// Requires AVX2.
func MaskI32gatherPs(src M256, base_addr float32, vindex M256i, mask M256, scale int) M256 {
	return M256(maskI32gatherPs([8]float32(src), base_addr, [32]byte(vindex), [8]float32(mask), scale))
}

func maskI32gatherPs(src [8]float32, base_addr float32, vindex [32]byte, mask [8]float32, scale int) [8]float32


// MmaskI32gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 32-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 32-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+31:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERDPS'. Intrinsic: '_mm256_mmask_i32gather_ps'.
// Requires AVX512F.
func MmaskI32gatherPs(src M256, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256 {
	return M256(mmaskI32gatherPs([8]float32(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI32gatherPs(src [8]float32, k uint8, vindex [32]byte, base_addr uintptr, scale int) [8]float32


// I32scatterEpi32: Scatter 32-bit integers from 'a' into memory using 32-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm256_i32scatter_epi32'.
// Requires AVX512F.
func I32scatterEpi32(base_addr uintptr, vindex M256i, a M256i, scale int)  {
	i32scatterEpi32(uintptr(base_addr), [32]byte(vindex), [32]byte(a), scale)
}

func i32scatterEpi32(base_addr uintptr, vindex [32]byte, a [32]byte, scale int) 


// MaskI32scatterEpi32: Scatter 32-bit integers from 'a' into memory using
// 32-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPSCATTERDD'. Intrinsic: '_mm256_mask_i32scatter_epi32'.
// Requires AVX512F.
func MaskI32scatterEpi32(base_addr uintptr, k Mmask8, vindex M256i, a M256i, scale int)  {
	maskI32scatterEpi32(uintptr(base_addr), uint8(k), [32]byte(vindex), [32]byte(a), scale)
}

func maskI32scatterEpi32(base_addr uintptr, k uint8, vindex [32]byte, a [32]byte, scale int) 


// I32scatterEpi64: Scatter 64-bit integers from 'a' into memory using 32-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 32-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm256_i32scatter_epi64'.
// Requires AVX512F.
func I32scatterEpi64(base_addr uintptr, vindex M128i, a M256i, scale int)  {
	i32scatterEpi64(uintptr(base_addr), [16]byte(vindex), [32]byte(a), scale)
}

func i32scatterEpi64(base_addr uintptr, vindex [16]byte, a [32]byte, scale int) 


// MaskI32scatterEpi64: Scatter 64-bit integers from 'a' into memory using
// 32-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 32-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERDQ'. Intrinsic: '_mm256_mask_i32scatter_epi64'.
// Requires AVX512F.
func MaskI32scatterEpi64(base_addr uintptr, k Mmask8, vindex M128i, a M256i, scale int)  {
	maskI32scatterEpi64(uintptr(base_addr), uint8(k), [16]byte(vindex), [32]byte(a), scale)
}

func maskI32scatterEpi64(base_addr uintptr, k uint8, vindex [16]byte, a [32]byte, scale int) 


// I32scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm256_i32scatter_pd'.
// Requires AVX512F.
func I32scatterPd(base_addr uintptr, vindex M128i, a M256d, scale int)  {
	i32scatterPd(uintptr(base_addr), [16]byte(vindex), [4]float64(a), scale)
}

func i32scatterPd(base_addr uintptr, vindex [16]byte, a [4]float64, scale int) 


// MaskI32scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			l := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+31:l])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERDPD'. Intrinsic: '_mm256_mask_i32scatter_pd'.
// Requires AVX512F.
func MaskI32scatterPd(base_addr uintptr, k Mmask8, vindex M128i, a M256d, scale int)  {
	maskI32scatterPd(uintptr(base_addr), uint8(k), [16]byte(vindex), [4]float64(a), scale)
}

func maskI32scatterPd(base_addr uintptr, k uint8, vindex [16]byte, a [4]float64, scale int) 


// I32scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm256_i32scatter_ps'.
// Requires AVX512F.
func I32scatterPs(base_addr uintptr, vindex M256i, a M256, scale int)  {
	i32scatterPs(uintptr(base_addr), [32]byte(vindex), [8]float32(a), scale)
}

func i32scatterPs(base_addr uintptr, vindex [32]byte, a [8]float32, scale int) 


// MaskI32scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 32-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 32-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+31:i])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VSCATTERDPS'. Intrinsic: '_mm256_mask_i32scatter_ps'.
// Requires AVX512F.
func MaskI32scatterPs(base_addr uintptr, k Mmask8, vindex M256i, a M256, scale int)  {
	maskI32scatterPs(uintptr(base_addr), uint8(k), [32]byte(vindex), [8]float32(a), scale)
}

func maskI32scatterPs(base_addr uintptr, k uint8, vindex [32]byte, a [8]float32, scale int) 


// I64gatherEpi32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm256_i64gather_epi32'.
// Requires AVX2.
func I64gatherEpi32(base_addr int, vindex M256i, scale int) M128i {
	return M128i(i64gatherEpi32(base_addr, [32]byte(vindex), scale))
}

func i64gatherEpi32(base_addr int, vindex [32]byte, scale int) [16]byte


// MaskI64gatherEpi32: Gather 32-bit integers from memory using 64-bit indices.
// 32-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm256_mask_i64gather_epi32'.
// Requires AVX2.
func MaskI64gatherEpi32(src M128i, base_addr int, vindex M256i, mask M128i, scale int) M128i {
	return M128i(maskI64gatherEpi32([16]byte(src), base_addr, [32]byte(vindex), [16]byte(mask), scale))
}

func maskI64gatherEpi32(src [16]byte, base_addr int, vindex [32]byte, mask [16]byte, scale int) [16]byte


// MmaskI64gatherEpi32: Gather 32-bit integers from memory using 64-bit
// indices. 32-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VPGATHERQD'. Intrinsic: '_mm256_mmask_i64gather_epi32'.
// Requires AVX512F.
func MmaskI64gatherEpi32(src M128i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M128i {
	return M128i(mmaskI64gatherEpi32([16]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherEpi32(src [16]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [16]byte


// I64gatherEpi64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst'. 'scale' should be 1, 2, 4
// or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm256_i64gather_epi64'.
// Requires AVX2.
func I64gatherEpi64(base_addr int, vindex M256i, scale int) M256i {
	return M256i(i64gatherEpi64(base_addr, [32]byte(vindex), scale))
}

func i64gatherEpi64(base_addr int, vindex [32]byte, scale int) [32]byte


// MaskI64gatherEpi64: Gather 64-bit integers from memory using 64-bit indices.
// 64-bit elements are loaded from addresses starting at 'base_addr' and offset
// by each 64-bit element in 'vindex' (each index is scaled by the factor in
// 'scale'). Gathered elements are merged into 'dst' using 'mask' (elements are
// copied from 'src' when the highest bit is not set in the corresponding
// element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm256_mask_i64gather_epi64'.
// Requires AVX2.
func MaskI64gatherEpi64(src M256i, base_addr int, vindex M256i, mask M256i, scale int) M256i {
	return M256i(maskI64gatherEpi64([32]byte(src), base_addr, [32]byte(vindex), [32]byte(mask), scale))
}

func maskI64gatherEpi64(src [32]byte, base_addr int, vindex [32]byte, mask [32]byte, scale int) [32]byte


// MmaskI64gatherEpi64: Gather 64-bit integers from memory using 64-bit
// indices. 64-bit elements are loaded from addresses starting at 'base_addr'
// and offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). Gathered elements are merged into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VPGATHERQQ'. Intrinsic: '_mm256_mmask_i64gather_epi64'.
// Requires AVX512F.
func MmaskI64gatherEpi64(src M256i, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256i {
	return M256i(mmaskI64gatherEpi64([32]byte(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherEpi64(src [32]byte, k uint8, vindex [32]byte, base_addr uintptr, scale int) [32]byte


// I64gatherPd: Gather double-precision (64-bit) floating-point elements from
// memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm256_i64gather_pd'.
// Requires AVX2.
func I64gatherPd(base_addr float64, vindex M256i, scale int) M256d {
	return M256d(i64gatherPd(base_addr, [32]byte(vindex), scale))
}

func i64gatherPd(base_addr float64, vindex [32]byte, scale int) [4]float64


// MaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+63] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		mask[MAX:256] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm256_mask_i64gather_pd'.
// Requires AVX2.
func MaskI64gatherPd(src M256d, base_addr float64, vindex M256i, mask M256d, scale int) M256d {
	return M256d(maskI64gatherPd([4]float64(src), base_addr, [32]byte(vindex), [4]float64(mask), scale))
}

func maskI64gatherPd(src [4]float64, base_addr float64, vindex [32]byte, mask [4]float64, scale int) [4]float64


// MmaskI64gatherPd: Gather double-precision (64-bit) floating-point elements
// from memory using 64-bit indices. 64-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:256] := 0
//
// Instruction: 'VGATHERQPD'. Intrinsic: '_mm256_mmask_i64gather_pd'.
// Requires AVX512F.
func MmaskI64gatherPd(src M256d, k Mmask8, vindex M256i, base_addr uintptr, scale int) M256d {
	return M256d(mmaskI64gatherPd([4]float64(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPd(src [4]float64, k uint8, vindex [32]byte, base_addr uintptr, scale int) [4]float64


// I64gatherPs: Gather single-precision (32-bit) floating-point elements from
// memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst'. 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//		ENDFOR
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm256_i64gather_ps'.
// Requires AVX2.
func I64gatherPs(base_addr float32, vindex M256i, scale int) M128 {
	return M128(i64gatherPs(base_addr, [32]byte(vindex), scale))
}

func i64gatherPs(base_addr float32, vindex [32]byte, scale int) [4]float32


// MaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using 'mask' (elements are copied from 'src' when the highest bit is
// not set in the corresponding element). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF mask[i+31]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				mask[i+31] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		mask[MAX:128] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm256_mask_i64gather_ps'.
// Requires AVX2.
func MaskI64gatherPs(src M128, base_addr float32, vindex M256i, mask M128, scale int) M128 {
	return M128(maskI64gatherPs([4]float32(src), base_addr, [32]byte(vindex), [4]float32(mask), scale))
}

func maskI64gatherPs(src [4]float32, base_addr float32, vindex [32]byte, mask [4]float32, scale int) [4]float32


// MmaskI64gatherPs: Gather single-precision (32-bit) floating-point elements
// from memory using 64-bit indices. 32-bit elements are loaded from addresses
// starting at 'base_addr' and offset by each 64-bit element in 'vindex' (each
// index is scaled by the factor in 'scale'). Gathered elements are merged into
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			m := j*64
//			IF k[j]
//				dst[i+31:i] := MEM[base_addr + SignExtend(vindex[i+63:i])*scale]
//				k[j] := 0
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//		dst[MAX:128] := 0
//
// Instruction: 'VGATHERQPS'. Intrinsic: '_mm256_mmask_i64gather_ps'.
// Requires AVX512F.
func MmaskI64gatherPs(src M128, k Mmask8, vindex M256i, base_addr uintptr, scale int) M128 {
	return M128(mmaskI64gatherPs([4]float32(src), uint8(k), [32]byte(vindex), uintptr(base_addr), scale))
}

func mmaskI64gatherPs(src [4]float32, k uint8, vindex [32]byte, base_addr uintptr, scale int) [4]float32


// I64scatterEpi32: Scatter 32-bit integers from 'a' into memory using 64-bit
// indices. 32-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm256_i64scatter_epi32'.
// Requires AVX512F.
func I64scatterEpi32(base_addr uintptr, vindex M256i, a M128i, scale int)  {
	i64scatterEpi32(uintptr(base_addr), [32]byte(vindex), [16]byte(a), scale)
}

func i64scatterEpi32(base_addr uintptr, vindex [32]byte, a [16]byte, scale int) 


// MaskI64scatterEpi32: Scatter 32-bit integers from 'a' into memory using
// 64-bit indices. 32-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERQD'. Intrinsic: '_mm256_mask_i64scatter_epi32'.
// Requires AVX512F.
func MaskI64scatterEpi32(base_addr uintptr, k Mmask8, vindex M256i, a M128i, scale int)  {
	maskI64scatterEpi32(uintptr(base_addr), uint8(k), [32]byte(vindex), [16]byte(a), scale)
}

func maskI64scatterEpi32(base_addr uintptr, k uint8, vindex [32]byte, a [16]byte, scale int) 


// I64scatterEpi64: Scatter 64-bit integers from 'a' into memory using 64-bit
// indices. 64-bit elements are stored at addresses starting at 'base_addr' and
// offset by each 64-bit element in 'vindex' (each index is scaled by the
// factor in 'scale'). 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm256_i64scatter_epi64'.
// Requires AVX512F.
func I64scatterEpi64(base_addr uintptr, vindex M256i, a M256i, scale int)  {
	i64scatterEpi64(uintptr(base_addr), [32]byte(vindex), [32]byte(a), scale)
}

func i64scatterEpi64(base_addr uintptr, vindex [32]byte, a [32]byte, scale int) 


// MaskI64scatterEpi64: Scatter 64-bit integers from 'a' into memory using
// 64-bit indices. 64-bit elements are stored at addresses starting at
// 'base_addr' and offset by each 64-bit element in 'vindex' (each index is
// scaled by the factor in 'scale') subject to mask 'k' (elements are not
// stored when the corresponding mask bit is not set). 'scale' should be 1, 2,
// 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPSCATTERQQ'. Intrinsic: '_mm256_mask_i64scatter_epi64'.
// Requires AVX512F.
func MaskI64scatterEpi64(base_addr uintptr, k Mmask8, vindex M256i, a M256i, scale int)  {
	maskI64scatterEpi64(uintptr(base_addr), uint8(k), [32]byte(vindex), [32]byte(a), scale)
}

func maskI64scatterEpi64(base_addr uintptr, k uint8, vindex [32]byte, a [32]byte, scale int) 


// I64scatterPd: Scatter double-precision (64-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale'). 'scale' should be
// 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm256_i64scatter_pd'.
// Requires AVX512F.
func I64scatterPd(base_addr uintptr, vindex M256i, a M256d, scale int)  {
	i64scatterPd(uintptr(base_addr), [32]byte(vindex), [4]float64(a), scale)
}

func i64scatterPd(base_addr uintptr, vindex [32]byte, a [4]float64, scale int) 


// MaskI64scatterPd: Scatter double-precision (64-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 64-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[i+63:i])*scale] := a[i+63:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERQPD'. Intrinsic: '_mm256_mask_i64scatter_pd'.
// Requires AVX512F.
func MaskI64scatterPd(base_addr uintptr, k Mmask8, vindex M256i, a M256d, scale int)  {
	maskI64scatterPd(uintptr(base_addr), uint8(k), [32]byte(vindex), [4]float64(a), scale)
}

func maskI64scatterPd(base_addr uintptr, k uint8, vindex [32]byte, a [4]float64, scale int) 


// I64scatterPs: Scatter single-precision (32-bit) floating-point elements from
// 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//		ENDFOR
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm256_i64scatter_ps'.
// Requires AVX512F.
func I64scatterPs(base_addr uintptr, vindex M256i, a M128, scale int)  {
	i64scatterPs(uintptr(base_addr), [32]byte(vindex), [4]float32(a), scale)
}

func i64scatterPs(base_addr uintptr, vindex [32]byte, a [4]float32, scale int) 


// MaskI64scatterPs: Scatter single-precision (32-bit) floating-point elements
// from 'a' into memory using 64-bit indices. 32-bit elements are stored at
// addresses starting at 'base_addr' and offset by each 64-bit element in
// 'vindex' (each index is scaled by the factor in 'scale') subject to mask 'k'
// (elements are not stored when the corresponding mask bit is not set).
// 'scale' should be 1, 2, 4 or 8. 
//
//		FOR j := 0 to 3
//			i := j*32
//			l := j*64
//			IF k[j]
//				MEM[base_addr + SignExtend(vindex[l+63:l])*scale] := a[i+31:i]
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VSCATTERQPS'. Intrinsic: '_mm256_mask_i64scatter_ps'.
// Requires AVX512F.
func MaskI64scatterPs(base_addr uintptr, k Mmask8, vindex M256i, a M128, scale int)  {
	maskI64scatterPs(uintptr(base_addr), uint8(k), [32]byte(vindex), [4]float32(a), scale)
}

func maskI64scatterPs(base_addr uintptr, k uint8, vindex [32]byte, a [4]float32, scale int) 


// IdivEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_idiv_epi32'.
// Requires AVX.
func IdivEpi32(a M256i, b M256i) M256i {
	return M256i(idivEpi32([32]byte(a), [32]byte(b)))
}

func idivEpi32(a [32]byte, b [32]byte) [32]byte


// IdivremEpi32: Divide packed 32-bit integers in 'a' by packed elements in
// 'b', store the truncated results in 'dst', and store the remainders as
// packed 32-bit integers into memory at 'mem_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_idivrem_epi32'.
// Requires AVX.
func IdivremEpi32(mem_addr M256i, a M256i, b M256i) M256i {
	return M256i(idivremEpi32([32]byte(mem_addr), [32]byte(a), [32]byte(b)))
}

func idivremEpi32(mem_addr [32]byte, a [32]byte, b [32]byte) [32]byte


// InsertEpi16: Copy 'a' to 'dst', and insert the 16-bit integer 'i' into 'dst'
// at the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*16
//		dst[sel+15:sel] := i[15:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi16'.
// Requires AVX.
func InsertEpi16(a M256i, i int16, index int) M256i {
	return M256i(insertEpi16([32]byte(a), i, index))
}

func insertEpi16(a [32]byte, i int16, index int) [32]byte


// InsertEpi32: Copy 'a' to 'dst', and insert the 32-bit integer 'i' into 'dst'
// at the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*32
//		dst[sel+31:sel] := i[31:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi32'.
// Requires AVX.
func InsertEpi32(a M256i, i int32, index int) M256i {
	return M256i(insertEpi32([32]byte(a), i, index))
}

func insertEpi32(a [32]byte, i int32, index int) [32]byte


// InsertEpi64: Copy 'a' to 'dst', and insert the 64-bit integer 'i' into 'dst'
// at the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*64
//		dst[sel+63:sel] := i[63:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi64'.
// Requires AVX.
func InsertEpi64(a M256i, i int64, index int) M256i {
	return M256i(insertEpi64([32]byte(a), i, index))
}

func insertEpi64(a [32]byte, i int64, index int) [32]byte


// InsertEpi8: Copy 'a' to 'dst', and insert the 8-bit integer 'i' into 'dst'
// at the location specified by 'index'. 
//
//		dst[255:0] := a[255:0]
//		sel := index*8
//		dst[sel+7:sel] := i[7:0]
//
// Instruction: '...'. Intrinsic: '_mm256_insert_epi8'.
// Requires AVX.
func InsertEpi8(a M256i, i int8, index int) M256i {
	return M256i(insertEpi8([32]byte(a), i, index))
}

func insertEpi8(a [32]byte, i int8, index int) [32]byte


// Insertf128Pd: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// double-precision (64-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_insertf128_pd'.
// Requires AVX.
func Insertf128Pd(a M256d, b M128d, imm8 int) M256d {
	return M256d(insertf128Pd([4]float64(a), [2]float64(b), imm8))
}

func insertf128Pd(a [4]float64, b [2]float64, imm8 int) [4]float64


// Insertf128Ps: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_insertf128_ps'.
// Requires AVX.
func Insertf128Ps(a M256, b M128, imm8 int) M256 {
	return M256(insertf128Ps([8]float32(a), [4]float32(b), imm8))
}

func insertf128Ps(a [8]float32, b [4]float32, imm8 int) [8]float32


// Insertf128Si256: Copy 'a' to 'dst', then insert 128 bits from 'b' into 'dst'
// at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_insertf128_si256'.
// Requires AVX.
func Insertf128Si256(a M256i, b M128i, imm8 int) M256i {
	return M256i(insertf128Si256([32]byte(a), [16]byte(b), imm8))
}

func insertf128Si256(a [32]byte, b [16]byte, imm8 int) [32]byte


// Insertf32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// single-precision (32-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_insertf32x4'.
// Requires AVX512F.
func Insertf32x4(a M256, b M128, imm8 int) M256 {
	return M256(insertf32x4([8]float32(a), [4]float32(b), imm8))
}

func insertf32x4(a [8]float32, b [4]float32, imm8 int) [8]float32


// MaskInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_mask_insertf32x4'.
// Requires AVX512F.
func MaskInsertf32x4(src M256, k Mmask8, a M256, b M128, imm8 int) M256 {
	return M256(maskInsertf32x4([8]float32(src), uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func maskInsertf32x4(src [8]float32, k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// MaskzInsertf32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed single-precision (32-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF32X4'. Intrinsic: '_mm256_maskz_insertf32x4'.
// Requires AVX512F.
func MaskzInsertf32x4(k Mmask8, a M256, b M128, imm8 int) M256 {
	return M256(maskzInsertf32x4(uint8(k), [8]float32(a), [4]float32(b), imm8))
}

func maskzInsertf32x4(k uint8, a [8]float32, b [4]float32, imm8 int) [8]float32


// Insertf64x2: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// double-precision (64-bit) floating-point elements) from 'b' into 'dst' at
// the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm256_insertf64x2'.
// Requires AVX512DQ.
func Insertf64x2(a M256d, b M128d, imm8 int) M256d {
	return M256d(insertf64x2([4]float64(a), [2]float64(b), imm8))
}

func insertf64x2(a [4]float64, b [2]float64, imm8 int) [4]float64


// MaskInsertf64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm256_mask_insertf64x2'.
// Requires AVX512DQ.
func MaskInsertf64x2(src M256d, k Mmask8, a M256d, b M128d, imm8 int) M256d {
	return M256d(maskInsertf64x2([4]float64(src), uint8(k), [4]float64(a), [2]float64(b), imm8))
}

func maskInsertf64x2(src [4]float64, k uint8, a [4]float64, b [2]float64, imm8 int) [4]float64


// MaskzInsertf64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed double-precision (64-bit) floating-point elements) from 'b' into
// 'tmp' at the location specified by 'imm8'.  Store 'tmp' to 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF64X2'. Intrinsic: '_mm256_maskz_insertf64x2'.
// Requires AVX512DQ.
func MaskzInsertf64x2(k Mmask8, a M256d, b M128d, imm8 int) M256d {
	return M256d(maskzInsertf64x2(uint8(k), [4]float64(a), [2]float64(b), imm8))
}

func maskzInsertf64x2(k uint8, a [4]float64, b [2]float64, imm8 int) [4]float64


// Inserti128Si256: Copy 'a' to 'dst', then insert 128 bits (composed of
// integer data) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI128'. Intrinsic: '_mm256_inserti128_si256'.
// Requires AVX2.
func Inserti128Si256(a M256i, b M128i, imm8 int) M256i {
	return M256i(inserti128Si256([32]byte(a), [16]byte(b), imm8))
}

func inserti128Si256(a [32]byte, b [16]byte, imm8 int) [32]byte


// Inserti32x4: Copy 'a' to 'dst', then insert 128 bits (composed of 4 packed
// 32-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_inserti32x4'.
// Requires AVX512F.
func Inserti32x4(a M256i, b M128i, imm8 int) M256i {
	return M256i(inserti32x4([32]byte(a), [16]byte(b), imm8))
}

func inserti32x4(a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_mask_inserti32x4'.
// Requires AVX512F.
func MaskInserti32x4(src M256i, k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskInserti32x4([32]byte(src), uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskInserti32x4(src [32]byte, k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskzInserti32x4: Copy 'a' to 'tmp', then insert 128 bits (composed of 4
// packed 32-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI32X4'. Intrinsic: '_mm256_maskz_inserti32x4'.
// Requires AVX512F.
func MaskzInserti32x4(k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskzInserti32x4(uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskzInserti32x4(k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// Inserti64x2: Copy 'a' to 'dst', then insert 128 bits (composed of 2 packed
// 64-bit integers) from 'b' into 'dst' at the location specified by 'imm8'. 
//
//		dst[255:0] := a[255:0]
//		CASE imm8[7:0] of
//		0: dst[127:0] := b[127:0]
//		1: dst[255:128] := b[127:0]
//		ESAC
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm256_inserti64x2'.
// Requires AVX512DQ.
func Inserti64x2(a M256i, b M128i, imm8 int) M256i {
	return M256i(inserti64x2([32]byte(a), [16]byte(b), imm8))
}

func inserti64x2(a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskInserti64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm256_mask_inserti64x2'.
// Requires AVX512DQ.
func MaskInserti64x2(src M256i, k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskInserti64x2([32]byte(src), uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskInserti64x2(src [32]byte, k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// MaskzInserti64x2: Copy 'a' to 'tmp', then insert 128 bits (composed of 2
// packed 64-bit integers) from 'b' into 'tmp' at the location specified by
// 'imm8'.  Store 'tmp' to 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp[255:0] := a[255:0]
//		CASE (imm8[1:0]) of
//		0: tmp[127:0] := b[127:0]
//		1: tmp[255:128] := b[127:0]
//		ESAC
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTI64X2'. Intrinsic: '_mm256_maskz_inserti64x2'.
// Requires AVX512DQ.
func MaskzInserti64x2(k Mmask8, a M256i, b M128i, imm8 int) M256i {
	return M256i(maskzInserti64x2(uint8(k), [32]byte(a), [16]byte(b), imm8))
}

func maskzInserti64x2(k uint8, a [32]byte, b [16]byte, imm8 int) [32]byte


// InvcbrtPd: Compute the inverse cube root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := InvCubeRoot(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invcbrt_pd'.
// Requires AVX.
func InvcbrtPd(a M256d) M256d {
	return M256d(invcbrtPd([4]float64(a)))
}

func invcbrtPd(a [4]float64) [4]float64


// InvcbrtPs: Compute the inverse cube root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := InvCubeRoot(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invcbrt_ps'.
// Requires AVX.
func InvcbrtPs(a M256) M256 {
	return M256(invcbrtPs([8]float32(a)))
}

func invcbrtPs(a [8]float32) [8]float32


// InvsqrtPd: Compute the inverse square root of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := InvSQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invsqrt_pd'.
// Requires AVX.
func InvsqrtPd(a M256d) M256d {
	return M256d(invsqrtPd([4]float64(a)))
}

func invsqrtPd(a [4]float64) [4]float64


// InvsqrtPs: Compute the inverse square root of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := InvSQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_invsqrt_ps'.
// Requires AVX.
func InvsqrtPs(a M256) M256 {
	return M256(invsqrtPs([8]float32(a)))
}

func invsqrtPs(a [8]float32) [8]float32


// IremEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_irem_epi32'.
// Requires AVX.
func IremEpi32(a M256i, b M256i) M256i {
	return M256i(iremEpi32([32]byte(a), [32]byte(b)))
}

func iremEpi32(a [32]byte, b [32]byte) [32]byte


// LddquSi256: Load 256-bits of integer data from unaligned memory into 'dst'.
// This intrinsic may perform better than '_mm256_loadu_si256' when the data
// crosses a cache line boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VLDDQU'. Intrinsic: '_mm256_lddqu_si256'.
// Requires AVX.
func LddquSi256(mem_addr M256iConst) M256i {
	return M256i(lddquSi256(mem_addr))
}

func lddquSi256(mem_addr M256iConst) [32]byte


// MaskLoadEpi32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_load_epi32'.
// Requires AVX512F.
func MaskLoadEpi32(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoadEpi32([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadEpi32(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoadEpi32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_maskz_load_epi32'.
// Requires AVX512F.
func MaskzLoadEpi32(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoadEpi32(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi32(k uint8, mem_addr uintptr) [32]byte


// MaskLoadEpi64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_load_epi64'.
// Requires AVX512F.
func MaskLoadEpi64(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoadEpi64([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadEpi64(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoadEpi64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_maskz_load_epi64'.
// Requires AVX512F.
func MaskzLoadEpi64(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoadEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzLoadEpi64(k uint8, mem_addr uintptr) [32]byte


// LoadPd: Load 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_load_pd'.
// Requires AVX.
func LoadPd(mem_addr float64) M256d {
	return M256d(loadPd(mem_addr))
}

func loadPd(mem_addr float64) [4]float64


// MaskLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 32-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_load_pd'.
// Requires AVX512F.
func MaskLoadPd(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskLoadPd([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPd(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzLoadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 32-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_maskz_load_pd'.
// Requires AVX512F.
func MaskzLoadPd(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzLoadPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPd(k uint8, mem_addr uintptr) [4]float64


// LoadPs: Load 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_load_ps'.
// Requires AVX.
func LoadPs(mem_addr float32) M256 {
	return M256(loadPs(mem_addr))
}

func loadPs(mem_addr float32) [8]float32


// MaskLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 'mem_addr' must be aligned on a
// 32-byte boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_load_ps'.
// Requires AVX512F.
func MaskLoadPs(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskLoadPs([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoadPs(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzLoadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 'mem_addr' must be aligned on a 32-byte
// boundary or a general-protection exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_maskz_load_ps'.
// Requires AVX512F.
func MaskzLoadPs(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzLoadPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoadPs(k uint8, mem_addr uintptr) [8]float32


// LoadSi256: Load 256-bits of integer data from memory into 'dst'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA'. Intrinsic: '_mm256_load_si256'.
// Requires AVX.
func LoadSi256(mem_addr M256iConst) M256i {
	return M256i(loadSi256(mem_addr))
}

func loadSi256(mem_addr M256iConst) [32]byte


// MaskLoaduEpi16: Load packed 16-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_mask_loadu_epi16'.
// Requires AVX512BW.
func MaskLoaduEpi16(src M256i, k Mmask16, mem_addr uintptr) M256i {
	return M256i(maskLoaduEpi16([32]byte(src), uint16(k), uintptr(mem_addr)))
}

func maskLoaduEpi16(src [32]byte, k uint16, mem_addr uintptr) [32]byte


// MaskzLoaduEpi16: Load packed 16-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := MEM[mem_addr+i+15:mem_addr+i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_maskz_loadu_epi16'.
// Requires AVX512BW.
func MaskzLoaduEpi16(k Mmask16, mem_addr uintptr) M256i {
	return M256i(maskzLoaduEpi16(uint16(k), uintptr(mem_addr)))
}

func maskzLoaduEpi16(k uint16, mem_addr uintptr) [32]byte


// MaskLoaduEpi32: Load packed 32-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_mask_loadu_epi32'.
// Requires AVX512F.
func MaskLoaduEpi32(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoaduEpi32([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi32(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoaduEpi32: Load packed 32-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_maskz_loadu_epi32'.
// Requires AVX512F.
func MaskzLoaduEpi32(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoaduEpi32(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi32(k uint8, mem_addr uintptr) [32]byte


// MaskLoaduEpi64: Load packed 64-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_mask_loadu_epi64'.
// Requires AVX512F.
func MaskLoaduEpi64(src M256i, k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskLoaduEpi64([32]byte(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduEpi64(src [32]byte, k uint8, mem_addr uintptr) [32]byte


// MaskzLoaduEpi64: Load packed 64-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_maskz_loadu_epi64'.
// Requires AVX512F.
func MaskzLoaduEpi64(k Mmask8, mem_addr uintptr) M256i {
	return M256i(maskzLoaduEpi64(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduEpi64(k uint8, mem_addr uintptr) [32]byte


// MaskLoaduEpi8: Load packed 8-bit integers from memory into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_mask_loadu_epi8'.
// Requires AVX512BW.
func MaskLoaduEpi8(src M256i, k Mmask32, mem_addr uintptr) M256i {
	return M256i(maskLoaduEpi8([32]byte(src), uint32(k), uintptr(mem_addr)))
}

func maskLoaduEpi8(src [32]byte, k uint32, mem_addr uintptr) [32]byte


// MaskzLoaduEpi8: Load packed 8-bit integers from memory into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := MEM[mem_addr+i+7:mem_addr+i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_maskz_loadu_epi8'.
// Requires AVX512BW.
func MaskzLoaduEpi8(k Mmask32, mem_addr uintptr) M256i {
	return M256i(maskzLoaduEpi8(uint32(k), uintptr(mem_addr)))
}

func maskzLoaduEpi8(k uint32, mem_addr uintptr) [32]byte


// LoaduPd: Load 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_loadu_pd'.
// Requires AVX.
func LoaduPd(mem_addr float64) M256d {
	return M256d(loaduPd(mem_addr))
}

func loaduPd(mem_addr float64) [4]float64


// MaskLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_mask_loadu_pd'.
// Requires AVX512F.
func MaskLoaduPd(src M256d, k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskLoaduPd([4]float64(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPd(src [4]float64, k uint8, mem_addr uintptr) [4]float64


// MaskzLoaduPd: Load packed double-precision (64-bit) floating-point elements
// from memoy into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_maskz_loadu_pd'.
// Requires AVX512F.
func MaskzLoaduPd(k Mmask8, mem_addr uintptr) M256d {
	return M256d(maskzLoaduPd(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPd(k uint8, mem_addr uintptr) [4]float64


// LoaduPs: Load 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_loadu_ps'.
// Requires AVX.
func LoaduPs(mem_addr float32) M256 {
	return M256(loaduPs(mem_addr))
}

func loaduPs(mem_addr float32) [8]float32


// MaskLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_mask_loadu_ps'.
// Requires AVX512F.
func MaskLoaduPs(src M256, k Mmask8, mem_addr uintptr) M256 {
	return M256(maskLoaduPs([8]float32(src), uint8(k), uintptr(mem_addr)))
}

func maskLoaduPs(src [8]float32, k uint8, mem_addr uintptr) [8]float32


// MaskzLoaduPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set).
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_maskz_loadu_ps'.
// Requires AVX512F.
func MaskzLoaduPs(k Mmask8, mem_addr uintptr) M256 {
	return M256(maskzLoaduPs(uint8(k), uintptr(mem_addr)))
}

func maskzLoaduPs(k uint8, mem_addr uintptr) [8]float32


// LoaduSi256: Load 256-bits of integer data from memory into 'dst'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU'. Intrinsic: '_mm256_loadu_si256'.
// Requires AVX.
func LoaduSi256(mem_addr M256iConst) M256i {
	return M256i(loaduSi256(mem_addr))
}

func loaduSi256(mem_addr M256iConst) [32]byte


// Loadu2M128: Load two 128-bit values (composed of 4 packed single-precision
// (32-bit) floating-point elements) from memory, and combine them into a
// 256-bit value in 'dst'.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[loaddr+127:loaddr]
//		dst[255:128] := MEM[hiaddr+127:hiaddr]
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_loadu2_m128'.
// Requires AVX.
func Loadu2M128(hiaddr float32, loaddr float32) M256 {
	return M256(loadu2M128(hiaddr, loaddr))
}

func loadu2M128(hiaddr float32, loaddr float32) [8]float32


// Loadu2M128d: Load two 128-bit values (composed of 2 packed double-precision
// (64-bit) floating-point elements) from memory, and combine them into a
// 256-bit value in 'dst'.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[loaddr+127:loaddr]
//		dst[255:128] := MEM[hiaddr+127:hiaddr]
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_loadu2_m128d'.
// Requires AVX.
func Loadu2M128d(hiaddr float64, loaddr float64) M256d {
	return M256d(loadu2M128d(hiaddr, loaddr))
}

func loadu2M128d(hiaddr float64, loaddr float64) [4]float64


// Loadu2M128i: Load two 128-bit values (composed of integer data) from memory,
// and combine them into a 256-bit value in 'dst'.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		dst[127:0] := MEM[loaddr+127:loaddr]
//		dst[255:128] := MEM[hiaddr+127:hiaddr]
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_loadu2_m128i'.
// Requires AVX.
func Loadu2M128i(hiaddr M128iConst, loaddr M128iConst) M256i {
	return M256i(loadu2M128i(hiaddr, loaddr))
}

func loadu2M128i(hiaddr M128iConst, loaddr M128iConst) [32]byte


// LogPd: Compute the natural logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ln(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log_pd'.
// Requires AVX.
func LogPd(a M256d) M256d {
	return M256d(logPd([4]float64(a)))
}

func logPd(a [4]float64) [4]float64


// LogPs: Compute the natural logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ln(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log_ps'.
// Requires AVX.
func LogPs(a M256) M256 {
	return M256(logPs([8]float32(a)))
}

func logPs(a [8]float32) [8]float32


// Log10Pd: Compute the base-10 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := log10(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log10_pd'.
// Requires AVX.
func Log10Pd(a M256d) M256d {
	return M256d(log10Pd([4]float64(a)))
}

func log10Pd(a [4]float64) [4]float64


// Log10Ps: Compute the base-10 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := log10(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log10_ps'.
// Requires AVX.
func Log10Ps(a M256) M256 {
	return M256(log10Ps([8]float32(a)))
}

func log10Ps(a [8]float32) [8]float32


// Log1pPd: Compute the natural logarithm of one plus packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ln(1.0 + a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log1p_pd'.
// Requires AVX.
func Log1pPd(a M256d) M256d {
	return M256d(log1pPd([4]float64(a)))
}

func log1pPd(a [4]float64) [4]float64


// Log1pPs: Compute the natural logarithm of one plus packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ln(1.0 + a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log1p_ps'.
// Requires AVX.
func Log1pPs(a M256) M256 {
	return M256(log1pPs([8]float32(a)))
}

func log1pPs(a [8]float32) [8]float32


// Log2Pd: Compute the base-2 logarithm of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := log2(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log2_pd'.
// Requires AVX.
func Log2Pd(a M256d) M256d {
	return M256d(log2Pd([4]float64(a)))
}

func log2Pd(a [4]float64) [4]float64


// Log2Ps: Compute the base-2 logarithm of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := log2(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_log2_ps'.
// Requires AVX.
func Log2Ps(a M256) M256 {
	return M256(log2Ps([8]float32(a)))
}

func log2Ps(a [8]float32) [8]float32


// LogbPd: Convert the exponent of each packed double-precision (64-bit)
// floating-point element in 'a' to a double-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ConvertExpFP64(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_logb_pd'.
// Requires AVX.
func LogbPd(a M256d) M256d {
	return M256d(logbPd([4]float64(a)))
}

func logbPd(a [4]float64) [4]float64


// LogbPs: Convert the exponent of each packed single-precision (32-bit)
// floating-point element in 'a' to a single-precision floating-point number
// representing the integer exponent, and store the results in 'dst'. This
// intrinsic essentially calculates 'floor(log2(x))' for each element. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ConvertExpFP32(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_logb_ps'.
// Requires AVX.
func LogbPs(a M256) M256 {
	return M256(logbPs([8]float32(a)))
}

func logbPs(a [8]float32) [8]float32


// LzcntEpi32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			tmp := 31
//			dst[i+31:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+31:i] := dst[i+31:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm256_lzcnt_epi32'.
// Requires AVX512CD.
func LzcntEpi32(a M256i) M256i {
	return M256i(lzcntEpi32([32]byte(a)))
}

func lzcntEpi32(a [32]byte) [32]byte


// MaskLzcntEpi32: Counts the number of leading zero bits in each packed 32-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm256_mask_lzcnt_epi32'.
// Requires AVX512CD.
func MaskLzcntEpi32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskLzcntEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskLzcntEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzLzcntEpi32: Counts the number of leading zero bits in each packed
// 32-bit integer in 'a', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp := 31
//				dst[i+31:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+31:i] := dst[i+31:i] + 1
//				OD
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTD'. Intrinsic: '_mm256_maskz_lzcnt_epi32'.
// Requires AVX512CD.
func MaskzLzcntEpi32(k Mmask8, a M256i) M256i {
	return M256i(maskzLzcntEpi32(uint8(k), [32]byte(a)))
}

func maskzLzcntEpi32(k uint8, a [32]byte) [32]byte


// LzcntEpi64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp := 63
//			dst[i+63:i] := 0
//			DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//				tmp := tmp - 1
//				dst[i+63:i] := dst[i+63:i] + 1
//			OD
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm256_lzcnt_epi64'.
// Requires AVX512CD.
func LzcntEpi64(a M256i) M256i {
	return M256i(lzcntEpi64([32]byte(a)))
}

func lzcntEpi64(a [32]byte) [32]byte


// MaskLzcntEpi64: Counts the number of leading zero bits in each packed 64-bit
// integer in 'a', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm256_mask_lzcnt_epi64'.
// Requires AVX512CD.
func MaskLzcntEpi64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskLzcntEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskLzcntEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzLzcntEpi64: Counts the number of leading zero bits in each packed
// 64-bit integer in 'a', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp := 63
//				dst[i+63:i] := 0
//				DO WHILE (tmp >= 0 AND a[i+tmp] == 0)
//					tmp := tmp - 1
//					dst[i+63:i] := dst[i+63:i] + 1
//				OD
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPLZCNTQ'. Intrinsic: '_mm256_maskz_lzcnt_epi64'.
// Requires AVX512CD.
func MaskzLzcntEpi64(k Mmask8, a M256i) M256i {
	return M256i(maskzLzcntEpi64(uint8(k), [32]byte(a)))
}

func maskzLzcntEpi64(k uint8, a [32]byte) [32]byte


// MaddEpi16: Multiply packed signed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			st[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm256_madd_epi16'.
// Requires AVX2.
func MaddEpi16(a M256i, b M256i) M256i {
	return M256i(maddEpi16([32]byte(a), [32]byte(b)))
}

func maddEpi16(a [32]byte, b [32]byte) [32]byte


// MaskMaddEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm256_mask_madd_epi16'.
// Requires AVX512BW.
func MaskMaddEpi16(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaddEpi16([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaddEpi16(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaddEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers. Horizontally add adjacent pairs of
// intermediate 32-bit integers, and pack the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i+16]*b[i+31:i+16] + a[i+15:i]*b[i+15:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDWD'. Intrinsic: '_mm256_maskz_madd_epi16'.
// Requires AVX512BW.
func MaskzMaddEpi16(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaddEpi16(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaddEpi16(k uint8, a [32]byte, b [32]byte) [32]byte


// Madd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm256_madd52hi_epu64'.
// Requires AVX512VL.
func Madd52hiEpu64(a M256i, b M256i, c M256i) M256i {
	return M256i(madd52hiEpu64([32]byte(a), [32]byte(b), [32]byte(c)))
}

func madd52hiEpu64(a [32]byte, b [32]byte, c [32]byte) [32]byte


// MaskMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm256_mask_madd52hi_epu64'.
// Requires AVX512VL.
func MaskMadd52hiEpu64(a M256i, k Mmask8, b M256i, c M256i) M256i {
	return M256i(maskMadd52hiEpu64([32]byte(a), uint8(k), [32]byte(b), [32]byte(c)))
}

func maskMadd52hiEpu64(a [32]byte, k uint8, b [32]byte, c [32]byte) [32]byte


// MaskzMadd52hiEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the high
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[103:52])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52HUQ'. Intrinsic: '_mm256_maskz_madd52hi_epu64'.
// Requires AVX512VL.
func MaskzMadd52hiEpu64(k Mmask8, a M256i, b M256i, c M256i) M256i {
	return M256i(maskzMadd52hiEpu64(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c)))
}

func maskzMadd52hiEpu64(k uint8, a [32]byte, b [32]byte, c [32]byte) [32]byte


// Madd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//			dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm256_madd52lo_epu64'.
// Requires AVX512VL.
func Madd52loEpu64(a M256i, b M256i, c M256i) M256i {
	return M256i(madd52loEpu64([32]byte(a), [32]byte(b), [32]byte(c)))
}

func madd52loEpu64(a [32]byte, b [32]byte, c [32]byte) [32]byte


// MaskMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'a' when the corresponding mask bit
// is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm256_mask_madd52lo_epu64'.
// Requires AVX512VL.
func MaskMadd52loEpu64(a M256i, k Mmask8, b M256i, c M256i) M256i {
	return M256i(maskMadd52loEpu64([32]byte(a), uint8(k), [32]byte(b), [32]byte(c)))
}

func maskMadd52loEpu64(a [32]byte, k uint8, b [32]byte, c [32]byte) [32]byte


// MaskzMadd52loEpu64: Multiply packed unsigned 52-bit integers in each 64-bit
// element of 'b' and 'c' to form a 104-bit intermediate result. Add the low
// 52-bit unsigned integer from the intermediate result with the corresponding
// unsigned 64-bit integer in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := ZeroExtend64(b[i+51:i]) * ZeroExtend64(c[i+51:i])
//				dst[i+63:i] := a[i+63:i] + ZeroExtend64(tmp[51:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADD52LUQ'. Intrinsic: '_mm256_maskz_madd52lo_epu64'.
// Requires AVX512VL.
func MaskzMadd52loEpu64(k Mmask8, a M256i, b M256i, c M256i) M256i {
	return M256i(maskzMadd52loEpu64(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c)))
}

func maskzMadd52loEpu64(k uint8, a [32]byte, b [32]byte, c [32]byte) [32]byte


// MaddubsEpi16: Vertically multiply each unsigned 8-bit integer from 'a' with
// the corresponding signed 8-bit integer from 'b', producing intermediate
// signed 16-bit integers. Horizontally add adjacent pairs of intermediate
// signed 16-bit integers, and pack the saturated results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm256_maddubs_epi16'.
// Requires AVX2.
func MaddubsEpi16(a M256i, b M256i) M256i {
	return M256i(maddubsEpi16([32]byte(a), [32]byte(b)))
}

func maddubsEpi16(a [32]byte, b [32]byte) [32]byte


// MaskMaddubsEpi16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm256_mask_maddubs_epi16'.
// Requires AVX512BW.
func MaskMaddubsEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMaddubsEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMaddubsEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMaddubsEpi16: Multiply packed unsigned 8-bit integers in 'a' by packed
// signed 8-bit integers in 'b', producing intermediate signed 16-bit integers.
// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
// pack the saturated results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16( a[i+15:i+8]*b[i+15:i+8] + a[i+7:i]*b[i+7:i] )
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMADDUBSW'. Intrinsic: '_mm256_maskz_maddubs_epi16'.
// Requires AVX512BW.
func MaskzMaddubsEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMaddubsEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMaddubsEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// MaskloadEpi32: Load packed 32-bit integers from memory into 'dst' using
// 'mask' (elements are zeroed out when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMASKMOVD'. Intrinsic: '_mm256_maskload_epi32'.
// Requires AVX2.
func MaskloadEpi32(mem_addr int, mask M256i) M256i {
	return M256i(maskloadEpi32(mem_addr, [32]byte(mask)))
}

func maskloadEpi32(mem_addr int, mask [32]byte) [32]byte


// MaskloadEpi64: Load packed 64-bit integers from memory into 'dst' using
// 'mask' (elements are zeroed out when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMASKMOVQ'. Intrinsic: '_mm256_maskload_epi64'.
// Requires AVX2.
func MaskloadEpi64(mem_addr int, mask M256i) M256i {
	return M256i(maskloadEpi64(mem_addr, [32]byte(mask)))
}

func maskloadEpi64(mem_addr int, mask [32]byte) [32]byte


// MaskloadPd: Load packed double-precision (64-bit) floating-point elements
// from memory into 'dst' using 'mask' (elements are zeroed out when the high
// bit of the corresponding element is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				dst[i+63:i] := MEM[mem_addr+i+63:mem_addr+i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMASKMOVPD'. Intrinsic: '_mm256_maskload_pd'.
// Requires AVX.
func MaskloadPd(mem_addr float64, mask M256i) M256d {
	return M256d(maskloadPd(mem_addr, [32]byte(mask)))
}

func maskloadPd(mem_addr float64, mask [32]byte) [4]float64


// MaskloadPs: Load packed single-precision (32-bit) floating-point elements
// from memory into 'dst' using 'mask' (elements are zeroed out when the high
// bit of the corresponding element is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				dst[i+31:i] := MEM[mem_addr+i+31:mem_addr+i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMASKMOVPS'. Intrinsic: '_mm256_maskload_ps'.
// Requires AVX.
func MaskloadPs(mem_addr float32, mask M256i) M256 {
	return M256(maskloadPs(mem_addr, [32]byte(mask)))
}

func maskloadPs(mem_addr float32, mask [32]byte) [8]float32


// MaskstoreEpi32: Store packed 32-bit integers from 'a' into memory using
// 'mask' (elements are not stored when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VPMASKMOVD'. Intrinsic: '_mm256_maskstore_epi32'.
// Requires AVX2.
func MaskstoreEpi32(mem_addr int, mask M256i, a M256i)  {
	maskstoreEpi32(mem_addr, [32]byte(mask), [32]byte(a))
}

func maskstoreEpi32(mem_addr int, mask [32]byte, a [32]byte) 


// MaskstoreEpi64: Store packed 64-bit integers from 'a' into memory using
// 'mask' (elements are not stored when the highest bit is not set in the
// corresponding element). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VPMASKMOVQ'. Intrinsic: '_mm256_maskstore_epi64'.
// Requires AVX2.
func MaskstoreEpi64(mem_addr int64, mask M256i, a M256i)  {
	maskstoreEpi64(mem_addr, [32]byte(mask), [32]byte(a))
}

func maskstoreEpi64(mem_addr int64, mask [32]byte, a [32]byte) 


// MaskstorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using 'mask'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF mask[i+63]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMASKMOVPD'. Intrinsic: '_mm256_maskstore_pd'.
// Requires AVX.
func MaskstorePd(mem_addr float64, mask M256i, a M256d)  {
	maskstorePd(mem_addr, [32]byte(mask), [4]float64(a))
}

func maskstorePd(mem_addr float64, mask [32]byte, a [4]float64) 


// MaskstorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using 'mask'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF mask[i+31]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMASKMOVPS'. Intrinsic: '_mm256_maskstore_ps'.
// Requires AVX.
func MaskstorePs(mem_addr float32, mask M256i, a M256)  {
	maskstorePs(mem_addr, [32]byte(mask), [8]float32(a))
}

func maskstorePs(mem_addr float32, mask [32]byte, a [8]float32) 


// MaskMaxEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm256_mask_max_epi16'.
// Requires AVX512BW.
func MaskMaxEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMaxEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm256_maskz_max_epi16'.
// Requires AVX512BW.
func MaskzMaxEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// MaxEpi16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSW'. Intrinsic: '_mm256_max_epi16'.
// Requires AVX2.
func MaxEpi16(a M256i, b M256i) M256i {
	return M256i(maxEpi16([32]byte(a), [32]byte(b)))
}

func maxEpi16(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_mask_max_epi32'.
// Requires AVX512F.
func MaskMaxEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_maskz_max_epi32'.
// Requires AVX512F.
func MaskzMaxEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpi32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSD'. Intrinsic: '_mm256_max_epi32'.
// Requires AVX2.
func MaxEpi32(a M256i, b M256i) M256i {
	return M256i(maxEpi32([32]byte(a), [32]byte(b)))
}

func maxEpi32(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_mask_max_epi64'.
// Requires AVX512F.
func MaskMaxEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_maskz_max_epi64'.
// Requires AVX512F.
func MaskzMaxEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSQ'. Intrinsic: '_mm256_max_epi64'.
// Requires AVX512F.
func MaxEpi64(a M256i, b M256i) M256i {
	return M256i(maxEpi64([32]byte(a), [32]byte(b)))
}

func maxEpi64(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm256_mask_max_epi8'.
// Requires AVX512BW.
func MaskMaxEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMaxEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm256_maskz_max_epi8'.
// Requires AVX512BW.
func MaskzMaxEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// MaxEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// maximum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXSB'. Intrinsic: '_mm256_max_epi8'.
// Requires AVX2.
func MaxEpi8(a M256i, b M256i) M256i {
	return M256i(maxEpi8([32]byte(a), [32]byte(b)))
}

func maxEpi8(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm256_mask_max_epu16'.
// Requires AVX512BW.
func MaskMaxEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] > b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm256_maskz_max_epu16'.
// Requires AVX512BW.
func MaskzMaxEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// MaxEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] > b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUW'. Intrinsic: '_mm256_max_epu16'.
// Requires AVX2.
func MaxEpu16(a M256i, b M256i) M256i {
	return M256i(maxEpu16([32]byte(a), [32]byte(b)))
}

func maxEpu16(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_mask_max_epu32'.
// Requires AVX512F.
func MaskMaxEpu32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] > b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_maskz_max_epu32'.
// Requires AVX512F.
func MaskzMaxEpu32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] > b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUD'. Intrinsic: '_mm256_max_epu32'.
// Requires AVX2.
func MaxEpu32(a M256i, b M256i) M256i {
	return M256i(maxEpu32([32]byte(a), [32]byte(b)))
}

func maxEpu32(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_mask_max_epu64'.
// Requires AVX512F.
func MaskMaxEpu64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] > b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_maskz_max_epu64'.
// Requires AVX512F.
func MaskzMaxEpu64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaxEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] > b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUQ'. Intrinsic: '_mm256_max_epu64'.
// Requires AVX512F.
func MaxEpu64(a M256i, b M256i) M256i {
	return M256i(maxEpu64([32]byte(a), [32]byte(b)))
}

func maxEpu64(a [32]byte, b [32]byte) [32]byte


// MaskMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm256_mask_max_epu8'.
// Requires AVX512BW.
func MaskMaxEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMaxEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMaxEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed maximum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] > b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm256_maskz_max_epu8'.
// Requires AVX512BW.
func MaskzMaxEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMaxEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMaxEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// MaxEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed maximum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] > b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMAXUB'. Intrinsic: '_mm256_max_epu8'.
// Requires AVX2.
func MaxEpu8(a M256i, b M256i) M256i {
	return M256i(maxEpu8([32]byte(a), [32]byte(b)))
}

func maxEpu8(a [32]byte, b [32]byte) [32]byte


// MaskMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_mask_max_pd'.
// Requires AVX512F.
func MaskMaxPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMaxPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMaxPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMaxPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_maskz_max_pd'.
// Requires AVX512F.
func MaskzMaxPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMaxPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMaxPd(k uint8, a [4]float64, b [4]float64) [4]float64


// MaxPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MAX(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPD'. Intrinsic: '_mm256_max_pd'.
// Requires AVX.
func MaxPd(a M256d, b M256d) M256d {
	return M256d(maxPd([4]float64(a), [4]float64(b)))
}

func maxPd(a [4]float64, b [4]float64) [4]float64


// MaskMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_mask_max_ps'.
// Requires AVX512F.
func MaskMaxPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMaxPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMaxPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMaxPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed maximum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_maskz_max_ps'.
// Requires AVX512F.
func MaskzMaxPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMaxPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMaxPs(k uint8, a [8]float32, b [8]float32) [8]float32


// MaxPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed maximum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MAX(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMAXPS'. Intrinsic: '_mm256_max_ps'.
// Requires AVX.
func MaxPs(a M256, b M256) M256 {
	return M256(maxPs([8]float32(a), [8]float32(b)))
}

func maxPs(a [8]float32, b [8]float32) [8]float32


// MaskMinEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm256_mask_min_epi16'.
// Requires AVX512BW.
func MaskMinEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMinEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpi16: Compare packed 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm256_maskz_min_epi16'.
// Requires AVX512BW.
func MaskzMinEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMinEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// MinEpi16: Compare packed 16-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSW'. Intrinsic: '_mm256_min_epi16'.
// Requires AVX2.
func MinEpi16(a M256i, b M256i) M256i {
	return M256i(minEpi16([32]byte(a), [32]byte(b)))
}

func minEpi16(a [32]byte, b [32]byte) [32]byte


// MaskMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_mask_min_epi32'.
// Requires AVX512F.
func MaskMinEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_maskz_min_epi32'.
// Requires AVX512F.
func MaskzMinEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpi32: Compare packed 32-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSD'. Intrinsic: '_mm256_min_epi32'.
// Requires AVX2.
func MinEpi32(a M256i, b M256i) M256i {
	return M256i(minEpi32([32]byte(a), [32]byte(b)))
}

func minEpi32(a [32]byte, b [32]byte) [32]byte


// MaskMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_mask_min_epi64'.
// Requires AVX512F.
func MaskMinEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_maskz_min_epi64'.
// Requires AVX512F.
func MaskzMinEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpi64: Compare packed 64-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSQ'. Intrinsic: '_mm256_min_epi64'.
// Requires AVX512F.
func MinEpi64(a M256i, b M256i) M256i {
	return M256i(minEpi64([32]byte(a), [32]byte(b)))
}

func minEpi64(a [32]byte, b [32]byte) [32]byte


// MaskMinEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using writemask 'k' (elements are copied from 'src'
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm256_mask_min_epi8'.
// Requires AVX512BW.
func MaskMinEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMinEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm256_maskz_min_epi8'.
// Requires AVX512BW.
func MaskzMinEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMinEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// MinEpi8: Compare packed 8-bit integers in 'a' and 'b', and store packed
// minimum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINSB'. Intrinsic: '_mm256_min_epi8'.
// Requires AVX2.
func MinEpi8(a M256i, b M256i) M256i {
	return M256i(minEpi8([32]byte(a), [32]byte(b)))
}

func minEpi8(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm256_mask_min_epu16'.
// Requires AVX512BW.
func MaskMinEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMinEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF a[i+15:i] < b[i+15:i]
//					dst[i+15:i] := a[i+15:i]
//				ELSE
//					dst[i+15:i] := b[i+15:i]
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm256_maskz_min_epu16'.
// Requires AVX512BW.
func MaskzMinEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// MinEpu16: Compare packed unsigned 16-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15:i] < b[i+15:i]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := b[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUW'. Intrinsic: '_mm256_min_epu16'.
// Requires AVX2.
func MinEpu16(a M256i, b M256i) M256i {
	return M256i(minEpu16([32]byte(a), [32]byte(b)))
}

func minEpu16(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_mask_min_epu32'.
// Requires AVX512F.
func MaskMinEpu32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF a[i+31:i] < b[i+31:i]
//					dst[i+31:i] := a[i+31:i]
//				ELSE
//					dst[i+31:i] := b[i+31:i]
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_maskz_min_epu32'.
// Requires AVX512F.
func MaskzMinEpu32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpu32: Compare packed unsigned 32-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31:i] < b[i+31:i]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := b[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUD'. Intrinsic: '_mm256_min_epu32'.
// Requires AVX2.
func MinEpu32(a M256i, b M256i) M256i {
	return M256i(minEpu32([32]byte(a), [32]byte(b)))
}

func minEpu32(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_mask_min_epu64'.
// Requires AVX512F.
func MaskMinEpu64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMinEpu64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF a[i+63:i] < b[i+63:i]
//					dst[i+63:i] := a[i+63:i]
//				ELSE
//					dst[i+63:i] := b[i+63:i]
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_maskz_min_epu64'.
// Requires AVX512F.
func MaskzMinEpu64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu64(k uint8, a [32]byte, b [32]byte) [32]byte


// MinEpu64: Compare packed unsigned 64-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63:i] < b[i+63:i]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := b[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUQ'. Intrinsic: '_mm256_min_epu64'.
// Requires AVX512F.
func MinEpu64(a M256i, b M256i) M256i {
	return M256i(minEpu64([32]byte(a), [32]byte(b)))
}

func minEpu64(a [32]byte, b [32]byte) [32]byte


// MaskMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm256_mask_min_epu8'.
// Requires AVX512BW.
func MaskMinEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMinEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMinEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and
// store packed minimum values in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF a[i+7:i] < b[i+7:i]
//					dst[i+7:i] := a[i+7:i]
//				ELSE
//					dst[i+7:i] := b[i+7:i]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm256_maskz_min_epu8'.
// Requires AVX512BW.
func MaskzMinEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMinEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMinEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// MinEpu8: Compare packed unsigned 8-bit integers in 'a' and 'b', and store
// packed minimum values in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7:i] < b[i+7:i]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := b[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMINUB'. Intrinsic: '_mm256_min_epu8'.
// Requires AVX2.
func MinEpu8(a M256i, b M256i) M256i {
	return M256i(minEpu8([32]byte(a), [32]byte(b)))
}

func minEpu8(a [32]byte, b [32]byte) [32]byte


// MaskMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_mask_min_pd'.
// Requires AVX512F.
func MaskMinPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMinPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMinPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMinPd: Compare packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_maskz_min_pd'.
// Requires AVX512F.
func MaskzMinPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMinPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMinPd(k uint8, a [4]float64, b [4]float64) [4]float64


// MinPd: Compare packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := MIN(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPD'. Intrinsic: '_mm256_min_pd'.
// Requires AVX.
func MinPd(a M256d, b M256d) M256d {
	return M256d(minPd([4]float64(a), [4]float64(b)))
}

func minPd(a [4]float64, b [4]float64) [4]float64


// MaskMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_mask_min_ps'.
// Requires AVX512F.
func MaskMinPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMinPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMinPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMinPs: Compare packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store packed minimum values in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_maskz_min_ps'.
// Requires AVX512F.
func MaskzMinPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMinPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMinPs(k uint8, a [8]float32, b [8]float32) [8]float32


// MinPs: Compare packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store packed minimum values in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := MIN(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMINPS'. Intrinsic: '_mm256_min_ps'.
// Requires AVX.
func MinPs(a M256, b M256) M256 {
	return M256(minPs([8]float32(a), [8]float32(b)))
}

func minPs(a [8]float32, b [8]float32) [8]float32


// MaskMovEpi16: Move packed 16-bit integers from 'a' into 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_mask_mov_epi16'.
// Requires AVX512BW.
func MaskMovEpi16(src M256i, k Mmask16, a M256i) M256i {
	return M256i(maskMovEpi16([32]byte(src), uint16(k), [32]byte(a)))
}

func maskMovEpi16(src [32]byte, k uint16, a [32]byte) [32]byte


// MaskzMovEpi16: Move packed 16-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_maskz_mov_epi16'.
// Requires AVX512BW.
func MaskzMovEpi16(k Mmask16, a M256i) M256i {
	return M256i(maskzMovEpi16(uint16(k), [32]byte(a)))
}

func maskzMovEpi16(k uint16, a [32]byte) [32]byte


// MaskMovEpi32: Move packed 32-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_mov_epi32'.
// Requires AVX512F.
func MaskMovEpi32(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskMovEpi32([32]byte(src), uint8(k), [32]byte(a)))
}

func maskMovEpi32(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzMovEpi32: Move packed 32-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_maskz_mov_epi32'.
// Requires AVX512F.
func MaskzMovEpi32(k Mmask8, a M256i) M256i {
	return M256i(maskzMovEpi32(uint8(k), [32]byte(a)))
}

func maskzMovEpi32(k uint8, a [32]byte) [32]byte


// MaskMovEpi64: Move packed 64-bit integers from 'a' to 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_mov_epi64'.
// Requires AVX512F.
func MaskMovEpi64(src M256i, k Mmask8, a M256i) M256i {
	return M256i(maskMovEpi64([32]byte(src), uint8(k), [32]byte(a)))
}

func maskMovEpi64(src [32]byte, k uint8, a [32]byte) [32]byte


// MaskzMovEpi64: Move packed 64-bit integers from 'a' into 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_maskz_mov_epi64'.
// Requires AVX512F.
func MaskzMovEpi64(k Mmask8, a M256i) M256i {
	return M256i(maskzMovEpi64(uint8(k), [32]byte(a)))
}

func maskzMovEpi64(k uint8, a [32]byte) [32]byte


// MaskMovEpi8: Move packed 8-bit integers from 'a' into 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_mask_mov_epi8'.
// Requires AVX512BW.
func MaskMovEpi8(src M256i, k Mmask32, a M256i) M256i {
	return M256i(maskMovEpi8([32]byte(src), uint32(k), [32]byte(a)))
}

func maskMovEpi8(src [32]byte, k uint32, a [32]byte) [32]byte


// MaskzMovEpi8: Move packed 8-bit integers from 'a' into 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_maskz_mov_epi8'.
// Requires AVX512BW.
func MaskzMovEpi8(k Mmask32, a M256i) M256i {
	return M256i(maskzMovEpi8(uint32(k), [32]byte(a)))
}

func maskzMovEpi8(k uint32, a [32]byte) [32]byte


// MaskMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_mov_pd'.
// Requires AVX512F.
func MaskMovPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskMovPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskMovPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzMovPd: Move packed double-precision (64-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_maskz_mov_pd'.
// Requires AVX512F.
func MaskzMovPd(k Mmask8, a M256d) M256d {
	return M256d(maskzMovPd(uint8(k), [4]float64(a)))
}

func maskzMovPd(k uint8, a [4]float64) [4]float64


// MaskMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' to 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_mov_ps'.
// Requires AVX512F.
func MaskMovPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskMovPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMovPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMovPs: Move packed single-precision (32-bit) floating-point elements
// from 'a' into 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_maskz_mov_ps'.
// Requires AVX512F.
func MaskzMovPs(k Mmask8, a M256) M256 {
	return M256(maskzMovPs(uint8(k), [8]float32(a)))
}

func maskzMovPs(k uint8, a [8]float32) [8]float32


// MaskMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_mask_movedup_pd'.
// Requires AVX512F.
func MaskMovedupPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskMovedupPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskMovedupPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzMovedupPd: Duplicate even-indexed double-precision (64-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[63:0] := a[63:0]
//		tmp[127:64] := a[63:0]
//		tmp[191:128] := a[191:128]
//		tmp[255:192] := a[191:128]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_maskz_movedup_pd'.
// Requires AVX512F.
func MaskzMovedupPd(k Mmask8, a M256d) M256d {
	return M256d(maskzMovedupPd(uint8(k), [4]float64(a)))
}

func maskzMovedupPd(k uint8, a [4]float64) [4]float64


// MovedupPd: Duplicate even-indexed double-precision (64-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[63:0] := a[63:0]
//		dst[127:64] := a[63:0]
//		dst[191:128] := a[191:128]
//		dst[255:192] := a[191:128]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDDUP'. Intrinsic: '_mm256_movedup_pd'.
// Requires AVX.
func MovedupPd(a M256d) M256d {
	return M256d(movedupPd([4]float64(a)))
}

func movedupPd(a [4]float64) [4]float64


// MaskMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_mask_movehdup_ps'.
// Requires AVX512F.
func MaskMovehdupPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskMovehdupPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMovehdupPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMovehdupPs: Duplicate odd-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[63:32] 
//		tmp[63:32] := a[63:32] 
//		tmp[95:64] := a[127:96] 
//		tmp[127:96] := a[127:96]
//		tmp[159:128] := a[191:160] 
//		tmp[191:160] := a[191:160] 
//		tmp[223:192] := a[255:224] 
//		tmp[255:224] := a[255:224]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_maskz_movehdup_ps'.
// Requires AVX512F.
func MaskzMovehdupPs(k Mmask8, a M256) M256 {
	return M256(maskzMovehdupPs(uint8(k), [8]float32(a)))
}

func maskzMovehdupPs(k uint8, a [8]float32) [8]float32


// MovehdupPs: Duplicate odd-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[63:32] 
//		dst[63:32] := a[63:32] 
//		dst[95:64] := a[127:96] 
//		dst[127:96] := a[127:96]
//		dst[159:128] := a[191:160] 
//		dst[191:160] := a[191:160] 
//		dst[223:192] := a[255:224] 
//		dst[255:224] := a[255:224]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSHDUP'. Intrinsic: '_mm256_movehdup_ps'.
// Requires AVX.
func MovehdupPs(a M256) M256 {
	return M256(movehdupPs([8]float32(a)))
}

func movehdupPs(a [8]float32) [8]float32


// MaskMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR	
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_mask_moveldup_ps'.
// Requires AVX512F.
func MaskMoveldupPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskMoveldupPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskMoveldupPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzMoveldupPs: Duplicate even-indexed single-precision (32-bit)
// floating-point elements from 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp[31:0] := a[31:0] 
//		tmp[63:32] := a[31:0] 
//		tmp[95:64] := a[95:64] 
//		tmp[127:96] := a[95:64]
//		tmp[159:128] := a[159:128] 
//		tmp[191:160] := a[159:128] 
//		tmp[223:192] := a[223:192] 
//		tmp[255:224] := a[223:192]
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_maskz_moveldup_ps'.
// Requires AVX512F.
func MaskzMoveldupPs(k Mmask8, a M256) M256 {
	return M256(maskzMoveldupPs(uint8(k), [8]float32(a)))
}

func maskzMoveldupPs(k uint8, a [8]float32) [8]float32


// MoveldupPs: Duplicate even-indexed single-precision (32-bit) floating-point
// elements from 'a', and store the results in 'dst'. 
//
//		dst[31:0] := a[31:0] 
//		dst[63:32] := a[31:0] 
//		dst[95:64] := a[95:64] 
//		dst[127:96] := a[95:64]
//		dst[159:128] := a[159:128] 
//		dst[191:160] := a[159:128] 
//		dst[223:192] := a[223:192] 
//		dst[255:224] := a[223:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVSLDUP'. Intrinsic: '_mm256_moveldup_ps'.
// Requires AVX.
func MoveldupPs(a M256) M256 {
	return M256(moveldupPs([8]float32(a)))
}

func moveldupPs(a [8]float32) [8]float32


// MovemaskEpi8: Create mask from the most significant bit of each 8-bit
// element in 'a', and store the result in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[j] := a[i+7]
//		ENDFOR
//
// Instruction: 'VPMOVMSKB'. Intrinsic: '_mm256_movemask_epi8'.
// Requires AVX2.
func MovemaskEpi8(a M256i) int {
	return int(movemaskEpi8([32]byte(a)))
}

func movemaskEpi8(a [32]byte) int


// MovemaskPd: Set each bit of mask 'dst' based on the most significant bit of
// the corresponding packed double-precision (64-bit) floating-point element in
// 'a'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63]
//				dst[j] := 1
//			ELSE
//				dst[j] := 0
//			FI
//		ENDFOR
//		dst[MAX:4] := 0
//
// Instruction: 'VMOVMSKPD'. Intrinsic: '_mm256_movemask_pd'.
// Requires AVX.
func MovemaskPd(a M256d) int {
	return int(movemaskPd([4]float64(a)))
}

func movemaskPd(a [4]float64) int


// MovemaskPs: Set each bit of mask 'dst' based on the most significant bit of
// the corresponding packed single-precision (32-bit) floating-point element in
// 'a'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31]
//				dst[j] := 1
//			ELSE
//				dst[j] := 0
//			FI
//		ENDFOR
//		dst[MAX:8] := 0
//
// Instruction: 'VMOVMSKPS'. Intrinsic: '_mm256_movemask_ps'.
// Requires AVX.
func MovemaskPs(a M256) int {
	return int(movemaskPs([8]float32(a)))
}

func movemaskPs(a [8]float32) int


// Movepi16Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 16-bit integer in 'a'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF a[i+15]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPMOVW2M'. Intrinsic: '_mm256_movepi16_mask'.
// Requires AVX512BW.
func Movepi16Mask(a M256i) Mmask16 {
	return Mmask16(movepi16Mask([32]byte(a)))
}

func movepi16Mask(a [32]byte) uint16


// Movepi32Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 32-bit integer in 'a'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF a[i+31]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPMOVD2M'. Intrinsic: '_mm256_movepi32_mask'.
// Requires AVX512DQ.
func Movepi32Mask(a M256i) Mmask8 {
	return Mmask8(movepi32Mask([32]byte(a)))
}

func movepi32Mask(a [32]byte) uint8


// Movepi64Mask: Set each bit of mask register 'k' based on the most
// significant bit of the corresponding packed 64-bit integer in 'a'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF a[i+63]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPMOVQ2M'. Intrinsic: '_mm256_movepi64_mask'.
// Requires AVX512DQ.
func Movepi64Mask(a M256i) Mmask8 {
	return Mmask8(movepi64Mask([32]byte(a)))
}

func movepi64Mask(a [32]byte) uint8


// Movepi8Mask: Set each bit of mask register 'k' based on the most significant
// bit of the corresponding packed 8-bit integer in 'a'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF a[i+7]
//				k[j] := 1
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPMOVB2M'. Intrinsic: '_mm256_movepi8_mask'.
// Requires AVX512BW.
func Movepi8Mask(a M256i) Mmask32 {
	return Mmask32(movepi8Mask([32]byte(a)))
}

func movepi8Mask(a [32]byte) uint32


// MovmEpi16: Set each packed 16-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := 0xFFFF
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2W'. Intrinsic: '_mm256_movm_epi16'.
// Requires AVX512BW.
func MovmEpi16(k Mmask16) M256i {
	return M256i(movmEpi16(uint16(k)))
}

func movmEpi16(k uint16) [32]byte


// MovmEpi32: Set each packed 32-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := 0xFFFFFFFF
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2D'. Intrinsic: '_mm256_movm_epi32'.
// Requires AVX512DQ.
func MovmEpi32(k Mmask8) M256i {
	return M256i(movmEpi32(uint8(k)))
}

func movmEpi32(k uint8) [32]byte


// MovmEpi64: Set each packed 64-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := 0xFFFFFFFFffffffff
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2Q'. Intrinsic: '_mm256_movm_epi64'.
// Requires AVX512DQ.
func MovmEpi64(k Mmask8) M256i {
	return M256i(movmEpi64(uint8(k)))
}

func movmEpi64(k uint8) [32]byte


// MovmEpi8: Set each packed 8-bit integer in 'dst' to all ones or all zeros
// based on the value of the corresponding bit in 'k'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := 0xFF
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMOVM2B'. Intrinsic: '_mm256_movm_epi8'.
// Requires AVX512BW.
func MovmEpi8(k Mmask32) M256i {
	return M256i(movmEpi8(uint32(k)))
}

func movmEpi8(k uint32) [32]byte


// MpsadbwEpu8: Compute the sum of absolute differences (SADs) of quadruplets
// of unsigned 8-bit integers in 'a' compared to those in 'b', and store the
// 16-bit results in 'dst'.
// 	Eight SADs are performed for each 128-bit lane using one quadruplet from
// 'b' and eight quadruplets from 'a'. One quadruplet is selected from 'b'
// starting at on the offset specified in 'imm8'. Eight quadruplets are formed
// from sequential 8-bit integers selected from 'a' starting at the offset
// specified in 'imm8'. 
//
//		MPSADBW(a[127:0], b[127:0], imm8[2:0]) {
//			i := imm8[2]*32
//			b_offset := imm8[1:0]*32
//			FOR j := 0 to 7
//				i := j*8
//				k := a_offset+i
//				l := b_offset
//				tmp[i+15:i] := ABS(a[k+7:k] - b[l+7:l]) + ABS(a[k+15:k+8] - b[l+15:l+8]) + ABS(a[k+23:k+16] - b[l+23:l+16]) + ABS(a[k+31:k+24] - b[l+31:l+24])
//			ENDFOR
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := MPSADBW(a[127:0], b[127:0], imm8[2:0])
//		dst[255:128] := MPSADBW(a[255:128], b[255:128], imm8[5:3])
//		dst[MAX:256] := 0
//
// Instruction: 'VMPSADBW'. Intrinsic: '_mm256_mpsadbw_epu8'.
// Requires AVX2.
func MpsadbwEpu8(a M256i, b M256i, imm8 int) M256i {
	return M256i(mpsadbwEpu8([32]byte(a), [32]byte(b), imm8))
}

func mpsadbwEpu8(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_mask_mul_epi32'.
// Requires AVX512F.
func MaskMulEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulEpi32: Multiply the low 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the signed 64-bit results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_maskz_mul_epi32'.
// Requires AVX512F.
func MaskzMulEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MulEpi32: Multiply the low 32-bit integers from each packed 64-bit element
// in 'a' and 'b', and store the signed 64-bit results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULDQ'. Intrinsic: '_mm256_mul_epi32'.
// Requires AVX2.
func MulEpi32(a M256i, b M256i) M256i {
	return M256i(mulEpi32([32]byte(a), [32]byte(b)))
}

func mulEpi32(a [32]byte, b [32]byte) [32]byte


// MaskMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_mask_mul_epu32'.
// Requires AVX512F.
func MaskMulEpu32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulEpu32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulEpu32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulEpu32: Multiply the low unsigned 32-bit integers from each packed
// 64-bit element in 'a' and 'b', and store the unsigned 64-bit results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_maskz_mul_epu32'.
// Requires AVX512F.
func MaskzMulEpu32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulEpu32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulEpu32(k uint8, a [32]byte, b [32]byte) [32]byte


// MulEpu32: Multiply the low unsigned 32-bit integers from each packed 64-bit
// element in 'a' and 'b', and store the unsigned 64-bit results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULUDQ'. Intrinsic: '_mm256_mul_epu32'.
// Requires AVX2.
func MulEpu32(a M256i, b M256i) M256i {
	return M256i(mulEpu32([32]byte(a), [32]byte(b)))
}

func mulEpu32(a [32]byte, b [32]byte) [32]byte


// MaskMulPd: Multiply packed double-precision (64-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_mask_mul_pd'.
// Requires AVX512F.
func MaskMulPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskMulPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskMulPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzMulPd: Multiply packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] * b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_maskz_mul_pd'.
// Requires AVX512F.
func MaskzMulPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzMulPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzMulPd(k uint8, a [4]float64, b [4]float64) [4]float64


// MulPd: Multiply packed double-precision (64-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] * b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPD'. Intrinsic: '_mm256_mul_pd'.
// Requires AVX.
func MulPd(a M256d, b M256d) M256d {
	return M256d(mulPd([4]float64(a), [4]float64(b)))
}

func mulPd(a [4]float64, b [4]float64) [4]float64


// MaskMulPs: Multiply packed single-precision (32-bit) floating-point elements
// in 'a' and 'b', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'src' when the corresponding mask bit is not set).  RM. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_mask_mul_ps'.
// Requires AVX512F.
func MaskMulPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskMulPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskMulPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzMulPs: Multiply packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] * b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_maskz_mul_ps'.
// Requires AVX512F.
func MaskzMulPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzMulPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzMulPs(k uint8, a [8]float32, b [8]float32) [8]float32


// MulPs: Multiply packed single-precision (32-bit) floating-point elements in
// 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] * b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMULPS'. Intrinsic: '_mm256_mul_ps'.
// Requires AVX.
func MulPs(a M256, b M256) M256 {
	return M256(mulPs([8]float32(a), [8]float32(b)))
}

func mulPs(a [8]float32, b [8]float32) [8]float32


// MaskMulhiEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm256_mask_mulhi_epi16'.
// Requires AVX512BW.
func MaskMulhiEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMulhiEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMulhiEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMulhiEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm256_maskz_mulhi_epi16'.
// Requires AVX512BW.
func MaskzMulhiEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMulhiEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMulhiEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// MulhiEpi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the high 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHW'. Intrinsic: '_mm256_mulhi_epi16'.
// Requires AVX2.
func MulhiEpi16(a M256i, b M256i) M256i {
	return M256i(mulhiEpi16([32]byte(a), [32]byte(b)))
}

func mulhiEpi16(a [32]byte, b [32]byte) [32]byte


// MaskMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm256_mask_mulhi_epu16'.
// Requires AVX512BW.
func MaskMulhiEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMulhiEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMulhiEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and
// 'b', producing intermediate 32-bit integers, and store the high 16 bits of
// the intermediate integers in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[31:16]
//			ELSE
//				dst[i+15:i] := o
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm256_maskz_mulhi_epu16'.
// Requires AVX512BW.
func MaskzMulhiEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMulhiEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMulhiEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// MulhiEpu16: Multiply the packed unsigned 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the high 16 bits of the
// intermediate integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[31:16]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHUW'. Intrinsic: '_mm256_mulhi_epu16'.
// Requires AVX2.
func MulhiEpu16(a M256i, b M256i) M256i {
	return M256i(mulhiEpu16([32]byte(a), [32]byte(b)))
}

func mulhiEpu16(a [32]byte, b [32]byte) [32]byte


// MaskMulhrsEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm256_mask_mulhrs_epi16'.
// Requires AVX512BW.
func MaskMulhrsEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMulhrsEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMulhrsEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMulhrsEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//				dst[i+15:i] := tmp[16:1]
//			ELSE
//				dst[i+15:i] := 9
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm256_maskz_mulhrs_epi16'.
// Requires AVX512BW.
func MaskzMulhrsEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMulhrsEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMulhrsEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// MulhrsEpi16: Multiply packed 16-bit integers in 'a' and 'b', producing
// intermediate signed 32-bit integers. Truncate each intermediate integer to
// the 18 most significant bits, round by adding 1, and store bits [16:1] to
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := ((a[i+15:i] * b[i+15:i]) >> 14) + 1
//			dst[i+15:i] := tmp[16:1]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULHRSW'. Intrinsic: '_mm256_mulhrs_epi16'.
// Requires AVX2.
func MulhrsEpi16(a M256i, b M256i) M256i {
	return M256i(mulhrsEpi16([32]byte(a), [32]byte(b)))
}

func mulhrsEpi16(a [32]byte, b [32]byte) [32]byte


// MaskMulloEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the low 16 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm256_mask_mullo_epi16'.
// Requires AVX512BW.
func MaskMulloEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskMulloEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskMulloEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzMulloEpi16: Multiply the packed 16-bit integers in 'a' and 'b',
// producing intermediate 32-bit integers, and store the low 16 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				tmp[31:0] := a[i+15:i] * b[i+15:i]
//				dst[i+15:i] := tmp[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm256_maskz_mullo_epi16'.
// Requires AVX512BW.
func MaskzMulloEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzMulloEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzMulloEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// MulloEpi16: Multiply the packed 16-bit integers in 'a' and 'b', producing
// intermediate 32-bit integers, and store the low 16 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			tmp[31:0] := a[i+15:i] * b[i+15:i]
//			dst[i+15:i] := tmp[15:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLW'. Intrinsic: '_mm256_mullo_epi16'.
// Requires AVX2.
func MulloEpi16(a M256i, b M256i) M256i {
	return M256i(mulloEpi16([32]byte(a), [32]byte(b)))
}

func mulloEpi16(a [32]byte, b [32]byte) [32]byte


// MaskMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_mask_mullo_epi32'.
// Requires AVX512F.
func MaskMulloEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulloEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulloEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b',
// producing intermediate 64-bit integers, and store the low 32 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				tmp[63:0] := a[i+31:i] * b[i+31:i]
//				dst[i+31:i] := tmp[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_maskz_mullo_epi32'.
// Requires AVX512F.
func MaskzMulloEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulloEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulloEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MulloEpi32: Multiply the packed 32-bit integers in 'a' and 'b', producing
// intermediate 64-bit integers, and store the low 32 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			tmp[63:0] := a[i+31:i] * b[i+31:i]
//			dst[i+31:i] := tmp[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLD'. Intrinsic: '_mm256_mullo_epi32'.
// Requires AVX2.
func MulloEpi32(a M256i, b M256i) M256i {
	return M256i(mulloEpi32([32]byte(a), [32]byte(b)))
}

func mulloEpi32(a [32]byte, b [32]byte) [32]byte


// MaskMulloEpi64: Multiply the packed 64-bit integers in 'a' and 'b',
// producing intermediate 128-bit integers, and store the low 64 bits of the
// intermediate integers in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm256_mask_mullo_epi64'.
// Requires AVX512DQ.
func MaskMulloEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskMulloEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskMulloEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzMulloEpi64: Multiply the packed 64-bit integers in 'a' and 'b',
// producing intermediate 128-bit integers, and store the low 64 bits of the
// intermediate integers in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				tmp[127:0] := a[i+63:i] * b[i+63:i]
//				dst[i+63:i] := tmp[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm256_maskz_mullo_epi64'.
// Requires AVX512DQ.
func MaskzMulloEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzMulloEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzMulloEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// MulloEpi64: Multiply the packed 64-bit integers in 'a' and 'b', producing
// intermediate 128-bit integers, and store the low 64 bits of the intermediate
// integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			tmp[127:0] := a[i+63:i] * b[i+63:i]
//			dst[i+63:i] := tmp[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULLQ'. Intrinsic: '_mm256_mullo_epi64'.
// Requires AVX512DQ.
func MulloEpi64(a M256i, b M256i) M256i {
	return M256i(mulloEpi64([32]byte(a), [32]byte(b)))
}

func mulloEpi64(a [32]byte, b [32]byte) [32]byte


// MaskMultishiftEpi64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR i := 0 to 3
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := src[q+j*8+7:q+j*8]
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm256_mask_multishift_epi64_epi8'.
// Requires AVX512VL.
func MaskMultishiftEpi64Epi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskMultishiftEpi64Epi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskMultishiftEpi64Epi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzMultishiftEpi64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR i := 0 to 3
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				IF k[i*8+j]
//					dst[q+j*8+7:q+j*8] := tmp8[7:0]
//				ELSE
//					dst[q+j*8+7:q+j*8] := 0
//				FI
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm256_maskz_multishift_epi64_epi8'.
// Requires AVX512VL.
func MaskzMultishiftEpi64Epi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzMultishiftEpi64Epi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzMultishiftEpi64Epi8(k uint32, a [32]byte, b [32]byte) [32]byte


// MultishiftEpi64Epi8: For each 64-bit element in 'b', select 8 unaligned
// bytes using a byte-granular shift control within the corresponding 64-bit
// element of 'a', and store the 8 assembled bytes to the corresponding 64-bit
// element of 'dst'. 
//
//		FOR i := 0 to 3
//			q := i * 64
//			FOR j := 0 to 7
//				tmp8 := 0
//				ctrl := a[q+j*8+7:q+j*8] & 63
//				FOR l := 0 to 7
//					tmp8[k] := b[q+((ctrl+k) & 63)]
//				ENDFOR
//				dst[q+j*8+7:q+j*8] := tmp8[7:0]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPMULTISHIFTQB'. Intrinsic: '_mm256_multishift_epi64_epi8'.
// Requires AVX512VL.
func MultishiftEpi64Epi8(a M256i, b M256i) M256i {
	return M256i(multishiftEpi64Epi8([32]byte(a), [32]byte(b)))
}

func multishiftEpi64Epi8(a [32]byte, b [32]byte) [32]byte


// MaskOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_mask_or_epi32'.
// Requires AVX512F.
func MaskOrEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskOrEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskOrEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOrEpi32: Compute the bitwise OR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORD'. Intrinsic: '_mm256_maskz_or_epi32'.
// Requires AVX512F.
func MaskzOrEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzOrEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzOrEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_mask_or_epi64'.
// Requires AVX512F.
func MaskOrEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskOrEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskOrEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzOrEpi64: Compute the bitwise OR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPORQ'. Intrinsic: '_mm256_maskz_or_epi64'.
// Requires AVX512F.
func MaskzOrEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzOrEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzOrEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm256_mask_or_pd'.
// Requires AVX512DQ.
func MaskOrPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskOrPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskOrPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzOrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm256_maskz_or_pd'.
// Requires AVX512DQ.
func MaskzOrPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzOrPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzOrPd(k uint8, a [4]float64, b [4]float64) [4]float64


// OrPd: Compute the bitwise OR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] BITWISE OR b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPD'. Intrinsic: '_mm256_or_pd'.
// Requires AVX.
func OrPd(a M256d, b M256d) M256d {
	return M256d(orPd([4]float64(a), [4]float64(b)))
}

func orPd(a [4]float64, b [4]float64) [4]float64


// MaskOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm256_mask_or_ps'.
// Requires AVX512DQ.
func MaskOrPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskOrPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskOrPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzOrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm256_maskz_or_ps'.
// Requires AVX512DQ.
func MaskzOrPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzOrPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzOrPs(k uint8, a [8]float32, b [8]float32) [8]float32


// OrPs: Compute the bitwise OR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] BITWISE OR b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VORPS'. Intrinsic: '_mm256_or_ps'.
// Requires AVX.
func OrPs(a M256, b M256) M256 {
	return M256(orPs([8]float32(a), [8]float32(b)))
}

func orPs(a [8]float32, b [8]float32) [8]float32


// OrSi256: Compute the bitwise OR of 256 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := (a[255:0] OR b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPOR'. Intrinsic: '_mm256_or_si256'.
// Requires AVX2.
func OrSi256(a M256i, b M256i) M256i {
	return M256i(orSi256([32]byte(a), [32]byte(b)))
}

func orSi256(a [32]byte, b [32]byte) [32]byte


// MaskPacksEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using signed saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm256_mask_packs_epi16'.
// Requires AVX512BW.
func MaskPacksEpi16(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskPacksEpi16([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskPacksEpi16(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzPacksEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using signed saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm256_maskz_packs_epi16'.
// Requires AVX512BW.
func MaskzPacksEpi16(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzPacksEpi16(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzPacksEpi16(k uint32, a [32]byte, b [32]byte) [32]byte


// PacksEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_Int8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_Int8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_Int8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_Int8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_Int8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_Int8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_Int8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_Int8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_Int8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_Int8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_Int8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_Int8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_Int8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_Int8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_Int8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_Int8 (b[127:112])
//		dst[135:128] := Saturate_Int16_To_Int8 (a[143:128])
//		dst[143:136] := Saturate_Int16_To_Int8 (a[159:144])
//		dst[151:144] := Saturate_Int16_To_Int8 (a[175:160])
//		dst[159:152] := Saturate_Int16_To_Int8 (a[191:176])
//		dst[167:160] := Saturate_Int16_To_Int8 (a[207:192])
//		dst[175:168] := Saturate_Int16_To_Int8 (a[223:208])
//		dst[183:176] := Saturate_Int16_To_Int8 (a[239:224])
//		dst[191:184] := Saturate_Int16_To_Int8 (a[255:240])
//		dst[199:192] := Saturate_Int16_To_Int8 (b[143:128])
//		dst[207:200] := Saturate_Int16_To_Int8 (b[159:144])
//		dst[215:208] := Saturate_Int16_To_Int8 (b[175:160])
//		dst[223:216] := Saturate_Int16_To_Int8 (b[191:176])
//		dst[231:224] := Saturate_Int16_To_Int8 (b[207:192])
//		dst[239:232] := Saturate_Int16_To_Int8 (b[223:208])
//		dst[247:240] := Saturate_Int16_To_Int8 (b[239:224])
//		dst[255:248] := Saturate_Int16_To_Int8 (b[255:240])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSWB'. Intrinsic: '_mm256_packs_epi16'.
// Requires AVX2.
func PacksEpi16(a M256i, b M256i) M256i {
	return M256i(packsEpi16([32]byte(a), [32]byte(b)))
}

func packsEpi16(a [32]byte, b [32]byte) [32]byte


// MaskPacksEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm256_mask_packs_epi32'.
// Requires AVX512BW.
func MaskPacksEpi32(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskPacksEpi32([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskPacksEpi32(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzPacksEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using signed saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm256_maskz_packs_epi32'.
// Requires AVX512BW.
func MaskzPacksEpi32(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzPacksEpi32(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzPacksEpi32(k uint16, a [32]byte, b [32]byte) [32]byte


// PacksEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed 16-bit
// integers using signed saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_Int16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_Int16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_Int16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_Int16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_Int16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_Int16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_Int16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_Int16 (b[127:96])
//		dst[143:128] := Saturate_Int32_To_Int16 (a[159:128])
//		dst[159:144] := Saturate_Int32_To_Int16 (a[191:160])
//		dst[175:160] := Saturate_Int32_To_Int16 (a[223:192])
//		dst[191:176] := Saturate_Int32_To_Int16 (a[255:224])
//		dst[207:192] := Saturate_Int32_To_Int16 (b[159:128])
//		dst[223:208] := Saturate_Int32_To_Int16 (b[191:160])
//		dst[239:224] := Saturate_Int32_To_Int16 (b[223:192])
//		dst[255:240] := Saturate_Int32_To_Int16 (b[255:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKSSDW'. Intrinsic: '_mm256_packs_epi32'.
// Requires AVX2.
func PacksEpi32(a M256i, b M256i) M256i {
	return M256i(packsEpi32([32]byte(a), [32]byte(b)))
}

func packsEpi32(a [32]byte, b [32]byte) [32]byte


// MaskPackusEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm256_mask_packus_epi16'.
// Requires AVX512BW.
func MaskPackusEpi16(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskPackusEpi16([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskPackusEpi16(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzPackusEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed
// 8-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		tmp_dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		tmp_dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		tmp_dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		tmp_dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		tmp_dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		tmp_dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		tmp_dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		tmp_dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		tmp_dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		tmp_dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		tmp_dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		tmp_dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		tmp_dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		tmp_dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		tmp_dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		tmp_dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		tmp_dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		tmp_dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		tmp_dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		tmp_dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		tmp_dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		tmp_dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		tmp_dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		tmp_dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		tmp_dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		tmp_dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		tmp_dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		tmp_dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		tmp_dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		tmp_dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		tmp_dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm256_maskz_packus_epi16'.
// Requires AVX512BW.
func MaskzPackusEpi16(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzPackusEpi16(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzPackusEpi16(k uint32, a [32]byte, b [32]byte) [32]byte


// PackusEpi16: Convert packed 16-bit integers from 'a' and 'b' to packed 8-bit
// integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[7:0] := Saturate_Int16_To_UnsignedInt8 (a[15:0])
//		dst[15:8] := Saturate_Int16_To_UnsignedInt8 (a[31:16])
//		dst[23:16] := Saturate_Int16_To_UnsignedInt8 (a[47:32])
//		dst[31:24] := Saturate_Int16_To_UnsignedInt8 (a[63:48])
//		dst[39:32] := Saturate_Int16_To_UnsignedInt8 (a[79:64])
//		dst[47:40] := Saturate_Int16_To_UnsignedInt8 (a[95:80])
//		dst[55:48] := Saturate_Int16_To_UnsignedInt8 (a[111:96])
//		dst[63:56] := Saturate_Int16_To_UnsignedInt8 (a[127:112])
//		dst[71:64] := Saturate_Int16_To_UnsignedInt8 (b[15:0])
//		dst[79:72] := Saturate_Int16_To_UnsignedInt8 (b[31:16])
//		dst[87:80] := Saturate_Int16_To_UnsignedInt8 (b[47:32])
//		dst[95:88] := Saturate_Int16_To_UnsignedInt8 (b[63:48])
//		dst[103:96] := Saturate_Int16_To_UnsignedInt8 (b[79:64])
//		dst[111:104] := Saturate_Int16_To_UnsignedInt8 (b[95:80])
//		dst[119:112] := Saturate_Int16_To_UnsignedInt8 (b[111:96])
//		dst[127:120] := Saturate_Int16_To_UnsignedInt8 (b[127:112])
//		dst[135:128] := Saturate_Int16_To_UnsignedInt8 (a[143:128])
//		dst[143:136] := Saturate_Int16_To_UnsignedInt8 (a[159:144])
//		dst[151:144] := Saturate_Int16_To_UnsignedInt8 (a[175:160])
//		dst[159:152] := Saturate_Int16_To_UnsignedInt8 (a[191:176])
//		dst[167:160] := Saturate_Int16_To_UnsignedInt8 (a[207:192])
//		dst[175:168] := Saturate_Int16_To_UnsignedInt8 (a[223:208])
//		dst[183:176] := Saturate_Int16_To_UnsignedInt8 (a[239:224])
//		dst[191:184] := Saturate_Int16_To_UnsignedInt8 (a[255:240])
//		dst[199:192] := Saturate_Int16_To_UnsignedInt8 (b[143:128])
//		dst[207:200] := Saturate_Int16_To_UnsignedInt8 (b[159:144])
//		dst[215:208] := Saturate_Int16_To_UnsignedInt8 (b[175:160])
//		dst[223:216] := Saturate_Int16_To_UnsignedInt8 (b[191:176])
//		dst[231:224] := Saturate_Int16_To_UnsignedInt8 (b[207:192])
//		dst[239:232] := Saturate_Int16_To_UnsignedInt8 (b[223:208])
//		dst[247:240] := Saturate_Int16_To_UnsignedInt8 (b[239:224])
//		dst[255:248] := Saturate_Int16_To_UnsignedInt8 (b[255:240])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSWB'. Intrinsic: '_mm256_packus_epi16'.
// Requires AVX2.
func PackusEpi16(a M256i, b M256i) M256i {
	return M256i(packusEpi16([32]byte(a), [32]byte(b)))
}

func packusEpi16(a [32]byte, b [32]byte) [32]byte


// MaskPackusEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm256_mask_packus_epi32'.
// Requires AVX512BW.
func MaskPackusEpi32(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskPackusEpi32([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskPackusEpi32(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzPackusEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). 
//
//		tmp_dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		tmp_dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		tmp_dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		tmp_dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		tmp_dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		tmp_dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		tmp_dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		tmp_dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		tmp_dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		tmp_dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		tmp_dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		tmp_dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		tmp_dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		tmp_dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		tmp_dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		tmp_dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm256_maskz_packus_epi32'.
// Requires AVX512BW.
func MaskzPackusEpi32(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzPackusEpi32(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzPackusEpi32(k uint16, a [32]byte, b [32]byte) [32]byte


// PackusEpi32: Convert packed 32-bit integers from 'a' and 'b' to packed
// 16-bit integers using unsigned saturation, and store the results in 'dst'. 
//
//		dst[15:0] := Saturate_Int32_To_UnsignedInt16 (a[31:0])
//		dst[31:16] := Saturate_Int32_To_UnsignedInt16 (a[63:32])
//		dst[47:32] := Saturate_Int32_To_UnsignedInt16 (a[95:64])
//		dst[63:48] := Saturate_Int32_To_UnsignedInt16 (a[127:96])
//		dst[79:64] := Saturate_Int32_To_UnsignedInt16 (b[31:0])
//		dst[95:80] := Saturate_Int32_To_UnsignedInt16 (b[63:32])
//		dst[111:96] := Saturate_Int32_To_UnsignedInt16 (b[95:64])
//		dst[127:112] := Saturate_Int32_To_UnsignedInt16 (b[127:96])
//		dst[143:128] := Saturate_Int32_To_UnsignedInt16 (a[159:128])
//		dst[159:144] := Saturate_Int32_To_UnsignedInt16 (a[191:160])
//		dst[175:160] := Saturate_Int32_To_UnsignedInt16 (a[223:192])
//		dst[191:176] := Saturate_Int32_To_UnsignedInt16 (a[255:224])
//		dst[207:192] := Saturate_Int32_To_UnsignedInt16 (b[159:128])
//		dst[223:208] := Saturate_Int32_To_UnsignedInt16 (b[191:160])
//		dst[239:224] := Saturate_Int32_To_UnsignedInt16 (b[223:192])
//		dst[255:240] := Saturate_Int32_To_UnsignedInt16 (b[255:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPACKUSDW'. Intrinsic: '_mm256_packus_epi32'.
// Requires AVX2.
func PackusEpi32(a M256i, b M256i) M256i {
	return M256i(packusEpi32([32]byte(a), [32]byte(b)))
}

func packusEpi32(a [32]byte, b [32]byte) [32]byte


// MaskPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permute_pd'.
// Requires AVX512F.
func MaskPermutePd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskPermutePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskPermutePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzPermutePd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		IF (imm8[0] == 0) tmp_dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) tmp_dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) tmp_dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) tmp_dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) tmp_dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) tmp_dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) tmp_dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permute_pd'.
// Requires AVX512F.
func MaskzPermutePd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzPermutePd(uint8(k), [4]float64(a), imm8))
}

func maskzPermutePd(k uint8, a [4]float64, imm8 int) [4]float64


// PermutePd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		IF (imm8[0] == 0) dst[63:0] := a[63:0]
//		IF (imm8[0] == 1) dst[63:0] := a[127:64]
//		IF (imm8[1] == 0) dst[127:64] := a[63:0]
//		IF (imm8[1] == 1) dst[127:64] := a[127:64]
//		IF (imm8[2] == 0) dst[191:128] := a[191:128]
//		IF (imm8[2] == 1) dst[191:128] := a[255:192]
//		IF (imm8[3] == 0) dst[255:192] := a[191:128]
//		IF (imm8[3] == 1) dst[255:192] := a[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_permute_pd'.
// Requires AVX.
func PermutePd(a M256d, imm8 int) M256d {
	return M256d(permutePd([4]float64(a), imm8))
}

func permutePd(a [4]float64, imm8 int) [4]float64


// MaskPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permute_ps'.
// Requires AVX512F.
func MaskPermutePs(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskPermutePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskPermutePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzPermutePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permute_ps'.
// Requires AVX512F.
func MaskzPermutePs(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzPermutePs(uint8(k), [8]float32(a), imm8))
}

func maskzPermutePs(k uint8, a [8]float32, imm8 int) [8]float32


// PermutePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_permute_ps'.
// Requires AVX.
func PermutePs(a M256, imm8 int) M256 {
	return M256(permutePs([8]float32(a), imm8))
}

func permutePs(a [8]float32, imm8 int) [8]float32


// Permute2f128Pd: Shuffle 128-bits (composed of 2 packed double-precision
// (64-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2F128'. Intrinsic: '_mm256_permute2f128_pd'.
// Requires AVX.
func Permute2f128Pd(a M256d, b M256d, imm8 int) M256d {
	return M256d(permute2f128Pd([4]float64(a), [4]float64(b), imm8))
}

func permute2f128Pd(a [4]float64, b [4]float64, imm8 int) [4]float64


// Permute2f128Ps: Shuffle 128-bits (composed of 4 packed single-precision
// (32-bit) floating-point elements) selected by 'imm8' from 'a' and 'b', and
// store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2F128'. Intrinsic: '_mm256_permute2f128_ps'.
// Requires AVX.
func Permute2f128Ps(a M256, b M256, imm8 int) M256 {
	return M256(permute2f128Ps([8]float32(a), [8]float32(b), imm8))
}

func permute2f128Ps(a [8]float32, b [8]float32, imm8 int) [8]float32


// Permute2f128Si256: Shuffle 128-bits (composed of integer data) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2F128'. Intrinsic: '_mm256_permute2f128_si256'.
// Requires AVX.
func Permute2f128Si256(a M256i, b M256i, imm8 int) M256i {
	return M256i(permute2f128Si256([32]byte(a), [32]byte(b), imm8))
}

func permute2f128Si256(a [32]byte, b [32]byte, imm8 int) [32]byte


// Permute2x128Si256: Shuffle 128-bits (composed of integer data) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT4(src1, src2, control){
//			CASE(control[1:0])
//			0:	tmp[127:0] := src1[127:0]
//			1:	tmp[127:0] := src1[255:128]
//			2:	tmp[127:0] := src2[127:0]
//			3:	tmp[127:0] := src2[255:128]
//			ESAC
//			IF control[3]
//				tmp[127:0] := 0
//			FI
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT4(a[255:0], b[255:0], imm8[3:0])
//		dst[255:128] := SELECT4(a[255:0], b[255:0], imm8[7:4])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERM2I128'. Intrinsic: '_mm256_permute2x128_si256'.
// Requires AVX2.
func Permute2x128Si256(a M256i, b M256i, imm8 int) M256i {
	return M256i(permute2x128Si256([32]byte(a), [32]byte(b), imm8))
}

func permute2x128Si256(a [32]byte, b [32]byte, imm8 int) [32]byte


// Permute4x64Epi64: Shuffle 64-bit integers in 'a' across lanes using the
// control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permute4x64_epi64'.
// Requires AVX2.
func Permute4x64Epi64(a M256i, imm8 int) M256i {
	return M256i(permute4x64Epi64([32]byte(a), imm8))
}

func permute4x64Epi64(a [32]byte, imm8 int) [32]byte


// Permute4x64Pd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permute4x64_pd'.
// Requires AVX2.
func Permute4x64Pd(a M256d, imm8 int) M256d {
	return M256d(permute4x64Pd([4]float64(a), imm8))
}

func permute4x64Pd(a [4]float64, imm8 int) [4]float64


// MaskPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_mask_permutevar_pd'.
// Requires AVX512F.
func MaskPermutevarPd(src M256d, k Mmask8, a M256d, b M256i) M256d {
	return M256d(maskPermutevarPd([4]float64(src), uint8(k), [4]float64(a), [32]byte(b)))
}

func maskPermutevarPd(src [4]float64, k uint8, a [4]float64, b [32]byte) [4]float64


// MaskzPermutevarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		IF (b[1] == 0) tmp_dst[63:0] := a[63:0]
//		IF (b[1] == 1) tmp_dst[63:0] := a[127:64]
//		IF (b[65] == 0) tmp_dst[127:64] := a[63:0]
//		IF (b[65] == 1) tmp_dst[127:64] := a[127:64]
//		IF (b[129] == 0) tmp_dst[191:128] := a[191:128]
//		IF (b[129] == 1) tmp_dst[191:128] := a[255:192]
//		IF (b[193] == 0) tmp_dst[255:192] := a[191:128]
//		IF (b[193] == 1) tmp_dst[255:192] := a[255:192]
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_maskz_permutevar_pd'.
// Requires AVX512F.
func MaskzPermutevarPd(k Mmask8, a M256d, b M256i) M256d {
	return M256d(maskzPermutevarPd(uint8(k), [4]float64(a), [32]byte(b)))
}

func maskzPermutevarPd(k uint8, a [4]float64, b [32]byte) [4]float64


// PermutevarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		IF (b[1] == 0) dst[63:0] := a[63:0]
//		IF (b[1] == 1) dst[63:0] := a[127:64]
//		IF (b[65] == 0) dst[127:64] := a[63:0]
//		IF (b[65] == 1) dst[127:64] := a[127:64]
//		IF (b[129] == 0) dst[191:128] := a[191:128]
//		IF (b[129] == 1) dst[191:128] := a[255:192]
//		IF (b[193] == 0) dst[255:192] := a[191:128]
//		IF (b[193] == 1) dst[255:192] := a[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPD'. Intrinsic: '_mm256_permutevar_pd'.
// Requires AVX.
func PermutevarPd(a M256d, b M256i) M256d {
	return M256d(permutevarPd([4]float64(a), [32]byte(b)))
}

func permutevarPd(a [4]float64, b [32]byte) [4]float64


// MaskPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_mask_permutevar_ps'.
// Requires AVX512F.
func MaskPermutevarPs(src M256, k Mmask8, a M256, b M256i) M256 {
	return M256(maskPermutevarPs([8]float32(src), uint8(k), [8]float32(a), [32]byte(b)))
}

func maskPermutevarPs(src [8]float32, k uint8, a [8]float32, b [32]byte) [8]float32


// MaskzPermutevarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' within 128-bit lanes using the control in 'b', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], b[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], b[33:32])
//		tmp_dst[95:64] := SELECT4(a[127:0], b[65:64])
//		tmp_dst[127:96] := SELECT4(a[127:0], b[97:96])
//		tmp_dst[159:128] := SELECT4(a[255:128], b[129:128])
//		tmp_dst[191:160] := SELECT4(a[255:128], b[161:160])
//		tmp_dst[223:192] := SELECT4(a[255:128], b[193:192])
//		tmp_dst[255:224] := SELECT4(a[255:128], b[225:224])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_maskz_permutevar_ps'.
// Requires AVX512F.
func MaskzPermutevarPs(k Mmask8, a M256, b M256i) M256 {
	return M256(maskzPermutevarPs(uint8(k), [8]float32(a), [32]byte(b)))
}

func maskzPermutevarPs(k uint8, a [8]float32, b [32]byte) [8]float32


// PermutevarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'b', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], b[1:0])
//		dst[63:32] := SELECT4(a[127:0], b[33:32])
//		dst[95:64] := SELECT4(a[127:0], b[65:64])
//		dst[127:96] := SELECT4(a[127:0], b[97:96])
//		dst[159:128] := SELECT4(a[255:128], b[129:128])
//		dst[191:160] := SELECT4(a[255:128], b[161:160])
//		dst[223:192] := SELECT4(a[255:128], b[193:192])
//		dst[255:224] := SELECT4(a[255:128], b[225:224])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMILPS'. Intrinsic: '_mm256_permutevar_ps'.
// Requires AVX.
func PermutevarPs(a M256, b M256i) M256 {
	return M256(permutevarPs([8]float32(a), [32]byte(b)))
}

func permutevarPs(a [8]float32, b [32]byte) [8]float32


// Permutevar8x32Epi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_permutevar8x32_epi32'.
// Requires AVX2.
func Permutevar8x32Epi32(a M256i, idx M256i) M256i {
	return M256i(permutevar8x32Epi32([32]byte(a), [32]byte(idx)))
}

func permutevar8x32Epi32(a [32]byte, idx [32]byte) [32]byte


// Permutevar8x32Ps: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_permutevar8x32_ps'.
// Requires AVX2.
func Permutevar8x32Ps(a M256, idx M256i) M256 {
	return M256(permutevar8x32Ps([8]float32(a), [32]byte(idx)))
}

func permutevar8x32Ps(a [8]float32, idx [32]byte) [8]float32


// MaskPermutexEpi64: Shuffle 64-bit integers in 'a' across lanes lanes using
// the control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutex_epi64'.
// Requires AVX512F.
func MaskPermutexEpi64(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskPermutexEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskPermutexEpi64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzPermutexEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// control in 'imm8', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutex_epi64'.
// Requires AVX512F.
func MaskzPermutexEpi64(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzPermutexEpi64(uint8(k), [32]byte(a), imm8))
}

func maskzPermutexEpi64(k uint8, a [32]byte, imm8 int) [32]byte


// PermutexEpi64: Shuffle 64-bit integers in 'a' across lanes using the control
// in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutex_epi64'.
// Requires AVX512F.
func PermutexEpi64(a M256i, imm8 int) M256i {
	return M256i(permutexEpi64([32]byte(a), imm8))
}

func permutexEpi64(a [32]byte, imm8 int) [32]byte


// MaskPermutexPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the control in 'imm8', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutex_pd'.
// Requires AVX512F.
func MaskPermutexPd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskPermutexPd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskPermutexPd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzPermutexPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		tmp_dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		tmp_dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		tmp_dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		tmp_dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutex_pd'.
// Requires AVX512F.
func MaskzPermutexPd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzPermutexPd(uint8(k), [4]float64(a), imm8))
}

func maskzPermutexPd(k uint8, a [4]float64, imm8 int) [4]float64


// PermutexPd: Shuffle double-precision (64-bit) floating-point elements in 'a'
// across lanes using the control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[63:0] := src[63:0]
//			1:	tmp[63:0] := src[127:64]
//			2:	tmp[63:0] := src[191:128]
//			3:	tmp[63:0] := src[255:192]
//			ESAC
//			RETURN tmp[63:0]
//		}
//		
//		dst[63:0] := SELECT4(a[255:0], imm8[1:0])
//		dst[127:64] := SELECT4(a[255:0], imm8[3:2])
//		dst[191:128] := SELECT4(a[255:0], imm8[5:4])
//		dst[255:192] := SELECT4(a[255:0], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutex_pd'.
// Requires AVX512F.
func PermutexPd(a M256d, imm8 int) M256d {
	return M256d(permutexPd([4]float64(a), imm8))
}

func permutexPd(a [4]float64, imm8 int) [4]float64


// MaskPermutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+3:i]
//				dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2W'. Intrinsic: '_mm256_mask_permutex2var_epi16'.
// Requires AVX512BW.
func MaskPermutex2varEpi16(a M256i, k Mmask16, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2varEpi16([32]byte(a), uint16(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2varEpi16(a [32]byte, k uint16, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+3:i]
//				dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := idx[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2W'. Intrinsic: '_mm256_mask2_permutex2var_epi16'.
// Requires AVX512BW.
func Mask2Permutex2varEpi16(a M256i, idx M256i, k Mmask16, b M256i) M256i {
	return M256i(mask2Permutex2varEpi16([32]byte(a), [32]byte(idx), uint16(k), [32]byte(b)))
}

func mask2Permutex2varEpi16(a [32]byte, idx [32]byte, k uint16, b [32]byte) [32]byte


// MaskzPermutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				off := 16*idx[i+3:i]
//				dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm256_maskz_permutex2var_epi16'.
// Requires AVX512BW.
func MaskzPermutex2varEpi16(k Mmask16, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2varEpi16(uint16(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2varEpi16(k uint16, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2varEpi16: Shuffle 16-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			off := 16*idx[i+3:i]
//			dst[i+15:i] := idx[i+4] ? b[off+15:off] : a[off+15:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2W, VPERMT2W'. Intrinsic: '_mm256_permutex2var_epi16'.
// Requires AVX512BW.
func Permutex2varEpi16(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2varEpi16([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2varEpi16(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+3:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+4] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2D'. Intrinsic: '_mm256_mask_permutex2var_epi32'.
// Requires AVX512F.
func MaskPermutex2varEpi32(a M256i, k Mmask8, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2varEpi32([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2varEpi32(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D'. Intrinsic: '_mm256_mask2_permutex2var_epi32'.
// Requires AVX512F.
func Mask2Permutex2varEpi32(a M256i, idx M256i, k Mmask8, b M256i) M256i {
	return M256i(mask2Permutex2varEpi32([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func mask2Permutex2varEpi32(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// MaskzPermutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_maskz_permutex2var_epi32'.
// Requires AVX512F.
func MaskzPermutex2varEpi32(k Mmask8, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2varEpi32(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2varEpi32(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2varEpi32: Shuffle 32-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2D, VPERMT2D'. Intrinsic: '_mm256_permutex2var_epi32'.
// Requires AVX512F.
func Permutex2varEpi32(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2varEpi32([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2varEpi32(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2Q'. Intrinsic: '_mm256_mask_permutex2var_epi64'.
// Requires AVX512F.
func MaskPermutex2varEpi64(a M256i, k Mmask8, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2varEpi64([32]byte(a), uint8(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2varEpi64(a [32]byte, k uint8, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'idx' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q'. Intrinsic: '_mm256_mask2_permutex2var_epi64'.
// Requires AVX512F.
func Mask2Permutex2varEpi64(a M256i, idx M256i, k Mmask8, b M256i) M256i {
	return M256i(mask2Permutex2varEpi64([32]byte(a), [32]byte(idx), uint8(k), [32]byte(b)))
}

func mask2Permutex2varEpi64(a [32]byte, idx [32]byte, k uint8, b [32]byte) [32]byte


// MaskzPermutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_maskz_permutex2var_epi64'.
// Requires AVX512F.
func MaskzPermutex2varEpi64(k Mmask8, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2varEpi64(uint8(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2varEpi64(k uint8, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2varEpi64: Shuffle 64-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2Q, VPERMT2Q'. Intrinsic: '_mm256_permutex2var_epi64'.
// Requires AVX512F.
func Permutex2varEpi64(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2varEpi64([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2varEpi64(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+4:i]
//				dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2B'. Intrinsic: '_mm256_mask_permutex2var_epi8'.
// Requires AVX512VL.
func MaskPermutex2varEpi8(a M256i, k Mmask32, idx M256i, b M256i) M256i {
	return M256i(maskPermutex2varEpi8([32]byte(a), uint32(k), [32]byte(idx), [32]byte(b)))
}

func maskPermutex2varEpi8(a [32]byte, k uint32, idx [32]byte, b [32]byte) [32]byte


// Mask2Permutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'a' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+4:i]
//				dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm256_mask2_permutex2var_epi8'.
// Requires AVX512VL.
func Mask2Permutex2varEpi8(a M256i, idx M256i, k Mmask32, b M256i) M256i {
	return M256i(mask2Permutex2varEpi8([32]byte(a), [32]byte(idx), uint32(k), [32]byte(b)))
}

func mask2Permutex2varEpi8(a [32]byte, idx [32]byte, k uint32, b [32]byte) [32]byte


// MaskzPermutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes
// using the corresponding selector and index in 'idx', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				off := 8*idx[i+4:i]
//				dst[i+7:i] := idx[i+5] ? b[off+7:off] : a[off+7:off]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2B, VPERMT2B'. Intrinsic: '_mm256_maskz_permutex2var_epi8'.
// Requires AVX512VL.
func MaskzPermutex2varEpi8(k Mmask32, a M256i, idx M256i, b M256i) M256i {
	return M256i(maskzPermutex2varEpi8(uint32(k), [32]byte(a), [32]byte(idx), [32]byte(b)))
}

func maskzPermutex2varEpi8(k uint32, a [32]byte, idx [32]byte, b [32]byte) [32]byte


// Permutex2varEpi8: Shuffle 8-bit integers in 'a' and 'b' across lanes using
// the corresponding selector and index in 'idx', and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			off := 8*idx[i+4:i]
//			dst[i+7:i] := idx[i+6] ? b[off+5:off] : a[off+7:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2B'. Intrinsic: '_mm256_permutex2var_epi8'.
// Requires AVX512VL.
func Permutex2varEpi8(a M256i, idx M256i, b M256i) M256i {
	return M256i(permutex2varEpi8([32]byte(a), [32]byte(idx), [32]byte(b)))
}

func permutex2varEpi8(a [32]byte, idx [32]byte, b [32]byte) [32]byte


// MaskPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := a[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PD'. Intrinsic: '_mm256_mask_permutex2var_pd'.
// Requires AVX512F.
func MaskPermutex2varPd(a M256d, k Mmask8, idx M256i, b M256d) M256d {
	return M256d(maskPermutex2varPd([4]float64(a), uint8(k), [32]byte(idx), [4]float64(b)))
}

func maskPermutex2varPd(a [4]float64, k uint8, idx [32]byte, b [4]float64) [4]float64


// Mask2Permutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := idx[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD'. Intrinsic: '_mm256_mask2_permutex2var_pd'.
// Requires AVX512F.
func Mask2Permutex2varPd(a M256d, idx M256i, k Mmask8, b M256d) M256d {
	return M256d(mask2Permutex2varPd([4]float64(a), [32]byte(idx), uint8(k), [4]float64(b)))
}

func mask2Permutex2varPd(a [4]float64, idx [32]byte, k uint8, b [4]float64) [4]float64


// MaskzPermutex2varPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := (idx[i+2]) ? b[off+63:off] : a[off+63:off]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_maskz_permutex2var_pd'.
// Requires AVX512F.
func MaskzPermutex2varPd(k Mmask8, a M256d, idx M256i, b M256d) M256d {
	return M256d(maskzPermutex2varPd(uint8(k), [4]float64(a), [32]byte(idx), [4]float64(b)))
}

func maskzPermutex2varPd(k uint8, a [4]float64, idx [32]byte, b [4]float64) [4]float64


// Permutex2varPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			off := idx[i+1:i]*64
//			dst[i+63:i] := idx[i+2] ? b[off+63:off] : a[off+63:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PD, VPERMT2PD'. Intrinsic: '_mm256_permutex2var_pd'.
// Requires AVX512F.
func Permutex2varPd(a M256d, idx M256i, b M256d) M256d {
	return M256d(permutex2varPd([4]float64(a), [32]byte(idx), [4]float64(b)))
}

func permutex2varPd(a [4]float64, idx [32]byte, b [4]float64) [4]float64


// MaskPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'a' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMT2PS'. Intrinsic: '_mm256_mask_permutex2var_ps'.
// Requires AVX512F.
func MaskPermutex2varPs(a M256, k Mmask8, idx M256i, b M256) M256 {
	return M256(maskPermutex2varPs([8]float32(a), uint8(k), [32]byte(idx), [8]float32(b)))
}

func maskPermutex2varPs(a [8]float32, k uint8, idx [32]byte, b [8]float32) [8]float32


// Mask2Permutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using writemask 'k' (elements
// are copied from 'idx' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := idx[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS'. Intrinsic: '_mm256_mask2_permutex2var_ps'.
// Requires AVX512F.
func Mask2Permutex2varPs(a M256, idx M256i, k Mmask8, b M256) M256 {
	return M256(mask2Permutex2varPs([8]float32(a), [32]byte(idx), uint8(k), [8]float32(b)))
}

func mask2Permutex2varPs(a [8]float32, idx [32]byte, k uint8, b [8]float32) [8]float32


// MaskzPermutex2varPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' and 'b' across lanes using the corresponding selector and
// index in 'idx', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := (idx[i+3]) ? b[off+31:off] : a[off+31:off]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_maskz_permutex2var_ps'.
// Requires AVX512F.
func MaskzPermutex2varPs(k Mmask8, a M256, idx M256i, b M256) M256 {
	return M256(maskzPermutex2varPs(uint8(k), [8]float32(a), [32]byte(idx), [8]float32(b)))
}

func maskzPermutex2varPs(k uint8, a [8]float32, idx [32]byte, b [8]float32) [8]float32


// Permutex2varPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' and 'b' across lanes using the corresponding selector and index in
// 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			off := idx[i+2:i]*32
//			dst[i+31:i] := idx[i+3] ? b[off+31:off] : a[off+31:off]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMI2PS, VPERMT2PS'. Intrinsic: '_mm256_permutex2var_ps'.
// Requires AVX512F.
func Permutex2varPs(a M256, idx M256i, b M256) M256 {
	return M256(permutex2varPs([8]float32(a), [32]byte(idx), [8]float32(b)))
}

func permutex2varPs(a [8]float32, idx [32]byte, b [8]float32) [8]float32


// MaskPermutexvarEpi16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			id := idx[i+3:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm256_mask_permutexvar_epi16'.
// Requires AVX512BW.
func MaskPermutexvarEpi16(src M256i, k Mmask16, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvarEpi16([32]byte(src), uint16(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvarEpi16(src [32]byte, k uint16, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvarEpi16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			id := idx[i+3:i]*16
//			IF k[j]
//				dst[i+15:i] := a[id+15:id]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm256_maskz_permutexvar_epi16'.
// Requires AVX512BW.
func MaskzPermutexvarEpi16(k Mmask16, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvarEpi16(uint16(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvarEpi16(k uint16, idx [32]byte, a [32]byte) [32]byte


// PermutexvarEpi16: Shuffle 16-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			id := idx[i+3:i]*16
//			dst[i+15:i] := a[id+15:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMW'. Intrinsic: '_mm256_permutexvar_epi16'.
// Requires AVX512BW.
func PermutexvarEpi16(idx M256i, a M256i) M256i {
	return M256i(permutexvarEpi16([32]byte(idx), [32]byte(a)))
}

func permutexvarEpi16(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_mask_permutexvar_epi32'.
// Requires AVX512F.
func MaskPermutexvarEpi32(src M256i, k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvarEpi32([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvarEpi32(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_maskz_permutexvar_epi32'.
// Requires AVX512F.
func MaskzPermutexvarEpi32(k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvarEpi32(uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvarEpi32(k uint8, idx [32]byte, a [32]byte) [32]byte


// PermutexvarEpi32: Shuffle 32-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMD'. Intrinsic: '_mm256_permutexvar_epi32'.
// Requires AVX512F.
func PermutexvarEpi32(idx M256i, a M256i) M256i {
	return M256i(permutexvarEpi32([32]byte(idx), [32]byte(a)))
}

func permutexvarEpi32(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_mask_permutexvar_epi64'.
// Requires AVX512F.
func MaskPermutexvarEpi64(src M256i, k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvarEpi64([32]byte(src), uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvarEpi64(src [32]byte, k uint8, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_maskz_permutexvar_epi64'.
// Requires AVX512F.
func MaskzPermutexvarEpi64(k Mmask8, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvarEpi64(uint8(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvarEpi64(k uint8, idx [32]byte, a [32]byte) [32]byte


// PermutexvarEpi64: Shuffle 64-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMQ'. Intrinsic: '_mm256_permutexvar_epi64'.
// Requires AVX512F.
func PermutexvarEpi64(idx M256i, a M256i) M256i {
	return M256i(permutexvarEpi64([32]byte(idx), [32]byte(a)))
}

func permutexvarEpi64(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvarEpi8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			id := idx[i+4:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm256_mask_permutexvar_epi8'.
// Requires AVX512VL.
func MaskPermutexvarEpi8(src M256i, k Mmask32, idx M256i, a M256i) M256i {
	return M256i(maskPermutexvarEpi8([32]byte(src), uint32(k), [32]byte(idx), [32]byte(a)))
}

func maskPermutexvarEpi8(src [32]byte, k uint32, idx [32]byte, a [32]byte) [32]byte


// MaskzPermutexvarEpi8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			id := idx[i+4:i]*8
//			IF k[j]
//				dst[i+7:i] := a[id+7:id]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm256_maskz_permutexvar_epi8'.
// Requires AVX512VL.
func MaskzPermutexvarEpi8(k Mmask32, idx M256i, a M256i) M256i {
	return M256i(maskzPermutexvarEpi8(uint32(k), [32]byte(idx), [32]byte(a)))
}

func maskzPermutexvarEpi8(k uint32, idx [32]byte, a [32]byte) [32]byte


// PermutexvarEpi8: Shuffle 8-bit integers in 'a' across lanes using the
// corresponding index in 'idx', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			id := idx[i+4:i]*8
//			dst[i+7:i] := a[id+7:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMB'. Intrinsic: '_mm256_permutexvar_epi8'.
// Requires AVX512VL.
func PermutexvarEpi8(idx M256i, a M256i) M256i {
	return M256i(permutexvarEpi8([32]byte(idx), [32]byte(a)))
}

func permutexvarEpi8(idx [32]byte, a [32]byte) [32]byte


// MaskPermutexvarPd: Shuffle double-precision (64-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_mask_permutexvar_pd'.
// Requires AVX512F.
func MaskPermutexvarPd(src M256d, k Mmask8, idx M256i, a M256d) M256d {
	return M256d(maskPermutexvarPd([4]float64(src), uint8(k), [32]byte(idx), [4]float64(a)))
}

func maskPermutexvarPd(src [4]float64, k uint8, idx [32]byte, a [4]float64) [4]float64


// MaskzPermutexvarPd: Shuffle double-precision (64-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			IF k[j]
//				dst[i+63:i] := a[id+63:id]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_maskz_permutexvar_pd'.
// Requires AVX512F.
func MaskzPermutexvarPd(k Mmask8, idx M256i, a M256d) M256d {
	return M256d(maskzPermutexvarPd(uint8(k), [32]byte(idx), [4]float64(a)))
}

func maskzPermutexvarPd(k uint8, idx [32]byte, a [4]float64) [4]float64


// PermutexvarPd: Shuffle double-precision (64-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			id := idx[i+1:i]*64
//			dst[i+63:i] := a[id+63:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPD'. Intrinsic: '_mm256_permutexvar_pd'.
// Requires AVX512F.
func PermutexvarPd(idx M256i, a M256d) M256d {
	return M256d(permutexvarPd([32]byte(idx), [4]float64(a)))
}

func permutexvarPd(idx [32]byte, a [4]float64) [4]float64


// MaskPermutexvarPs: Shuffle single-precision (32-bit) floating-point elements
// in 'a' across lanes using the corresponding index in 'idx', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_mask_permutexvar_ps'.
// Requires AVX512F.
func MaskPermutexvarPs(src M256, k Mmask8, idx M256i, a M256) M256 {
	return M256(maskPermutexvarPs([8]float32(src), uint8(k), [32]byte(idx), [8]float32(a)))
}

func maskPermutexvarPs(src [8]float32, k uint8, idx [32]byte, a [8]float32) [8]float32


// MaskzPermutexvarPs: Shuffle single-precision (32-bit) floating-point
// elements in 'a' across lanes using the corresponding index in 'idx', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			IF k[j]
//				dst[i+31:i] := a[id+31:id]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_maskz_permutexvar_ps'.
// Requires AVX512F.
func MaskzPermutexvarPs(k Mmask8, idx M256i, a M256) M256 {
	return M256(maskzPermutexvarPs(uint8(k), [32]byte(idx), [8]float32(a)))
}

func maskzPermutexvarPs(k uint8, idx [32]byte, a [8]float32) [8]float32


// PermutexvarPs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' across lanes using the corresponding index in 'idx'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			id := idx[i+2:i]*32
//			dst[i+31:i] := a[id+31:id]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPERMPS'. Intrinsic: '_mm256_permutexvar_ps'.
// Requires AVX512F.
func PermutexvarPs(idx M256i, a M256) M256 {
	return M256(permutexvarPs([32]byte(idx), [8]float32(a)))
}

func permutexvarPs(idx [32]byte, a [8]float32) [8]float32


// PowPd: Compute the exponential value of packed double-precision (64-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := (a[i+63:i])^(b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_pow_pd'.
// Requires AVX.
func PowPd(a M256d, b M256d) M256d {
	return M256d(powPd([4]float64(a), [4]float64(b)))
}

func powPd(a [4]float64, b [4]float64) [4]float64


// PowPs: Compute the exponential value of packed single-precision (32-bit)
// floating-point elements in 'a' raised by packed elements in 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := (a[i+31:i])^(b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_pow_ps'.
// Requires AVX.
func PowPs(a M256, b M256) M256 {
	return M256(powPs([8]float32(a), [8]float32(b)))
}

func powPs(a [8]float32, b [8]float32) [8]float32


// MaskRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm256_mask_range_pd'.
// Requires AVX512DQ.
func MaskRangePd(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskRangePd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskRangePd(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzRangePd: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm256_maskz_range_pd'.
// Requires AVX512DQ.
func MaskzRangePd(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzRangePd(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzRangePd(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// RangePd: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed double-precision (64-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[63:0], src2[63:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src1[63:0] : src2[63:0]
//			1: tmp[63:0] := (src1[63:0] <= src2[63:0]) ? src2[63:0] : src1[63:0]
//			2: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src1[63:0] : src2[63:0]
//			3: tmp[63:0] := (ABS(src1[63:0]) <= ABS(src2[63:0])) ? src2[63:0] : src1[63:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[63:0] := (src1[63] << 63) OR (tmp[62:0])
//			1: dst[63:0] := tmp[63:0]
//			2: dst[63:0] := (0 << 63) OR (tmp[62:0])
//			3: dst[63:0] := (1 << 63) OR (tmp[62:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RANGE(a[i+63:i], b[i+63:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPD'. Intrinsic: '_mm256_range_pd'.
// Requires AVX512DQ.
func RangePd(a M256d, b M256d, imm8 int) M256d {
	return M256d(rangePd([4]float64(a), [4]float64(b), imm8))
}

func rangePd(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm256_mask_range_ps'.
// Requires AVX512DQ.
func MaskRangePs(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskRangePs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskRangePs(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzRangePs: Calculate the max, min, absolute max, or absolute min
// (depending on control in 'imm8') for packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set).
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm256_maskz_range_ps'.
// Requires AVX512DQ.
func MaskzRangePs(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzRangePs(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzRangePs(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// RangePs: Calculate the max, min, absolute max, or absolute min (depending on
// control in 'imm8') for packed single-precision (32-bit) floating-point
// elements in 'a' and 'b', and store the results in 'dst'.
// 	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 =
// absolute max, 11 = absolute min.
// 	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from
// compare result, 10 = clear sign bit, 11 = set sign bit. 
//
//		RANGE(src1[31:0], src2[31:0], opCtl[1:0], signSelCtl[1:0])
//		{
//			CASE opCtl[1:0]
//			0: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src1[31:0] : src2[31:0]
//			1: tmp[31:0] := (src1[31:0] <= src2[31:0]) ? src2[31:0] : src1[31:0]
//			2: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src1[31:0] : src2[31:0]
//			3: tmp[31:0] := (ABS(src1[31:0]) <= ABS(src2[31:0])) ? src2[31:0] : src1[31:0]
//			ESAC
//			
//			CASE signSelCtl[1:0]
//			0: dst[31:0] := (src1[31] << 31) OR (tmp[30:0])
//			1: dst[31:0] := tmp[63:0]
//			2: dst[31:0] := (0 << 31) OR (tmp[30:0])
//			3: dst[31:0] := (1 << 31) OR (tmp[30:0])
//			ESAC
//			
//			RETURN dst
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RANGE(a[i+31:i], b[i+31:i], imm8[1:0], imm8[3:2])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRANGEPS'. Intrinsic: '_mm256_range_ps'.
// Requires AVX512DQ.
func RangePs(a M256, b M256, imm8 int) M256 {
	return M256(rangePs([8]float32(a), [8]float32(b), imm8))
}

func rangePs(a [8]float32, b [8]float32, imm8 int) [8]float32


// RcpPs: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 1.5*2^-12. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCPPS'. Intrinsic: '_mm256_rcp_ps'.
// Requires AVX.
func RcpPs(a M256) M256 {
	return M256(rcpPs([8]float32(a)))
}

func rcpPs(a [8]float32) [8]float32


// MaskRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_mask_rcp14_pd'.
// Requires AVX512F.
func MaskRcp14Pd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskRcp14Pd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskRcp14Pd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzRcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_maskz_rcp14_pd'.
// Requires AVX512F.
func MaskzRcp14Pd(k Mmask8, a M256d) M256d {
	return M256d(maskzRcp14Pd(uint8(k), [4]float64(a)))
}

func maskzRcp14Pd(k uint8, a [4]float64) [4]float64


// Rcp14Pd: Compute the approximate reciprocal of packed double-precision
// (64-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := APPROXIMATE(1.0/a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PD'. Intrinsic: '_mm256_rcp14_pd'.
// Requires AVX512F.
func Rcp14Pd(a M256d) M256d {
	return M256d(rcp14Pd([4]float64(a)))
}

func rcp14Pd(a [4]float64) [4]float64


// MaskRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using writemask 'k' (elements are copied from 'src' when the corresponding
// mask bit is not set). The maximum relative error for this approximation is
// less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_mask_rcp14_ps'.
// Requires AVX512F.
func MaskRcp14Ps(src M256, k Mmask8, a M256) M256 {
	return M256(maskRcp14Ps([8]float32(src), uint8(k), [8]float32(a)))
}

func maskRcp14Ps(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzRcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'
// using zeromask 'k' (elements are zeroed out when the corresponding mask bit
// is not set). The maximum relative error for this approximation is less than
// 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_maskz_rcp14_ps'.
// Requires AVX512F.
func MaskzRcp14Ps(k Mmask8, a M256) M256 {
	return M256(maskzRcp14Ps(uint8(k), [8]float32(a)))
}

func maskzRcp14Ps(k uint8, a [8]float32) [8]float32


// Rcp14Ps: Compute the approximate reciprocal of packed single-precision
// (32-bit) floating-point elements in 'a', and store the results in 'dst'. The
// maximum relative error for this approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0/a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRCP14PS'. Intrinsic: '_mm256_rcp14_ps'.
// Requires AVX512F.
func Rcp14Ps(a M256) M256 {
	return M256(rcp14Ps([8]float32(a)))
}

func rcp14Ps(a [8]float32) [8]float32


// MaskReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm256_mask_reduce_pd'.
// Requires AVX512DQ.
func MaskReducePd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskReducePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskReducePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzReducePd: Extract the reduced argument of packed double-precision
// (64-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm256_maskz_reduce_pd'.
// Requires AVX512DQ.
func MaskzReducePd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzReducePd(uint8(k), [4]float64(a), imm8))
}

func maskzReducePd(k uint8, a [4]float64, imm8 int) [4]float64


// ReducePd: Extract the reduced argument of packed double-precision (64-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPD(src1[63:0], imm8[7:0])
//		{
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[63:0] := pow(2, -m) * ROUND(pow(2, m) * src1[63:0], spe, rc_source, rc)
//			tmp[63:0] := src1[63:0] - tmp[63:0]
//			RETURN tmp[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ReduceArgumentPD(src[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPD'. Intrinsic: '_mm256_reduce_pd'.
// Requires AVX512DQ.
func ReducePd(a M256d, imm8 int) M256d {
	return M256d(reducePd([4]float64(a), imm8))
}

func reducePd(a [4]float64, imm8 int) [4]float64


// MaskReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm256_mask_reduce_ps'.
// Requires AVX512DQ.
func MaskReducePs(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskReducePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskReducePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzReducePs: Extract the reduced argument of packed single-precision
// (32-bit) floating-point elements in 'a' by the number of bits specified by
// 'imm8', and store the results in 'dst' using zeromask 'k' (elements are
// zeroed out when the corresponding mask bit is not set). 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm256_maskz_reduce_ps'.
// Requires AVX512DQ.
func MaskzReducePs(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzReducePs(uint8(k), [8]float32(a), imm8))
}

func maskzReducePs(k uint8, a [8]float32, imm8 int) [8]float32


// ReducePs: Extract the reduced argument of packed single-precision (32-bit)
// floating-point elements in 'a' by the number of bits specified by 'imm8',
// and store the results in 'dst'. 
//
//		ReduceArgumentPS(src1[31:0], imm8[7:0])
//		{
//			IF src1[31:0] == NAN
//				RETURN (convert src1[31:0] to QNaN)
//			FI
//			
//			m := imm8[7:4] // number of fraction bits after the binary point to be preserved
//			rc := imm8[1:0] // round control
//			rc_src := imm8[2] // round ccontrol source
//			spe := 0
//			tmp[31:0] := pow(2, -m)*ROUND(pow(2, m)*src1[31:0], spe, rc_source, rc)
//			tmp[31:0] := src1[31:0] - tmp[31:0]
//			RETURN tmp[31:0]
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ReduceArgumentPS(src[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VREDUCEPS'. Intrinsic: '_mm256_reduce_ps'.
// Requires AVX512DQ.
func ReducePs(a M256, imm8 int) M256 {
	return M256(reducePs([8]float32(a), imm8))
}

func reducePs(a [8]float32, imm8 int) [8]float32


// RemEpi16: Divide packed 16-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi16'.
// Requires AVX.
func RemEpi16(a M256i, b M256i) M256i {
	return M256i(remEpi16([32]byte(a), [32]byte(b)))
}

func remEpi16(a [32]byte, b [32]byte) [32]byte


// RemEpi32: Divide packed 32-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi32'.
// Requires AVX.
func RemEpi32(a M256i, b M256i) M256i {
	return M256i(remEpi32([32]byte(a), [32]byte(b)))
}

func remEpi32(a [32]byte, b [32]byte) [32]byte


// RemEpi64: Divide packed 64-bit integers in 'a' by packed elements in 'b',
// and store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi64'.
// Requires AVX.
func RemEpi64(a M256i, b M256i) M256i {
	return M256i(remEpi64([32]byte(a), [32]byte(b)))
}

func remEpi64(a [32]byte, b [32]byte) [32]byte


// RemEpi8: Divide packed 8-bit integers in 'a' by packed elements in 'b', and
// store the remainders as packed 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epi8'.
// Requires AVX.
func RemEpi8(a M256i, b M256i) M256i {
	return M256i(remEpi8([32]byte(a), [32]byte(b)))
}

func remEpi8(a [32]byte, b [32]byte) [32]byte


// RemEpu16: Divide packed unsigned 16-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := 16*j
//			dst[i+15:i] := REMAINDER(a[i+15:i] / b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu16'.
// Requires AVX.
func RemEpu16(a M256i, b M256i) M256i {
	return M256i(remEpu16([32]byte(a), [32]byte(b)))
}

func remEpu16(a [32]byte, b [32]byte) [32]byte


// RemEpu32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu32'.
// Requires AVX.
func RemEpu32(a M256i, b M256i) M256i {
	return M256i(remEpu32([32]byte(a), [32]byte(b)))
}

func remEpu32(a [32]byte, b [32]byte) [32]byte


// RemEpu64: Divide packed unsigned 64-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 3
//			i := 64*j
//			dst[i+63:i] := REMAINDER(a[i+63:i] / b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu64'.
// Requires AVX.
func RemEpu64(a M256i, b M256i) M256i {
	return M256i(remEpu64([32]byte(a), [32]byte(b)))
}

func remEpu64(a [32]byte, b [32]byte) [32]byte


// RemEpu8: Divide packed unsigned 8-bit integers in 'a' by packed elements in
// 'b', and store the remainders as packed unsigned 32-bit integers in 'dst'. 
//
//		FOR j := 0 to 31
//			i := 8*j
//			dst[i+7:i] := REMAINDER(a[i+7:i] / b[i+7:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_rem_epu8'.
// Requires AVX.
func RemEpu8(a M256i, b M256i) M256i {
	return M256i(remEpu8([32]byte(a), [32]byte(b)))
}

func remEpu8(a [32]byte, b [32]byte) [32]byte


// MaskRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_mask_rol_epi32'.
// Requires AVX512F.
func MaskRolEpi32(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRolEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRolEpi32(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_maskz_rol_epi32'.
// Requires AVX512F.
func MaskzRolEpi32(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRolEpi32(uint8(k), [32]byte(a), imm8))
}

func maskzRolEpi32(k uint8, a [32]byte, imm8 int) [32]byte


// RolEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLD'. Intrinsic: '_mm256_rol_epi32'.
// Requires AVX512F.
func RolEpi32(a M256i, imm8 int) M256i {
	return M256i(rolEpi32([32]byte(a), imm8))
}

func rolEpi32(a [32]byte, imm8 int) [32]byte


// MaskRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_mask_rol_epi64'.
// Requires AVX512F.
func MaskRolEpi64(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRolEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRolEpi64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_maskz_rol_epi64'.
// Requires AVX512F.
func MaskzRolEpi64(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRolEpi64(uint8(k), [32]byte(a), imm8))
}

func maskzRolEpi64(k uint8, a [32]byte, imm8 int) [32]byte


// RolEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLQ'. Intrinsic: '_mm256_rol_epi64'.
// Requires AVX512F.
func RolEpi64(a M256i, imm8 int) M256i {
	return M256i(rolEpi64([32]byte(a), imm8))
}

func rolEpi64(a [32]byte, imm8 int) [32]byte


// MaskRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_mask_rolv_epi32'.
// Requires AVX512F.
func MaskRolvEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRolvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRolvEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_maskz_rolv_epi32'.
// Requires AVX512F.
func MaskzRolvEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRolvEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRolvEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// RolvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src << count) OR (src >> (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := LEFT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVD'. Intrinsic: '_mm256_rolv_epi32'.
// Requires AVX512F.
func RolvEpi32(a M256i, b M256i) M256i {
	return M256i(rolvEpi32([32]byte(a), [32]byte(b)))
}

func rolvEpi32(a [32]byte, b [32]byte) [32]byte


// MaskRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_mask_rolv_epi64'.
// Requires AVX512F.
func MaskRolvEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRolvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRolvEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// left by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_maskz_rolv_epi64'.
// Requires AVX512F.
func MaskzRolvEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRolvEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRolvEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// RolvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the left
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		LEFT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src << count) OR (src >> (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := LEFT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPROLVQ'. Intrinsic: '_mm256_rolv_epi64'.
// Requires AVX512F.
func RolvEpi64(a M256i, b M256i) M256i {
	return M256i(rolvEpi64([32]byte(a), [32]byte(b)))
}

func rolvEpi64(a [32]byte, b [32]byte) [32]byte


// MaskRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_mask_ror_epi32'.
// Requires AVX512F.
func MaskRorEpi32(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRorEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRorEpi32(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_maskz_ror_epi32'.
// Requires AVX512F.
func MaskzRorEpi32(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRorEpi32(uint8(k), [32]byte(a), imm8))
}

func maskzRorEpi32(k uint8, a [32]byte, imm8 int) [32]byte


// RorEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORD'. Intrinsic: '_mm256_ror_epi32'.
// Requires AVX512F.
func RorEpi32(a M256i, imm8 int) M256i {
	return M256i(rorEpi32([32]byte(a), imm8))
}

func rorEpi32(a [32]byte, imm8 int) [32]byte


// MaskRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_mask_ror_epi64'.
// Requires AVX512F.
func MaskRorEpi64(src M256i, k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskRorEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskRorEpi64(src [32]byte, k uint8, a [32]byte, imm8 int) [32]byte


// MaskzRorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_maskz_ror_epi64'.
// Requires AVX512F.
func MaskzRorEpi64(k Mmask8, a M256i, imm8 int) M256i {
	return M256i(maskzRorEpi64(uint8(k), [32]byte(a), imm8))
}

func maskzRorEpi64(k uint8, a [32]byte, imm8 int) [32]byte


// RorEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in 'imm8', and store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORQ'. Intrinsic: '_mm256_ror_epi64'.
// Requires AVX512F.
func RorEpi64(a M256i, imm8 int) M256i {
	return M256i(rorEpi64([32]byte(a), imm8))
}

func rorEpi64(a [32]byte, imm8 int) [32]byte


// MaskRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_mask_rorv_epi32'.
// Requires AVX512F.
func MaskRorvEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRorvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRorvEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_maskz_rorv_epi32'.
// Requires AVX512F.
func MaskzRorvEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRorvEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRorvEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// RorvEpi32: Rotate the bits in each packed 32-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_DWORDS(src, count_src){
//			count := count_src modulo 32
//			RETURN (src >>count) OR (src << (32 - count))
//		}
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RIGHT_ROTATE_DWORDS(a[i+31:i], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVD'. Intrinsic: '_mm256_rorv_epi32'.
// Requires AVX512F.
func RorvEpi32(a M256i, b M256i) M256i {
	return M256i(rorvEpi32([32]byte(a), [32]byte(b)))
}

func rorvEpi32(a [32]byte, b [32]byte) [32]byte


// MaskRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_mask_rorv_epi64'.
// Requires AVX512F.
func MaskRorvEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskRorvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskRorvEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzRorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the
// right by the number of bits specified in the corresponding element of 'b',
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_maskz_rorv_epi64'.
// Requires AVX512F.
func MaskzRorvEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzRorvEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzRorvEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// RorvEpi64: Rotate the bits in each packed 64-bit integer in 'a' to the right
// by the number of bits specified in the corresponding element of 'b', and
// store the results in 'dst'. 
//
//		RIGHT_ROTATE_QWORDS(src, count_src){
//			count := count_src modulo 64
//			RETURN (src >> count) OR (src << (64 - count))
//		}
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RIGHT_ROTATE_QWORDS(a[i+63:i], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPRORVQ'. Intrinsic: '_mm256_rorv_epi64'.
// Requires AVX512F.
func RorvEpi64(a M256i, b M256i) M256i {
	return M256i(rorvEpi64([32]byte(a), [32]byte(b)))
}

func rorvEpi64(a [32]byte, b [32]byte) [32]byte


// RoundPd: Round the packed double-precision (64-bit) floating-point elements
// in 'a' using the 'rounding' parameter, and store the results as packed
// double-precision floating-point elements in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPD'. Intrinsic: '_mm256_round_pd'.
// Requires AVX.
func RoundPd(a M256d, rounding int) M256d {
	return M256d(roundPd([4]float64(a), rounding))
}

func roundPd(a [4]float64, rounding int) [4]float64


// RoundPs: Round the packed single-precision (32-bit) floating-point elements
// in 'a' using the 'rounding' parameter, and store the results as packed
// single-precision floating-point elements in 'dst'.
// 	Rounding is done according to the 'rounding' parameter, which can be one
// of:
//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
//     _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VROUNDPS'. Intrinsic: '_mm256_round_ps'.
// Requires AVX.
func RoundPs(a M256, rounding int) M256 {
	return M256(roundPs([8]float32(a), rounding))
}

func roundPs(a [8]float32, rounding int) [8]float32


// MaskRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_mask_roundscale_pd'.
// Requires AVX512F.
func MaskRoundscalePd(src M256d, k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskRoundscalePd([4]float64(src), uint8(k), [4]float64(a), imm8))
}

func maskRoundscalePd(src [4]float64, k uint8, a [4]float64, imm8 int) [4]float64


// MaskzRoundscalePd: Round packed double-precision (64-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_maskz_roundscale_pd'.
// Requires AVX512F.
func MaskzRoundscalePd(k Mmask8, a M256d, imm8 int) M256d {
	return M256d(maskzRoundscalePd(uint8(k), [4]float64(a), imm8))
}

func maskzRoundscalePd(k uint8, a [4]float64, imm8 int) [4]float64


// RoundscalePd: Round packed double-precision (64-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPD(src[63:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[63:0] := round_to_nearest_even_integer(2^M * src[63:0])
//			1: tmp[63:0] := round_to_equal_or_smaller_integer(2^M * src[63:0])
//			2: tmp[63:0] := round_to_equal_or_larger_integer(2^M * src[63:0])
//			3: tmp[63:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[63:0])
//			ESAC
//			
//			dst[63:0] := 2^-M * tmp[63:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[63:0] != dst[63:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[63:0]
//		}	
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := RoundTo_IntegerPD(a[i+63:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPD'. Intrinsic: '_mm256_roundscale_pd'.
// Requires AVX512F.
func RoundscalePd(a M256d, imm8 int) M256d {
	return M256d(roundscalePd([4]float64(a), imm8))
}

func roundscalePd(a [4]float64, imm8 int) [4]float64


// MaskRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_mask_roundscale_ps'.
// Requires AVX512F.
func MaskRoundscalePs(src M256, k Mmask8, a M256, imm8 int) M256 {
	return M256(maskRoundscalePs([8]float32(src), uint8(k), [8]float32(a), imm8))
}

func maskRoundscalePs(src [8]float32, k uint8, a [8]float32, imm8 int) [8]float32


// MaskzRoundscalePs: Round packed single-precision (32-bit) floating-point
// elements in 'a' to the number of fraction bits specified by 'imm8', and
// store the results in 'dst' using zeromask 'k' (elements are zeroed out when
// the corresponding mask bit is not set). 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_maskz_roundscale_ps'.
// Requires AVX512F.
func MaskzRoundscalePs(k Mmask8, a M256, imm8 int) M256 {
	return M256(maskzRoundscalePs(uint8(k), [8]float32(a), imm8))
}

func maskzRoundscalePs(k uint8, a [8]float32, imm8 int) [8]float32


// RoundscalePs: Round packed single-precision (32-bit) floating-point elements
// in 'a' to the number of fraction bits specified by 'imm8', and store the
// results in 'dst'. 
//
//		RoundTo_IntegerPS(src[31:0], imm8[7:0]){
//			IF(imm8[2] == 1)
//				rounding_direction := MXCSR.RC //Use the rounding mode specified by MXCSR.RC
//			ELSE
//				rounding_direction := imm8[1:0] //Use the rounding mode specified by imm8[1:0]
//			FI
//			
//			M := imm8[7:4] // The scaling factor (number of fraction bits to round to)
//			
//			CASE(rounding_direction)
//			0: tmp[31:0] := round_to_nearest_even_integer(2^M * src[31:0])
//			1: tmp[31:0] := round_to_equal_or_smaller_integer(2^M * src[31:0])
//			2: tmp[31:0] := round_to_equal_or_larger_integer(2^M * src[31:0])
//			3: tmp[31:0] := round_to_nearest_smallest_magnitude_integer(2^M * src[31:0])
//			ESAC
//			
//			dst[31:0] := 2^-M * tmp[31:0] // scale back down
//			
//			IF imm8[3] == 0 //check SPE
//				IF src[31:0] != dst[31:0] //check if precision has been lost
//					set_precision() //set #PE
//				FI
//			FI
//			RETURN dst[31:0]
//		}	
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := RoundTo_IntegerPS(a[i+31:i], imm8[7:0])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRNDSCALEPS'. Intrinsic: '_mm256_roundscale_ps'.
// Requires AVX512F.
func RoundscalePs(a M256, imm8 int) M256 {
	return M256(roundscalePs([8]float32(a), imm8))
}

func roundscalePs(a [8]float32, imm8 int) [8]float32


// RsqrtPs: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst'. The maximum relative error for this approximation is less
// than 1.5*2^-12. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRTPS'. Intrinsic: '_mm256_rsqrt_ps'.
// Requires AVX.
func RsqrtPs(a M256) M256 {
	return M256(rsqrtPs([8]float32(a)))
}

func rsqrtPs(a [8]float32) [8]float32


// MaskRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_mask_rsqrt14_pd'.
// Requires AVX512F.
func MaskRsqrt14Pd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskRsqrt14Pd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskRsqrt14Pd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzRsqrt14Pd: Compute the approximate reciprocal square root of packed
// double-precision (64-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := APPROXIMATE(1.0 / SQRT(a[i+63:i]))
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PD'. Intrinsic: '_mm256_maskz_rsqrt14_pd'.
// Requires AVX512F.
func MaskzRsqrt14Pd(k Mmask8, a M256d) M256d {
	return M256d(maskzRsqrt14Pd(uint8(k), [4]float64(a)))
}

func maskzRsqrt14Pd(k uint8, a [4]float64) [4]float64


// MaskRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_mask_rsqrt14_ps'.
// Requires AVX512F.
func MaskRsqrt14Ps(src M256, k Mmask8, a M256) M256 {
	return M256(maskRsqrt14Ps([8]float32(src), uint8(k), [8]float32(a)))
}

func maskRsqrt14Ps(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzRsqrt14Ps: Compute the approximate reciprocal square root of packed
// single-precision (32-bit) floating-point elements in 'a', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). The maximum relative error for this
// approximation is less than 2^-14. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := APPROXIMATE(1.0 / SQRT(a[i+31:i]))
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VRSQRT14PS'. Intrinsic: '_mm256_maskz_rsqrt14_ps'.
// Requires AVX512F.
func MaskzRsqrt14Ps(k Mmask8, a M256) M256 {
	return M256(maskzRsqrt14Ps(uint8(k), [8]float32(a)))
}

func maskzRsqrt14Ps(k uint8, a [8]float32) [8]float32


// SadEpu8: Compute the absolute differences of packed unsigned 8-bit integers
// in 'a' and 'b', then horizontally sum each consecutive 8 differences to
// produce four unsigned 16-bit integers, and pack these unsigned 16-bit
// integers in the low 16 bits of 64-bit elements in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			tmp[i+7:i] := ABS(a[i+7:i] - b[i+7:i])
//		ENDFOR
//		FOR j := 0 to 4
//			i := j*64
//			dst[i+15:i] := tmp[i+7:i] + tmp[i+15:i+8] + tmp[i+23:i+16] + tmp[i+31:i+24] + tmp[i+39:i+32] + tmp[i+47:i+40] + tmp[i+55:i+48] + tmp[i+63:i+56]
//			dst[i+63:i+16] := 0
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSADBW'. Intrinsic: '_mm256_sad_epu8'.
// Requires AVX2.
func SadEpu8(a M256i, b M256i) M256i {
	return M256i(sadEpu8([32]byte(a), [32]byte(b)))
}

func sadEpu8(a [32]byte, b [32]byte) [32]byte


// MaskScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_mask_scalef_pd'.
// Requires AVX512F.
func MaskScalefPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskScalefPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskScalefPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzScalefPd: Scale the packed double-precision (64-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_maskz_scalef_pd'.
// Requires AVX512F.
func MaskzScalefPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzScalefPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzScalefPd(k uint8, a [4]float64, b [4]float64) [4]float64


// ScalefPd: Scale the packed double-precision (64-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[63:0] := tmp_src1[63:0] * POW(2, FLOOR(tmp_src2[63:0]))
//			RETURN dst[63:0]
//		}
//		
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SCALE(a[i+63:0], b[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPD'. Intrinsic: '_mm256_scalef_pd'.
// Requires AVX512F.
func ScalefPd(a M256d, b M256d) M256d {
	return M256d(scalefPd([4]float64(a), [4]float64(b)))
}

func scalefPd(a [4]float64, b [4]float64) [4]float64


// MaskScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_mask_scalef_ps'.
// Requires AVX512F.
func MaskScalefPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskScalefPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskScalefPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzScalefPs: Scale the packed single-precision (32-bit) floating-point
// elements in 'a' using values from 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_maskz_scalef_ps'.
// Requires AVX512F.
func MaskzScalefPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzScalefPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzScalefPs(k uint8, a [8]float32, b [8]float32) [8]float32


// ScalefPs: Scale the packed single-precision (32-bit) floating-point elements
// in 'a' using values from 'b', and store the results in 'dst'. 
//
//		SCALE(src1, src2){
//			IF (src2 == NaN)
//				IF (src2 == SNaN)
//					RETURN QNAN(src2)
//				FI
//			ELSE IF (src1 == NaN)
//				IF (src1 == SNaN)
//					RETURN QNAN(src1)
//				FI
//				IF (src2 != INF)
//					RETURN QNAN(src1)
//				FI
//			ELSE
//				tmp_src2 := src2
//				tmp_src1 := src1
//				IF (src2 is denormal AND MXCSR.DAZ)
//					tmp_src2 := 0
//				FI
//				IF (src1 is denormal AND MXCSR.DAZ)
//					tmp_src1 := 0
//				FI
//			FI
//			dst[31:0] := tmp_src1[31:0] * POW(2, FLOOR(tmp_src2[31:0]))
//			RETURN dst[31:0]
//		}
//		
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SCALE(a[i+31:0], b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSCALEFPS'. Intrinsic: '_mm256_scalef_ps'.
// Requires AVX512F.
func ScalefPs(a M256, b M256) M256 {
	return M256(scalefPs([8]float32(a), [8]float32(b)))
}

func scalefPs(a [8]float32, b [8]float32) [8]float32


// SetEpi16: Set packed 16-bit integers in 'dst' with the supplied values. 
//
//		dst[15:0] := e0
//		dst[31:16] := e1
//		dst[47:32] := e2
//		dst[63:48] := e3
//		dst[79:64] := e4
//		dst[95:80] := e5
//		dst[111:96] := e6
//		dst[127:112] := e7
//		dst[145:128] := e8
//		dst[159:144] := e9
//		dst[175:160] := e10
//		dst[191:176] := e11
//		dst[207:192] := e12
//		dst[223:208] := e13
//		dst[239:224] := e14
//		dst[255:240] := e15
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi16'.
// Requires AVX.
func SetEpi16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) M256i {
	return M256i(setEpi16(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setEpi16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) [32]byte


// SetEpi32: Set packed 32-bit integers in 'dst' with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi32'.
// Requires AVX.
func SetEpi32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M256i {
	return M256i(setEpi32(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setEpi32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [32]byte


// SetEpi64x: Set packed 64-bit integers in 'dst' with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi64x'.
// Requires AVX.
func SetEpi64x(e3 int64, e2 int64, e1 int64, e0 int64) M256i {
	return M256i(setEpi64x(e3, e2, e1, e0))
}

func setEpi64x(e3 int64, e2 int64, e1 int64, e0 int64) [32]byte


// SetEpi8: Set packed 8-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[7:0] := e0
//		dst[15:8] := e1
//		dst[23:16] := e2
//		dst[31:24] := e3
//		dst[39:32] := e4
//		dst[47:40] := e5
//		dst[55:48] := e6
//		dst[63:56] := e7
//		dst[71:64] := e8
//		dst[79:72] := e9
//		dst[87:80] := e10
//		dst[95:88] := e11
//		dst[103:96] := e12
//		dst[111:104] := e13
//		dst[119:112] := e14
//		dst[127:120] := e15
//		dst[135:128] := e16
//		dst[143:136] := e17
//		dst[151:144] := e18
//		dst[159:152] := e19
//		dst[167:160] := e20
//		dst[175:168] := e21
//		dst[183:176] := e22
//		dst[191:184] := e23
//		dst[199:192] := e24
//		dst[207:200] := e25
//		dst[215:208] := e26
//		dst[223:216] := e27
//		dst[231:224] := e28
//		dst[239:232] := e29
//		dst[247:240] := e30
//		dst[255:248] := e31
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_epi8'.
// Requires AVX.
func SetEpi8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) M256i {
	return M256i(setEpi8(e31, e30, e29, e28, e27, e26, e25, e24, e23, e22, e21, e20, e19, e18, e17, e16, e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setEpi8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) [32]byte


// SetM128: Set packed __m256 vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_set_m128'.
// Requires AVX.
func SetM128(hi M128, lo M128) M256 {
	return M256(setM128([4]float32(hi), [4]float32(lo)))
}

func setM128(hi [4]float32, lo [4]float32) [8]float32


// SetM128d: Set packed __m256d vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_set_m128d'.
// Requires AVX.
func SetM128d(hi M128d, lo M128d) M256d {
	return M256d(setM128d([2]float64(hi), [2]float64(lo)))
}

func setM128d(hi [2]float64, lo [2]float64) [4]float64


// SetM128i: Set packed __m256i vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_set_m128i'.
// Requires AVX.
func SetM128i(hi M128i, lo M128i) M256i {
	return M256i(setM128i([16]byte(hi), [16]byte(lo)))
}

func setM128i(hi [16]byte, lo [16]byte) [32]byte


// SetPd: Set packed double-precision (64-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[63:0] := e0
//		dst[127:64] := e1
//		dst[191:128] := e2
//		dst[255:192] := e3
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_pd'.
// Requires AVX.
func SetPd(e3 float64, e2 float64, e1 float64, e0 float64) M256d {
	return M256d(setPd(e3, e2, e1, e0))
}

func setPd(e3 float64, e2 float64, e1 float64, e0 float64) [4]float64


// SetPs: Set packed single-precision (32-bit) floating-point elements in 'dst'
// with the supplied values. 
//
//		dst[31:0] := e0
//		dst[63:32] := e1
//		dst[95:64] := e2
//		dst[127:96] := e3
//		dst[159:128] := e4
//		dst[191:160] := e5
//		dst[223:192] := e6
//		dst[255:224] := e7
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set_ps'.
// Requires AVX.
func SetPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M256 {
	return M256(setPs(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [8]float32


// MaskSet1Epi16: Broadcast the low packed 16-bit integer from 'a' to all
// elements of 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_mask_set1_epi16'.
// Requires AVX512BW.
func MaskSet1Epi16(src M256i, k Mmask16, a int16) M256i {
	return M256i(maskSet1Epi16([32]byte(src), uint16(k), a))
}

func maskSet1Epi16(src [32]byte, k uint16, a int16) [32]byte


// MaskzSet1Epi16: Broadcast 16-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[15:0]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTW'. Intrinsic: '_mm256_maskz_set1_epi16'.
// Requires AVX512BW.
func MaskzSet1Epi16(k Mmask16, a int16) M256i {
	return M256i(maskzSet1Epi16(uint16(k), a))
}

func maskzSet1Epi16(k uint16, a int16) [32]byte


// Set1Epi16: Broadcast 16-bit integer 'a' to all all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastw'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[15:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi16'.
// Requires AVX.
func Set1Epi16(a int16) M256i {
	return M256i(set1Epi16(a))
}

func set1Epi16(a int16) [32]byte


// MaskSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_mask_set1_epi32'.
// Requires AVX512F.
func MaskSet1Epi32(src M256i, k Mmask8, a int) M256i {
	return M256i(maskSet1Epi32([32]byte(src), uint8(k), a))
}

func maskSet1Epi32(src [32]byte, k uint8, a int) [32]byte


// MaskzSet1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[31:0]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTD'. Intrinsic: '_mm256_maskz_set1_epi32'.
// Requires AVX512F.
func MaskzSet1Epi32(k Mmask8, a int) M256i {
	return M256i(maskzSet1Epi32(uint8(k), a))
}

func maskzSet1Epi32(k uint8, a int) [32]byte


// Set1Epi32: Broadcast 32-bit integer 'a' to all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastd'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi32'.
// Requires AVX.
func Set1Epi32(a int) M256i {
	return M256i(set1Epi32(a))
}

func set1Epi32(a int) [32]byte


// MaskSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_mask_set1_epi64'.
// Requires AVX512F.
func MaskSet1Epi64(src M256i, k Mmask8, a int64) M256i {
	return M256i(maskSet1Epi64([32]byte(src), uint8(k), a))
}

func maskSet1Epi64(src [32]byte, k uint8, a int64) [32]byte


// MaskzSet1Epi64: Broadcast 64-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[63:0]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTQ'. Intrinsic: '_mm256_maskz_set1_epi64'.
// Requires AVX512F.
func MaskzSet1Epi64(k Mmask8, a int64) M256i {
	return M256i(maskzSet1Epi64(uint8(k), a))
}

func maskzSet1Epi64(k uint8, a int64) [32]byte


// Set1Epi64x: Broadcast 64-bit integer 'a' to all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastq'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi64x'.
// Requires AVX.
func Set1Epi64x(a int64) M256i {
	return M256i(set1Epi64x(a))
}

func set1Epi64x(a int64) [32]byte


// MaskSet1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_mask_set1_epi8'.
// Requires AVX512BW.
func MaskSet1Epi8(src M256i, k Mmask32, a byte) M256i {
	return M256i(maskSet1Epi8([32]byte(src), uint32(k), a))
}

func maskSet1Epi8(src [32]byte, k uint32, a byte) [32]byte


// MaskzSet1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[7:0]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPBROADCASTB'. Intrinsic: '_mm256_maskz_set1_epi8'.
// Requires AVX512BW.
func MaskzSet1Epi8(k Mmask32, a byte) M256i {
	return M256i(maskzSet1Epi8(uint32(k), a))
}

func maskzSet1Epi8(k uint32, a byte) [32]byte


// Set1Epi8: Broadcast 8-bit integer 'a' to all elements of 'dst'. This
// intrinsic may generate the 'vpbroadcastb'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[7:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_epi8'.
// Requires AVX.
func Set1Epi8(a byte) M256i {
	return M256i(set1Epi8(a))
}

func set1Epi8(a byte) [32]byte


// Set1Pd: Broadcast double-precision (64-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[63:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_pd'.
// Requires AVX.
func Set1Pd(a float64) M256d {
	return M256d(set1Pd(a))
}

func set1Pd(a float64) [4]float64


// Set1Ps: Broadcast single-precision (32-bit) floating-point value 'a' to all
// elements of 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[31:0]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_set1_ps'.
// Requires AVX.
func Set1Ps(a float32) M256 {
	return M256(set1Ps(a))
}

func set1Ps(a float32) [8]float32


// SetrEpi16: Set packed 16-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[15:0] := e15
//		dst[31:16] := e14
//		dst[47:32] := e13
//		dst[63:48] := e12
//		dst[79:64] := e11
//		dst[95:80] := e10
//		dst[111:96] := e9
//		dst[127:112] := e8
//		dst[145:128] := e7
//		dst[159:144] := e6
//		dst[175:160] := e5
//		dst[191:176] := e4
//		dst[207:192] := e3
//		dst[223:208] := e2
//		dst[239:224] := e1
//		dst[255:240] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi16'.
// Requires AVX.
func SetrEpi16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) M256i {
	return M256i(setrEpi16(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrEpi16(e15 int16, e14 int16, e13 int16, e12 int16, e11 int16, e10 int16, e9 int16, e8 int16, e7 int16, e6 int16, e5 int16, e4 int16, e3 int16, e2 int16, e1 int16, e0 int16) [32]byte


// SetrEpi32: Set packed 32-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[31:0] := e7
//		dst[63:32] := e6
//		dst[95:64] := e5
//		dst[127:96] := e4
//		dst[159:128] := e3
//		dst[191:160] := e2
//		dst[223:192] := e1
//		dst[255:224] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi32'.
// Requires AVX.
func SetrEpi32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) M256i {
	return M256i(setrEpi32(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrEpi32(e7 int, e6 int, e5 int, e4 int, e3 int, e2 int, e1 int, e0 int) [32]byte


// SetrEpi64x: Set packed 64-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[63:0] := e3
//		dst[127:64] := e2
//		dst[191:128] := e1
//		dst[255:192] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi64x'.
// Requires AVX.
func SetrEpi64x(e3 int64, e2 int64, e1 int64, e0 int64) M256i {
	return M256i(setrEpi64x(e3, e2, e1, e0))
}

func setrEpi64x(e3 int64, e2 int64, e1 int64, e0 int64) [32]byte


// SetrEpi8: Set packed 8-bit integers in 'dst' with the supplied values in
// reverse order. 
//
//		dst[7:0] := e31
//		dst[15:8] := e30
//		dst[23:16] := e29
//		dst[31:24] := e28
//		dst[39:32] := e27
//		dst[47:40] := e26
//		dst[55:48] := e25
//		dst[63:56] := e24
//		dst[71:64] := e23
//		dst[79:72] := e22
//		dst[87:80] := e21
//		dst[95:88] := e20
//		dst[103:96] := e19
//		dst[111:104] := e18
//		dst[119:112] := e17
//		dst[127:120] := e16
//		dst[135:128] := e15
//		dst[143:136] := e14
//		dst[151:144] := e13
//		dst[159:152] := e12
//		dst[167:160] := e11
//		dst[175:168] := e10
//		dst[183:176] := e9
//		dst[191:184] := e8
//		dst[199:192] := e7
//		dst[207:200] := e6
//		dst[215:208] := e5
//		dst[223:216] := e4
//		dst[231:224] := e3
//		dst[239:232] := e2
//		dst[247:240] := e1
//		dst[255:248] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_epi8'.
// Requires AVX.
func SetrEpi8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) M256i {
	return M256i(setrEpi8(e31, e30, e29, e28, e27, e26, e25, e24, e23, e22, e21, e20, e19, e18, e17, e16, e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrEpi8(e31 byte, e30 byte, e29 byte, e28 byte, e27 byte, e26 byte, e25 byte, e24 byte, e23 byte, e22 byte, e21 byte, e20 byte, e19 byte, e18 byte, e17 byte, e16 byte, e15 byte, e14 byte, e13 byte, e12 byte, e11 byte, e10 byte, e9 byte, e8 byte, e7 byte, e6 byte, e5 byte, e4 byte, e3 byte, e2 byte, e1 byte, e0 byte) [32]byte


// SetrM128: Set packed __m256 vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_setr_m128'.
// Requires AVX.
func SetrM128(lo M128, hi M128) M256 {
	return M256(setrM128([4]float32(lo), [4]float32(hi)))
}

func setrM128(lo [4]float32, hi [4]float32) [8]float32


// SetrM128d: Set packed __m256d vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_setr_m128d'.
// Requires AVX.
func SetrM128d(lo M128d, hi M128d) M256d {
	return M256d(setrM128d([2]float64(lo), [2]float64(hi)))
}

func setrM128d(lo [2]float64, hi [2]float64) [4]float64


// SetrM128i: Set packed __m256i vector 'dst' with the supplied values. 
//
//		dst[127:0] := lo[127:0]
//		dst[255:128] := hi[127:0]
//		dst[MAX:256] := 0
//
// Instruction: 'VINSERTF128'. Intrinsic: '_mm256_setr_m128i'.
// Requires AVX.
func SetrM128i(lo M128i, hi M128i) M256i {
	return M256i(setrM128i([16]byte(lo), [16]byte(hi)))
}

func setrM128i(lo [16]byte, hi [16]byte) [32]byte


// SetrPd: Set packed double-precision (64-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[63:0] := e3
//		dst[127:64] := e2
//		dst[191:128] := e1
//		dst[255:192] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_pd'.
// Requires AVX.
func SetrPd(e3 float64, e2 float64, e1 float64, e0 float64) M256d {
	return M256d(setrPd(e3, e2, e1, e0))
}

func setrPd(e3 float64, e2 float64, e1 float64, e0 float64) [4]float64


// SetrPs: Set packed single-precision (32-bit) floating-point elements in
// 'dst' with the supplied values in reverse order. 
//
//		dst[31:0] := e7
//		dst[63:32] := e6
//		dst[95:64] := e5
//		dst[127:96] := e4
//		dst[159:128] := e3
//		dst[191:160] := e2
//		dst[223:192] := e1
//		dst[255:224] := e0
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_setr_ps'.
// Requires AVX.
func SetrPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) M256 {
	return M256(setrPs(e7, e6, e5, e4, e3, e2, e1, e0))
}

func setrPs(e7 float32, e6 float32, e5 float32, e4 float32, e3 float32, e2 float32, e1 float32, e0 float32) [8]float32


// SetzeroPd: Return vector of type __m256d with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_setzero_pd'.
// Requires AVX.
func SetzeroPd() M256d {
	return M256d(setzeroPd())
}

func setzeroPd() [4]float64


// SetzeroPs: Return vector of type __m256 with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_setzero_ps'.
// Requires AVX.
func SetzeroPs() M256 {
	return M256(setzeroPs())
}

func setzeroPs() [8]float32


// SetzeroSi256: Return vector of type __m256i with all elements set to zero. 
//
//		dst[MAX:0] := 0
//
// Instruction: 'VPXOR'. Intrinsic: '_mm256_setzero_si256'.
// Requires AVX.
func SetzeroSi256() M256i {
	return M256i(setzeroSi256())
}

func setzeroSi256() [32]byte


// MaskShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes using
// the control in 'imm8', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_mask_shuffle_epi32'.
// Requires AVX512F.
func MaskShuffleEpi32(src M256i, k Mmask8, a M256i, imm8 MMPERMENUM) M256i {
	return M256i(maskShuffleEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskShuffleEpi32(src [32]byte, k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// MaskzShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes using
// the control in 'imm8', and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_maskz_shuffle_epi32'.
// Requires AVX512F.
func MaskzShuffleEpi32(k Mmask8, a M256i, imm8 MMPERMENUM) M256i {
	return M256i(maskzShuffleEpi32(uint8(k), [32]byte(a), imm8))
}

func maskzShuffleEpi32(k uint8, a [32]byte, imm8 MMPERMENUM) [32]byte


// ShuffleEpi32: Shuffle 32-bit integers in 'a' within 128-bit lanes using the
// control in 'imm8', and store the results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(a[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(a[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(a[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(a[255:128], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFD'. Intrinsic: '_mm256_shuffle_epi32'.
// Requires AVX2.
func ShuffleEpi32(a M256i, imm8 int) M256i {
	return M256i(shuffleEpi32([32]byte(a), imm8))
}

func shuffleEpi32(a [32]byte, imm8 int) [32]byte


// MaskShuffleEpi8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm256_mask_shuffle_epi8'.
// Requires AVX512BW.
func MaskShuffleEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskShuffleEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskShuffleEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzShuffleEpi8: Shuffle packed 8-bit integers in 'a' according to shuffle
// control mask in the corresponding 8-bit element of 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				IF b[i+7] == 1
//					dst[i+7:i] := 0
//				ELSE
//					index[3:0] := b[i+3:i]
//					dst[i+7:i] := a[index*8+7:index*8]
//				FI
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm256_maskz_shuffle_epi8'.
// Requires AVX512BW.
func MaskzShuffleEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzShuffleEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzShuffleEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// ShuffleEpi8: Shuffle 8-bit integers in 'a' within 128-bit lanes according to
// shuffle control mask in the corresponding 8-bit element of 'b', and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*8
//			IF b[i+7] == 1
//				dst[i+7:i] := 0
//			ELSE
//				index[3:0] := b[i+3:i]
//				dst[i+7:i] := a[index*8+7:index*8]
//			FI
//			IF b[128+i+7] == 1
//				dst[128+i+7:i] := 0
//			ELSE
//				index[3:0] := b[128+i+3:128+i]
//				dst[128+i+7:i] := a[128+index*8+7:128+index*8]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFB'. Intrinsic: '_mm256_shuffle_epi8'.
// Requires AVX2.
func ShuffleEpi8(a M256i, b M256i) M256i {
	return M256i(shuffleEpi8([32]byte(a), [32]byte(b)))
}

func shuffleEpi8(a [32]byte, b [32]byte) [32]byte


// MaskShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_mask_shuffle_f32x4'.
// Requires AVX512F.
func MaskShuffleF32x4(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskShuffleF32x4([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskShuffleF32x4(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_maskz_shuffle_f32x4'.
// Requires AVX512F.
func MaskzShuffleF32x4(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzShuffleF32x4(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzShuffleF32x4(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// ShuffleF32x4: Shuffle 128-bits (composed of 4 single-precision (32-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF32X4'. Intrinsic: '_mm256_shuffle_f32x4'.
// Requires AVX512F.
func ShuffleF32x4(a M256, b M256, imm8 int) M256 {
	return M256(shuffleF32x4([8]float32(a), [8]float32(b), imm8))
}

func shuffleF32x4(a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using writemask 'k' (elements are copied from 'src' when
// the corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_mask_shuffle_f64x2'.
// Requires AVX512F.
func MaskShuffleF64x2(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskShuffleF64x2([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskShuffleF64x2(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst' using zeromask 'k' (elements are zeroed out when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[1])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_maskz_shuffle_f64x2'.
// Requires AVX512F.
func MaskzShuffleF64x2(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzShuffleF64x2(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzShuffleF64x2(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// ShuffleF64x2: Shuffle 128-bits (composed of 2 double-precision (64-bit)
// floating-point elements) selected by 'imm8' from 'a' and 'b', and store the
// results in 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[0])
//		dst[255:128] := SELECT2(b[255:0], imm8[1])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFF64X2'. Intrinsic: '_mm256_shuffle_f64x2'.
// Requires AVX512F.
func ShuffleF64x2(a M256d, b M256d, imm8 int) M256d {
	return M256d(shuffleF64x2([4]float64(a), [4]float64(b), imm8))
}

func shuffleF64x2(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_mask_shuffle_i32x4'.
// Requires AVX512F.
func MaskShuffleI32x4(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskShuffleI32x4([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskShuffleI32x4(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_maskz_shuffle_i32x4'.
// Requires AVX512F.
func MaskzShuffleI32x4(k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzShuffleI32x4(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzShuffleI32x4(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// ShuffleI32x4: Shuffle 128-bits (composed of 4 32-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI32X4'. Intrinsic: '_mm256_shuffle_i32x4'.
// Requires AVX512F.
func ShuffleI32x4(a M256i, b M256i, imm8 int) M256i {
	return M256i(shuffleI32x4([32]byte(a), [32]byte(b), imm8))
}

func shuffleI32x4(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using writemask
// 'k' (elements are copied from 'src' when the corresponding mask bit is not
// set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_mask_shuffle_i64x2'.
// Requires AVX512F.
func MaskShuffleI64x2(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskShuffleI64x2([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskShuffleI64x2(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected
// by 'imm8' from 'a' and 'b', and store the results in 'dst' using zeromask
// 'k' (elements are zeroed out when the corresponding mask bit is not set). 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		tmp_dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		tmp_dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_maskz_shuffle_i64x2'.
// Requires AVX512F.
func MaskzShuffleI64x2(k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskzShuffleI64x2(uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskzShuffleI64x2(k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// ShuffleI64x2: Shuffle 128-bits (composed of 2 64-bit integers) selected by
// 'imm8' from 'a' and 'b', and store the results in 'dst'. 
//
//		SELECT2(src, control){
//			CASE(control[0])
//			0:	tmp[127:0] := src[127:0]
//			1:	tmp[127:0] := src[255:128]
//			ESAC
//			RETURN tmp[127:0]
//		}
//		
//		dst[127:0] := SELECT2(a[255:0], imm8[1:0])
//		dst[255:128] := SELECT2(b[255:0], imm8[3:2])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFI64X2'. Intrinsic: '_mm256_shuffle_i64x2'.
// Requires AVX512F.
func ShuffleI64x2(a M256i, b M256i, imm8 int) M256i {
	return M256i(shuffleI64x2([32]byte(a), [32]byte(b), imm8))
}

func shuffleI64x2(a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_mask_shuffle_pd'.
// Requires AVX512F.
func MaskShufflePd(src M256d, k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskShufflePd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskShufflePd(src [4]float64, k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskzShufflePd: Shuffle double-precision (64-bit) floating-point elements
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		tmp_dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		tmp_dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		tmp_dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		tmp_dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_maskz_shuffle_pd'.
// Requires AVX512F.
func MaskzShufflePd(k Mmask8, a M256d, b M256d, imm8 int) M256d {
	return M256d(maskzShufflePd(uint8(k), [4]float64(a), [4]float64(b), imm8))
}

func maskzShufflePd(k uint8, a [4]float64, b [4]float64, imm8 int) [4]float64


// ShufflePd: Shuffle double-precision (64-bit) floating-point elements within
// 128-bit lanes using the control in 'imm8', and store the results in 'dst'. 
//
//		dst[63:0] := (imm8[0] == 0) ? a[63:0] : a[127:64]
//		dst[127:64] := (imm8[1] == 0) ? b[63:0] : b[127:64]
//		dst[191:128] := (imm8[2] == 0) ? a[191:128] : a[255:192]
//		dst[255:192] := (imm8[3] == 0) ? b[191:128] : b[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPD'. Intrinsic: '_mm256_shuffle_pd'.
// Requires AVX.
func ShufflePd(a M256d, b M256d, imm8 int) M256d {
	return M256d(shufflePd([4]float64(a), [4]float64(b), imm8))
}

func shufflePd(a [4]float64, b [4]float64, imm8 int) [4]float64


// MaskShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_mask_shuffle_ps'.
// Requires AVX512F.
func MaskShufflePs(src M256, k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskShufflePs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskShufflePs(src [8]float32, k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskzShufflePs: Shuffle single-precision (32-bit) floating-point elements in
// 'a' within 128-bit lanes using the control in 'imm8', and store the results
// in 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		tmp_dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		tmp_dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		tmp_dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		tmp_dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		tmp_dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		tmp_dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		tmp_dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		tmp_dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_maskz_shuffle_ps'.
// Requires AVX512F.
func MaskzShufflePs(k Mmask8, a M256, b M256, imm8 int) M256 {
	return M256(maskzShufflePs(uint8(k), [8]float32(a), [8]float32(b), imm8))
}

func maskzShufflePs(k uint8, a [8]float32, b [8]float32, imm8 int) [8]float32


// ShufflePs: Shuffle single-precision (32-bit) floating-point elements in 'a'
// within 128-bit lanes using the control in 'imm8', and store the results in
// 'dst'. 
//
//		SELECT4(src, control){
//			CASE(control[1:0])
//			0:	tmp[31:0] := src[31:0]
//			1:	tmp[31:0] := src[63:32]
//			2:	tmp[31:0] := src[95:64]
//			3:	tmp[31:0] := src[127:96]
//			ESAC
//			RETURN tmp[31:0]
//		}
//		
//		dst[31:0] := SELECT4(a[127:0], imm8[1:0])
//		dst[63:32] := SELECT4(a[127:0], imm8[3:2])
//		dst[95:64] := SELECT4(b[127:0], imm8[5:4])
//		dst[127:96] := SELECT4(b[127:0], imm8[7:6])
//		dst[159:128] := SELECT4(a[255:128], imm8[1:0])
//		dst[191:160] := SELECT4(a[255:128], imm8[3:2])
//		dst[223:192] := SELECT4(b[255:128], imm8[5:4])
//		dst[255:224] := SELECT4(b[255:128], imm8[7:6])
//		dst[MAX:256] := 0
//
// Instruction: 'VSHUFPS'. Intrinsic: '_mm256_shuffle_ps'.
// Requires AVX.
func ShufflePs(a M256, b M256, imm8 int) M256 {
	return M256(shufflePs([8]float32(a), [8]float32(b), imm8))
}

func shufflePs(a [8]float32, b [8]float32, imm8 int) [8]float32


// MaskShufflehiEpi16: Shuffle 16-bit integers in the high 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the high 64
// bits of 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		tmp_dst[191:128] := a[191:128]
//		tmp_dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		tmp_dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		tmp_dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		tmp_dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm256_mask_shufflehi_epi16'.
// Requires AVX512BW.
func MaskShufflehiEpi16(src M256i, k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskShufflehiEpi16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskShufflehiEpi16(src [32]byte, k uint16, a [32]byte, imm8 int) [32]byte


// MaskzShufflehiEpi16: Shuffle 16-bit integers in the high 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the high 64
// bits of 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp_dst[63:0] := a[63:0]
//		tmp_dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		tmp_dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		tmp_dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		tmp_dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		tmp_dst[191:128] := a[191:128]
//		tmp_dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		tmp_dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		tmp_dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		tmp_dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm256_maskz_shufflehi_epi16'.
// Requires AVX512BW.
func MaskzShufflehiEpi16(k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskzShufflehiEpi16(uint16(k), [32]byte(a), imm8))
}

func maskzShufflehiEpi16(k uint16, a [32]byte, imm8 int) [32]byte


// ShufflehiEpi16: Shuffle 16-bit integers in the high 64 bits of 128-bit lanes
// of 'a' using the control in 'imm8'. Store the results in the high 64 bits of
// 128-bit lanes of 'dst', with the low 64 bits of 128-bit lanes being copied
// from from 'a' to 'dst'. 
//
//		dst[63:0] := a[63:0]
//		dst[79:64] := (a >> (imm8[1:0] * 16))[79:64]
//		dst[95:80] := (a >> (imm8[3:2] * 16))[79:64]
//		dst[111:96] := (a >> (imm8[5:4] * 16))[79:64]
//		dst[127:112] := (a >> (imm8[7:6] * 16))[79:64]
//		dst[191:128] := a[191:128]
//		dst[207:192] := (a >> (imm8[1:0] * 16))[207:192]
//		dst[223:208] := (a >> (imm8[3:2] * 16))[207:192]
//		dst[239:224] := (a >> (imm8[5:4] * 16))[207:192]
//		dst[255:240] := (a >> (imm8[7:6] * 16))[207:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFHW'. Intrinsic: '_mm256_shufflehi_epi16'.
// Requires AVX2.
func ShufflehiEpi16(a M256i, imm8 int) M256i {
	return M256i(shufflehiEpi16([32]byte(a), imm8))
}

func shufflehiEpi16(a [32]byte, imm8 int) [32]byte


// MaskShuffleloEpi16: Shuffle 16-bit integers in the low 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the low 64
// bits of 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		tmp_dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		tmp_dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		tmp_dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		tmp_dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		tmp_dst[255:192] := a[255:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm256_mask_shufflelo_epi16'.
// Requires AVX512BW.
func MaskShuffleloEpi16(src M256i, k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskShuffleloEpi16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskShuffleloEpi16(src [32]byte, k uint16, a [32]byte, imm8 int) [32]byte


// MaskzShuffleloEpi16: Shuffle 16-bit integers in the low 64 bits of 128-bit
// lanes of 'a' using the control in 'imm8'. Store the results in the low 64
// bits of 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being
// copied from from 'a' to 'dst', using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		tmp_dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		tmp_dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		tmp_dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		tmp_dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		tmp_dst[127:64] := a[127:64]
//		tmp_dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		tmp_dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		tmp_dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		tmp_dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		tmp_dst[255:192] := a[255:192]
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm256_maskz_shufflelo_epi16'.
// Requires AVX512BW.
func MaskzShuffleloEpi16(k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskzShuffleloEpi16(uint16(k), [32]byte(a), imm8))
}

func maskzShuffleloEpi16(k uint16, a [32]byte, imm8 int) [32]byte


// ShuffleloEpi16: Shuffle 16-bit integers in the low 64 bits of 128-bit lanes
// of 'a' using the control in 'imm8'. Store the results in the low 64 bits of
// 128-bit lanes of 'dst', with the high 64 bits of 128-bit lanes being copied
// from from 'a' to 'dst'. 
//
//		dst[15:0] := (a >> (imm8[1:0] * 16))[15:0]
//		dst[31:16] := (a >> (imm8[3:2] * 16))[15:0]
//		dst[47:32] := (a >> (imm8[5:4] * 16))[15:0]
//		dst[63:48] := (a >> (imm8[7:6] * 16))[15:0]
//		dst[127:64] := a[127:64]
//		dst[143:128] := (a >> (imm8[1:0] * 16))[143:128]
//		dst[159:144] := (a >> (imm8[3:2] * 16))[143:128]
//		dst[175:160] := (a >> (imm8[5:4] * 16))[143:128]
//		dst[191:176] := (a >> (imm8[7:6] * 16))[143:128]
//		dst[255:192] := a[255:192]
//		dst[MAX:256] := 0
//
// Instruction: 'VPSHUFLW'. Intrinsic: '_mm256_shufflelo_epi16'.
// Requires AVX2.
func ShuffleloEpi16(a M256i, imm8 int) M256i {
	return M256i(shuffleloEpi16([32]byte(a), imm8))
}

func shuffleloEpi16(a [32]byte, imm8 int) [32]byte


// SignEpi16: Negate packed 16-bit integers in 'a' when the corresponding
// signed 16-bit integer in 'b' is negative, and store the results in 'dst'.
// Element in 'dst' are zeroed out when the corresponding element in 'b' is
// zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF b[i+15:i] < 0
//				dst[i+15:i] := NEG(a[i+15:i])
//			ELSE IF b[i+15:i] = 0
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSIGNW'. Intrinsic: '_mm256_sign_epi16'.
// Requires AVX2.
func SignEpi16(a M256i, b M256i) M256i {
	return M256i(signEpi16([32]byte(a), [32]byte(b)))
}

func signEpi16(a [32]byte, b [32]byte) [32]byte


// SignEpi32: Negate packed 32-bit integers in 'a' when the corresponding
// signed 32-bit integer in 'b' is negative, and store the results in 'dst'.
// Element in 'dst' are zeroed out when the corresponding element in 'b' is
// zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF b[i+31:i] < 0
//				dst[i+31:i] := NEG(a[i+31:i])
//			ELSE IF b[i+31:i] = 0
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := a[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSIGND'. Intrinsic: '_mm256_sign_epi32'.
// Requires AVX2.
func SignEpi32(a M256i, b M256i) M256i {
	return M256i(signEpi32([32]byte(a), [32]byte(b)))
}

func signEpi32(a [32]byte, b [32]byte) [32]byte


// SignEpi8: Negate packed 8-bit integers in 'a' when the corresponding signed
// 8-bit integer in 'b' is negative, and store the results in 'dst'. Element in
// 'dst' are zeroed out when the corresponding element in 'b' is zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF b[i+7:i] < 0
//				dst[i+7:i] := NEG(a[i+7:i])
//			ELSE IF b[i+7:i] = 0
//				dst[i+7:i] := 0
//			ELSE
//				dst[i+7:i] := a[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSIGNB'. Intrinsic: '_mm256_sign_epi8'.
// Requires AVX2.
func SignEpi8(a M256i, b M256i) M256i {
	return M256i(signEpi8([32]byte(a), [32]byte(b)))
}

func signEpi8(a [32]byte, b [32]byte) [32]byte


// SinPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sin_pd'.
// Requires AVX.
func SinPd(a M256d) M256d {
	return M256d(sinPd([4]float64(a)))
}

func sinPd(a [4]float64) [4]float64


// SinPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in radians, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sin_ps'.
// Requires AVX.
func SinPs(a M256) M256 {
	return M256(sinPs([8]float32(a)))
}

func sinPs(a [8]float32) [8]float32


// SincosPd: Compute the sine and cosine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, store the sine in
// 'dst', and store the cosine into memory at 'mem_addr'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SIN(a[i+63:i])
//			MEM[mem_addr+i+63:mem_addr+i] := COS(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sincos_pd'.
// Requires AVX.
func SincosPd(mem_addr M256d, a M256d) M256d {
	return M256d(sincosPd([4]float64(mem_addr), [4]float64(a)))
}

func sincosPd(mem_addr [4]float64, a [4]float64) [4]float64


// SincosPs: Compute the sine and cosine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, store the sine in
// 'dst', and store the cosine into memory at 'mem_addr'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SIN(a[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := COS(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sincos_ps'.
// Requires AVX.
func SincosPs(mem_addr M256, a M256) M256 {
	return M256(sincosPs([8]float32(mem_addr), [8]float32(a)))
}

func sincosPs(mem_addr [8]float32, a [8]float32) [8]float32


// SindPd: Compute the sine of packed double-precision (64-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SIND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sind_pd'.
// Requires AVX.
func SindPd(a M256d) M256d {
	return M256d(sindPd([4]float64(a)))
}

func sindPd(a [4]float64) [4]float64


// SindPs: Compute the sine of packed single-precision (32-bit) floating-point
// elements in 'a' expressed in degrees, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SIND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sind_ps'.
// Requires AVX.
func SindPs(a M256) M256 {
	return M256(sindPs([8]float32(a)))
}

func sindPs(a [8]float32) [8]float32


// SinhPd: Compute the hyperbolic sine of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SINH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sinh_pd'.
// Requires AVX.
func SinhPd(a M256d) M256d {
	return M256d(sinhPd([4]float64(a)))
}

func sinhPd(a [4]float64) [4]float64


// SinhPs: Compute the hyperbolic sine of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SINH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_sinh_ps'.
// Requires AVX.
func SinhPs(a M256) M256 {
	return M256(sinhPs([8]float32(a)))
}

func sinhPs(a [8]float32) [8]float32


// MaskSllEpi16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_mask_sll_epi16'.
// Requires AVX512BW.
func MaskSllEpi16(src M256i, k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskSllEpi16([32]byte(src), uint16(k), [32]byte(a), [16]byte(count)))
}

func maskSllEpi16(src [32]byte, k uint16, a [32]byte, count [16]byte) [32]byte


// MaskzSllEpi16: Shift packed 16-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_maskz_sll_epi16'.
// Requires AVX512BW.
func MaskzSllEpi16(k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskzSllEpi16(uint16(k), [32]byte(a), [16]byte(count)))
}

func maskzSllEpi16(k uint16, a [32]byte, count [16]byte) [32]byte


// SllEpi16: Shift packed 16-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_sll_epi16'.
// Requires AVX2.
func SllEpi16(a M256i, count M128i) M256i {
	return M256i(sllEpi16([32]byte(a), [16]byte(count)))
}

func sllEpi16(a [32]byte, count [16]byte) [32]byte


// MaskSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_sll_epi32'.
// Requires AVX512F.
func MaskSllEpi32(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSllEpi32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSllEpi32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_sll_epi32'.
// Requires AVX512F.
func MaskzSllEpi32(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSllEpi32(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSllEpi32(k uint8, a [32]byte, count [16]byte) [32]byte


// SllEpi32: Shift packed 32-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_sll_epi32'.
// Requires AVX2.
func SllEpi32(a M256i, count M128i) M256i {
	return M256i(sllEpi32([32]byte(a), [16]byte(count)))
}

func sllEpi32(a [32]byte, count [16]byte) [32]byte


// MaskSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_sll_epi64'.
// Requires AVX512F.
func MaskSllEpi64(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSllEpi64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSllEpi64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_sll_epi64'.
// Requires AVX512F.
func MaskzSllEpi64(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSllEpi64(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSllEpi64(k uint8, a [32]byte, count [16]byte) [32]byte


// SllEpi64: Shift packed 64-bit integers in 'a' left by 'count' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_sll_epi64'.
// Requires AVX2.
func SllEpi64(a M256i, count M128i) M256i {
	return M256i(sllEpi64([32]byte(a), [16]byte(count)))
}

func sllEpi64(a [32]byte, count [16]byte) [32]byte


// MaskSlliEpi16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_mask_slli_epi16'.
// Requires AVX512BW.
func MaskSlliEpi16(src M256i, k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskSlliEpi16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskSlliEpi16(src [32]byte, k uint16, a [32]byte, imm8 uint32) [32]byte


// MaskzSlliEpi16: Shift packed 16-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_maskz_slli_epi16'.
// Requires AVX512BW.
func MaskzSlliEpi16(k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlliEpi16(uint16(k), [32]byte(a), imm8))
}

func maskzSlliEpi16(k uint16, a [32]byte, imm8 uint32) [32]byte


// SlliEpi16: Shift packed 16-bit integers in 'a' left by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLW'. Intrinsic: '_mm256_slli_epi16'.
// Requires AVX2.
func SlliEpi16(a M256i, imm8 int) M256i {
	return M256i(slliEpi16([32]byte(a), imm8))
}

func slliEpi16(a [32]byte, imm8 int) [32]byte


// MaskSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_mask_slli_epi32'.
// Requires AVX512F.
func MaskSlliEpi32(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSlliEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSlliEpi32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_maskz_slli_epi32'.
// Requires AVX512F.
func MaskzSlliEpi32(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlliEpi32(uint8(k), [32]byte(a), imm8))
}

func maskzSlliEpi32(k uint8, a [32]byte, imm8 uint32) [32]byte


// SlliEpi32: Shift packed 32-bit integers in 'a' left by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLD'. Intrinsic: '_mm256_slli_epi32'.
// Requires AVX2.
func SlliEpi32(a M256i, imm8 int) M256i {
	return M256i(slliEpi32([32]byte(a), imm8))
}

func slliEpi32(a [32]byte, imm8 int) [32]byte


// MaskSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_mask_slli_epi64'.
// Requires AVX512F.
func MaskSlliEpi64(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSlliEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSlliEpi64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_maskz_slli_epi64'.
// Requires AVX512F.
func MaskzSlliEpi64(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSlliEpi64(uint8(k), [32]byte(a), imm8))
}

func maskzSlliEpi64(k uint8, a [32]byte, imm8 uint32) [32]byte


// SlliEpi64: Shift packed 64-bit integers in 'a' left by 'imm8' while shifting
// in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLQ'. Intrinsic: '_mm256_slli_epi64'.
// Requires AVX2.
func SlliEpi64(a M256i, imm8 int) M256i {
	return M256i(slliEpi64([32]byte(a), imm8))
}

func slliEpi64(a [32]byte, imm8 int) [32]byte


// SlliSi256: Shift 128-bit lanes in 'a' left by 'imm8' bytes while shifting in
// zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] << (tmp*8)
//		dst[255:128] := a[255:128] << (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLDQ'. Intrinsic: '_mm256_slli_si256'.
// Requires AVX2.
func SlliSi256(a M256i, imm8 int) M256i {
	return M256i(slliSi256([32]byte(a), imm8))
}

func slliSi256(a [32]byte, imm8 int) [32]byte


// MaskSllvEpi16: Shift packed 16-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm256_mask_sllv_epi16'.
// Requires AVX512BW.
func MaskSllvEpi16(src M256i, k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskSllvEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(count)))
}

func maskSllvEpi16(src [32]byte, k uint16, a [32]byte, count [32]byte) [32]byte


// MaskzSllvEpi16: Shift packed 16-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm256_maskz_sllv_epi16'.
// Requires AVX512BW.
func MaskzSllvEpi16(k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskzSllvEpi16(uint16(k), [32]byte(a), [32]byte(count)))
}

func maskzSllvEpi16(k uint16, a [32]byte, count [32]byte) [32]byte


// SllvEpi16: Shift packed 16-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] << count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVW'. Intrinsic: '_mm256_sllv_epi16'.
// Requires AVX512BW.
func SllvEpi16(a M256i, count M256i) M256i {
	return M256i(sllvEpi16([32]byte(a), [32]byte(count)))
}

func sllvEpi16(a [32]byte, count [32]byte) [32]byte


// MaskSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_mask_sllv_epi32'.
// Requires AVX512F.
func MaskSllvEpi32(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSllvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSllvEpi32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSllvEpi32: Shift packed 32-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_maskz_sllv_epi32'.
// Requires AVX512F.
func MaskzSllvEpi32(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSllvEpi32(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSllvEpi32(k uint8, a [32]byte, count [32]byte) [32]byte


// SllvEpi32: Shift packed 32-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] << count[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVD'. Intrinsic: '_mm256_sllv_epi32'.
// Requires AVX2.
func SllvEpi32(a M256i, count M256i) M256i {
	return M256i(sllvEpi32([32]byte(a), [32]byte(count)))
}

func sllvEpi32(a [32]byte, count [32]byte) [32]byte


// MaskSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_mask_sllv_epi64'.
// Requires AVX512F.
func MaskSllvEpi64(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSllvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSllvEpi64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSllvEpi64: Shift packed 64-bit integers in 'a' left by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_maskz_sllv_epi64'.
// Requires AVX512F.
func MaskzSllvEpi64(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSllvEpi64(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSllvEpi64(k uint8, a [32]byte, count [32]byte) [32]byte


// SllvEpi64: Shift packed 64-bit integers in 'a' left by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] << count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSLLVQ'. Intrinsic: '_mm256_sllv_epi64'.
// Requires AVX2.
func SllvEpi64(a M256i, count M256i) M256i {
	return M256i(sllvEpi64([32]byte(a), [32]byte(count)))
}

func sllvEpi64(a [32]byte, count [32]byte) [32]byte


// MaskSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_mask_sqrt_pd'.
// Requires AVX512F.
func MaskSqrtPd(src M256d, k Mmask8, a M256d) M256d {
	return M256d(maskSqrtPd([4]float64(src), uint8(k), [4]float64(a)))
}

func maskSqrtPd(src [4]float64, k uint8, a [4]float64) [4]float64


// MaskzSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SQRT(a[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_maskz_sqrt_pd'.
// Requires AVX512F.
func MaskzSqrtPd(k Mmask8, a M256d) M256d {
	return M256d(maskzSqrtPd(uint8(k), [4]float64(a)))
}

func maskzSqrtPd(k uint8, a [4]float64) [4]float64


// SqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPD'. Intrinsic: '_mm256_sqrt_pd'.
// Requires AVX.
func SqrtPd(a M256d) M256d {
	return M256d(sqrtPd([4]float64(a)))
}

func sqrtPd(a [4]float64) [4]float64


// MaskSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_mask_sqrt_ps'.
// Requires AVX512F.
func MaskSqrtPs(src M256, k Mmask8, a M256) M256 {
	return M256(maskSqrtPs([8]float32(src), uint8(k), [8]float32(a)))
}

func maskSqrtPs(src [8]float32, k uint8, a [8]float32) [8]float32


// MaskzSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SQRT(a[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_maskz_sqrt_ps'.
// Requires AVX512F.
func MaskzSqrtPs(k Mmask8, a M256) M256 {
	return M256(maskzSqrtPs(uint8(k), [8]float32(a)))
}

func maskzSqrtPs(k uint8, a [8]float32) [8]float32


// SqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSQRTPS'. Intrinsic: '_mm256_sqrt_ps'.
// Requires AVX.
func SqrtPs(a M256) M256 {
	return M256(sqrtPs([8]float32(a)))
}

func sqrtPs(a [8]float32) [8]float32


// MaskSraEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_mask_sra_epi16'.
// Requires AVX512BW.
func MaskSraEpi16(src M256i, k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskSraEpi16([32]byte(src), uint16(k), [32]byte(a), [16]byte(count)))
}

func maskSraEpi16(src [32]byte, k uint16, a [32]byte, count [16]byte) [32]byte


// MaskzSraEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_maskz_sra_epi16'.
// Requires AVX512BW.
func MaskzSraEpi16(k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskzSraEpi16(uint16(k), [32]byte(a), [16]byte(count)))
}

func maskzSraEpi16(k uint16, a [32]byte, count [16]byte) [32]byte


// SraEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_sra_epi16'.
// Requires AVX2.
func SraEpi16(a M256i, count M128i) M256i {
	return M256i(sraEpi16([32]byte(a), [16]byte(count)))
}

func sraEpi16(a [32]byte, count [16]byte) [32]byte


// MaskSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_sra_epi32'.
// Requires AVX512F.
func MaskSraEpi32(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSraEpi32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSraEpi32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_sra_epi32'.
// Requires AVX512F.
func MaskzSraEpi32(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSraEpi32(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSraEpi32(k uint8, a [32]byte, count [16]byte) [32]byte


// SraEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_sra_epi32'.
// Requires AVX2.
func SraEpi32(a M256i, count M128i) M256i {
	return M256i(sraEpi32([32]byte(a), [16]byte(count)))
}

func sraEpi32(a [32]byte, count [16]byte) [32]byte


// MaskSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_sra_epi64'.
// Requires AVX512F.
func MaskSraEpi64(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSraEpi64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSraEpi64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_sra_epi64'.
// Requires AVX512F.
func MaskzSraEpi64(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSraEpi64(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSraEpi64(k uint8, a [32]byte, count [16]byte) [32]byte


// SraEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_sra_epi64'.
// Requires AVX512F.
func SraEpi64(a M256i, count M128i) M256i {
	return M256i(sraEpi64([32]byte(a), [16]byte(count)))
}

func sraEpi64(a [32]byte, count [16]byte) [32]byte


// MaskSraiEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_mask_srai_epi16'.
// Requires AVX512BW.
func MaskSraiEpi16(src M256i, k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskSraiEpi16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskSraiEpi16(src [32]byte, k uint16, a [32]byte, imm8 uint32) [32]byte


// MaskzSraiEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := SignBit
//				ELSE
//					dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_maskz_srai_epi16'.
// Requires AVX512BW.
func MaskzSraiEpi16(k Mmask16, a M256i, imm8 uint32) M256i {
	return M256i(maskzSraiEpi16(uint16(k), [32]byte(a), imm8))
}

func maskzSraiEpi16(k uint16, a [32]byte, imm8 uint32) [32]byte


// SraiEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := SignBit
//			ELSE
//				dst[i+15:i] := SignExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAW'. Intrinsic: '_mm256_srai_epi16'.
// Requires AVX2.
func SraiEpi16(a M256i, imm8 int) M256i {
	return M256i(sraiEpi16([32]byte(a), imm8))
}

func sraiEpi16(a [32]byte, imm8 int) [32]byte


// MaskSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_mask_srai_epi32'.
// Requires AVX512F.
func MaskSraiEpi32(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSraiEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSraiEpi32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := SignBit
//				ELSE
//					dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_maskz_srai_epi32'.
// Requires AVX512F.
func MaskzSraiEpi32(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSraiEpi32(uint8(k), [32]byte(a), imm8))
}

func maskzSraiEpi32(k uint8, a [32]byte, imm8 uint32) [32]byte


// SraiEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := SignBit
//			ELSE
//				dst[i+31:i] := SignExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAD'. Intrinsic: '_mm256_srai_epi32'.
// Requires AVX2.
func SraiEpi32(a M256i, imm8 int) M256i {
	return M256i(sraiEpi32([32]byte(a), imm8))
}

func sraiEpi32(a [32]byte, imm8 int) [32]byte


// MaskSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_mask_srai_epi64'.
// Requires AVX512F.
func MaskSraiEpi64(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSraiEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSraiEpi64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := SignBit
//				ELSE
//					dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_maskz_srai_epi64'.
// Requires AVX512F.
func MaskzSraiEpi64(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSraiEpi64(uint8(k), [32]byte(a), imm8))
}

func maskzSraiEpi64(k uint8, a [32]byte, imm8 uint32) [32]byte


// SraiEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in sign bits, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := SignBit
//			ELSE
//				dst[i+63:i] := SignExtend(a[i+63:i] << imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAQ'. Intrinsic: '_mm256_srai_epi64'.
// Requires AVX512F.
func SraiEpi64(a M256i, imm8 uint32) M256i {
	return M256i(sraiEpi64([32]byte(a), imm8))
}

func sraiEpi64(a [32]byte, imm8 uint32) [32]byte


// MaskSravEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm256_mask_srav_epi16'.
// Requires AVX512BW.
func MaskSravEpi16(src M256i, k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskSravEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(count)))
}

func maskSravEpi16(src [32]byte, k uint16, a [32]byte, count [32]byte) [32]byte


// MaskzSravEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm256_maskz_srav_epi16'.
// Requires AVX512BW.
func MaskzSravEpi16(k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskzSravEpi16(uint16(k), [32]byte(a), [32]byte(count)))
}

func maskzSravEpi16(k uint16, a [32]byte, count [32]byte) [32]byte


// SravEpi16: Shift packed 16-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := SignExtend(a[i+15:i] >> count[i+15:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVW'. Intrinsic: '_mm256_srav_epi16'.
// Requires AVX512BW.
func SravEpi16(a M256i, count M256i) M256i {
	return M256i(sravEpi16([32]byte(a), [32]byte(count)))
}

func sravEpi16(a [32]byte, count [32]byte) [32]byte


// MaskSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_mask_srav_epi32'.
// Requires AVX512F.
func MaskSravEpi32(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSravEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSravEpi32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSravEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_maskz_srav_epi32'.
// Requires AVX512F.
func MaskzSravEpi32(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSravEpi32(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSravEpi32(k uint8, a [32]byte, count [32]byte) [32]byte


// SravEpi32: Shift packed 32-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SignExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVD'. Intrinsic: '_mm256_srav_epi32'.
// Requires AVX2.
func SravEpi32(a M256i, count M256i) M256i {
	return M256i(sravEpi32([32]byte(a), [32]byte(count)))
}

func sravEpi32(a [32]byte, count [32]byte) [32]byte


// MaskSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using writemask 'k' (elements are
// copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_mask_srav_epi64'.
// Requires AVX512F.
func MaskSravEpi64(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSravEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSravEpi64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSravEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in sign
// bits, and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_maskz_srav_epi64'.
// Requires AVX512F.
func MaskzSravEpi64(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSravEpi64(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSravEpi64(k uint8, a [32]byte, count [32]byte) [32]byte


// SravEpi64: Shift packed 64-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in sign bits, and
// store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SignExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRAVQ'. Intrinsic: '_mm256_srav_epi64'.
// Requires AVX512F.
func SravEpi64(a M256i, count M256i) M256i {
	return M256i(sravEpi64([32]byte(a), [32]byte(count)))
}

func sravEpi64(a [32]byte, count [32]byte) [32]byte


// MaskSrlEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_mask_srl_epi16'.
// Requires AVX512BW.
func MaskSrlEpi16(src M256i, k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskSrlEpi16([32]byte(src), uint16(k), [32]byte(a), [16]byte(count)))
}

func maskSrlEpi16(src [32]byte, k uint16, a [32]byte, count [16]byte) [32]byte


// MaskzSrlEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF count[63:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_maskz_srl_epi16'.
// Requires AVX512BW.
func MaskzSrlEpi16(k Mmask16, a M256i, count M128i) M256i {
	return M256i(maskzSrlEpi16(uint16(k), [32]byte(a), [16]byte(count)))
}

func maskzSrlEpi16(k uint16, a [32]byte, count [16]byte) [32]byte


// SrlEpi16: Shift packed 16-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF count[63:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_srl_epi16'.
// Requires AVX2.
func SrlEpi16(a M256i, count M128i) M256i {
	return M256i(srlEpi16([32]byte(a), [16]byte(count)))
}

func srlEpi16(a [32]byte, count [16]byte) [32]byte


// MaskSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srl_epi32'.
// Requires AVX512F.
func MaskSrlEpi32(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSrlEpi32([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSrlEpi32(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF count[63:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srl_epi32'.
// Requires AVX512F.
func MaskzSrlEpi32(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSrlEpi32(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSrlEpi32(k uint8, a [32]byte, count [16]byte) [32]byte


// SrlEpi32: Shift packed 32-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF count[63:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_srl_epi32'.
// Requires AVX2.
func SrlEpi32(a M256i, count M128i) M256i {
	return M256i(srlEpi32([32]byte(a), [16]byte(count)))
}

func srlEpi32(a [32]byte, count [16]byte) [32]byte


// MaskSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srl_epi64'.
// Requires AVX512F.
func MaskSrlEpi64(src M256i, k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskSrlEpi64([32]byte(src), uint8(k), [32]byte(a), [16]byte(count)))
}

func maskSrlEpi64(src [32]byte, k uint8, a [32]byte, count [16]byte) [32]byte


// MaskzSrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF count[63:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srl_epi64'.
// Requires AVX512F.
func MaskzSrlEpi64(k Mmask8, a M256i, count M128i) M256i {
	return M256i(maskzSrlEpi64(uint8(k), [32]byte(a), [16]byte(count)))
}

func maskzSrlEpi64(k uint8, a [32]byte, count [16]byte) [32]byte


// SrlEpi64: Shift packed 64-bit integers in 'a' right by 'count' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF count[63:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[63:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_srl_epi64'.
// Requires AVX2.
func SrlEpi64(a M256i, count M128i) M256i {
	return M256i(srlEpi64([32]byte(a), [16]byte(count)))
}

func srlEpi64(a [32]byte, count [16]byte) [32]byte


// MaskSrliEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_mask_srli_epi16'.
// Requires AVX512BW.
func MaskSrliEpi16(src M256i, k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskSrliEpi16([32]byte(src), uint16(k), [32]byte(a), imm8))
}

func maskSrliEpi16(src [32]byte, k uint16, a [32]byte, imm8 int) [32]byte


// MaskzSrliEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				IF imm8[7:0] > 15
//					dst[i+15:i] := 0
//				ELSE
//					dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_maskz_srli_epi16'.
// Requires AVX512BW.
func MaskzSrliEpi16(k Mmask16, a M256i, imm8 int) M256i {
	return M256i(maskzSrliEpi16(uint16(k), [32]byte(a), imm8))
}

func maskzSrliEpi16(k uint16, a [32]byte, imm8 int) [32]byte


// SrliEpi16: Shift packed 16-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF imm8[7:0] > 15
//				dst[i+15:i] := 0
//			ELSE
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLW'. Intrinsic: '_mm256_srli_epi16'.
// Requires AVX2.
func SrliEpi16(a M256i, imm8 int) M256i {
	return M256i(srliEpi16([32]byte(a), imm8))
}

func srliEpi16(a [32]byte, imm8 int) [32]byte


// MaskSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_mask_srli_epi32'.
// Requires AVX512F.
func MaskSrliEpi32(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrliEpi32([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrliEpi32(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				IF imm8[7:0] > 31
//					dst[i+31:i] := 0
//				ELSE
//					dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_maskz_srli_epi32'.
// Requires AVX512F.
func MaskzSrliEpi32(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrliEpi32(uint8(k), [32]byte(a), imm8))
}

func maskzSrliEpi32(k uint8, a [32]byte, imm8 uint32) [32]byte


// SrliEpi32: Shift packed 32-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF imm8[7:0] > 31
//				dst[i+31:i] := 0
//			ELSE
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLD'. Intrinsic: '_mm256_srli_epi32'.
// Requires AVX2.
func SrliEpi32(a M256i, imm8 int) M256i {
	return M256i(srliEpi32([32]byte(a), imm8))
}

func srliEpi32(a [32]byte, imm8 int) [32]byte


// MaskSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_mask_srli_epi64'.
// Requires AVX512F.
func MaskSrliEpi64(src M256i, k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskSrliEpi64([32]byte(src), uint8(k), [32]byte(a), imm8))
}

func maskSrliEpi64(src [32]byte, k uint8, a [32]byte, imm8 uint32) [32]byte


// MaskzSrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst' using zeromask 'k'
// (elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				IF imm8[7:0] > 63
//					dst[i+63:i] := 0
//				ELSE
//					dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//				FI
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_maskz_srli_epi64'.
// Requires AVX512F.
func MaskzSrliEpi64(k Mmask8, a M256i, imm8 uint32) M256i {
	return M256i(maskzSrliEpi64(uint8(k), [32]byte(a), imm8))
}

func maskzSrliEpi64(k uint8, a [32]byte, imm8 uint32) [32]byte


// SrliEpi64: Shift packed 64-bit integers in 'a' right by 'imm8' while
// shifting in zeros, and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF imm8[7:0] > 63
//				dst[i+63:i] := 0
//			ELSE
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> imm8[7:0])
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLQ'. Intrinsic: '_mm256_srli_epi64'.
// Requires AVX2.
func SrliEpi64(a M256i, imm8 int) M256i {
	return M256i(srliEpi64([32]byte(a), imm8))
}

func srliEpi64(a [32]byte, imm8 int) [32]byte


// SrliSi256: Shift 128-bit lanes in 'a' right by 'imm8' bytes while shifting
// in zeros, and store the results in 'dst'. 
//
//		tmp := imm8[7:0]
//		IF tmp > 15
//			tmp := 16
//		FI
//		dst[127:0] := a[127:0] >> (tmp*8)
//		dst[255:128] := a[255:128] >> (tmp*8)
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLDQ'. Intrinsic: '_mm256_srli_si256'.
// Requires AVX2.
func SrliSi256(a M256i, imm8 int) M256i {
	return M256i(srliSi256([32]byte(a), imm8))
}

func srliSi256(a [32]byte, imm8 int) [32]byte


// MaskSrlvEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+63:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm256_mask_srlv_epi16'.
// Requires AVX512BW.
func MaskSrlvEpi16(src M256i, k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskSrlvEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(count)))
}

func maskSrlvEpi16(src [32]byte, k uint16, a [32]byte, count [32]byte) [32]byte


// MaskzSrlvEpi16: Shift packed 16-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm256_maskz_srlv_epi16'.
// Requires AVX512BW.
func MaskzSrlvEpi16(k Mmask16, a M256i, count M256i) M256i {
	return M256i(maskzSrlvEpi16(uint16(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlvEpi16(k uint16, a [32]byte, count [32]byte) [32]byte


// SrlvEpi16: Shift packed 16-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := ZeroExtend(a[i+15:i] >> count[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVW'. Intrinsic: '_mm256_srlv_epi16'.
// Requires AVX512BW.
func SrlvEpi16(a M256i, count M256i) M256i {
	return M256i(srlvEpi16([32]byte(a), [32]byte(count)))
}

func srlvEpi16(a [32]byte, count [32]byte) [32]byte


// MaskSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_mask_srlv_epi32'.
// Requires AVX512F.
func MaskSrlvEpi32(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrlvEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrlvEpi32(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_maskz_srlv_epi32'.
// Requires AVX512F.
func MaskzSrlvEpi32(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrlvEpi32(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlvEpi32(k uint8, a [32]byte, count [32]byte) [32]byte


// SrlvEpi32: Shift packed 32-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ZeroExtend(a[i+31:i] >> count[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVD'. Intrinsic: '_mm256_srlv_epi32'.
// Requires AVX2.
func SrlvEpi32(a M256i, count M256i) M256i {
	return M256i(srlvEpi32([32]byte(a), [32]byte(count)))
}

func srlvEpi32(a [32]byte, count [32]byte) [32]byte


// MaskSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_mask_srlv_epi64'.
// Requires AVX512F.
func MaskSrlvEpi64(src M256i, k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskSrlvEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(count)))
}

func maskSrlvEpi64(src [32]byte, k uint8, a [32]byte, count [32]byte) [32]byte


// MaskzSrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount
// specified by the corresponding element in 'count' while shifting in zeros,
// and store the results in 'dst' using zeromask 'k' (elements are zeroed out
// when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_maskz_srlv_epi64'.
// Requires AVX512F.
func MaskzSrlvEpi64(k Mmask8, a M256i, count M256i) M256i {
	return M256i(maskzSrlvEpi64(uint8(k), [32]byte(a), [32]byte(count)))
}

func maskzSrlvEpi64(k uint8, a [32]byte, count [32]byte) [32]byte


// SrlvEpi64: Shift packed 64-bit integers in 'a' right by the amount specified
// by the corresponding element in 'count' while shifting in zeros, and store
// the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ZeroExtend(a[i+63:i] >> count[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSRLVQ'. Intrinsic: '_mm256_srlv_epi64'.
// Requires AVX2.
func SrlvEpi64(a M256i, count M256i) M256i {
	return M256i(srlvEpi64([32]byte(a), [32]byte(count)))
}

func srlvEpi64(a [32]byte, count [32]byte) [32]byte


// MaskStoreEpi32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA32'. Intrinsic: '_mm256_mask_store_epi32'.
// Requires AVX512F.
func MaskStoreEpi32(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreEpi32(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreEpi32(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreEpi64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQA64'. Intrinsic: '_mm256_mask_store_epi64'.
// Requires AVX512F.
func MaskStoreEpi64(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreEpi64(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreEpi64(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStorePd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_mask_store_pd'.
// Requires AVX512F.
func MaskStorePd(mem_addr uintptr, k Mmask8, a M256d)  {
	maskStorePd(uintptr(mem_addr), uint8(k), [4]float64(a))
}

func maskStorePd(mem_addr uintptr, k uint8, a [4]float64) 


// StorePd: Store 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVAPD'. Intrinsic: '_mm256_store_pd'.
// Requires AVX.
func StorePd(mem_addr float64, a M256d)  {
	storePd(mem_addr, [4]float64(a))
}

func storePd(mem_addr float64, a [4]float64) 


// MaskStorePs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_mask_store_ps'.
// Requires AVX512F.
func MaskStorePs(mem_addr uintptr, k Mmask8, a M256)  {
	maskStorePs(uintptr(mem_addr), uint8(k), [8]float32(a))
}

func maskStorePs(mem_addr uintptr, k uint8, a [8]float32) 


// StorePs: Store 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVAPS'. Intrinsic: '_mm256_store_ps'.
// Requires AVX.
func StorePs(mem_addr float32, a M256)  {
	storePs(mem_addr, [8]float32(a))
}

func storePs(mem_addr float32, a [8]float32) 


// StoreSi256: Store 256-bits of integer data from 'a' into memory.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVDQA'. Intrinsic: '_mm256_store_si256'.
// Requires AVX.
func StoreSi256(mem_addr M256i, a M256i)  {
	storeSi256([32]byte(mem_addr), [32]byte(a))
}

func storeSi256(mem_addr [32]byte, a [32]byte) 


// MaskStoreuEpi16: Store packed 16-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				MEM[mem_addr+i+15:mem_addr+i] := a[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVDQU16'. Intrinsic: '_mm256_mask_storeu_epi16'.
// Requires AVX512BW.
func MaskStoreuEpi16(mem_addr uintptr, k Mmask16, a M256i)  {
	maskStoreuEpi16(uintptr(mem_addr), uint16(k), [32]byte(a))
}

func maskStoreuEpi16(mem_addr uintptr, k uint16, a [32]byte) 


// MaskStoreuEpi32: Store packed 32-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU32'. Intrinsic: '_mm256_mask_storeu_epi32'.
// Requires AVX512F.
func MaskStoreuEpi32(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreuEpi32(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreuEpi32(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreuEpi64: Store packed 64-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU64'. Intrinsic: '_mm256_mask_storeu_epi64'.
// Requires AVX512F.
func MaskStoreuEpi64(mem_addr uintptr, k Mmask8, a M256i)  {
	maskStoreuEpi64(uintptr(mem_addr), uint8(k), [32]byte(a))
}

func maskStoreuEpi64(mem_addr uintptr, k uint8, a [32]byte) 


// MaskStoreuEpi8: Store packed 8-bit integers from 'a' into memory using
// writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				MEM[mem_addr+i+7:mem_addr+i] := a[i+7:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVDQU8'. Intrinsic: '_mm256_mask_storeu_epi8'.
// Requires AVX512BW.
func MaskStoreuEpi8(mem_addr uintptr, k Mmask32, a M256i)  {
	maskStoreuEpi8(uintptr(mem_addr), uint32(k), [32]byte(a))
}

func maskStoreuEpi8(mem_addr uintptr, k uint32, a [32]byte) 


// MaskStoreuPd: Store packed double-precision (64-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				MEM[mem_addr+i+63:mem_addr+i] := a[i+63:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_mask_storeu_pd'.
// Requires AVX512F.
func MaskStoreuPd(mem_addr uintptr, k Mmask8, a M256d)  {
	maskStoreuPd(uintptr(mem_addr), uint8(k), [4]float64(a))
}

func maskStoreuPd(mem_addr uintptr, k uint8, a [4]float64) 


// StoreuPd: Store 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVUPD'. Intrinsic: '_mm256_storeu_pd'.
// Requires AVX.
func StoreuPd(mem_addr float64, a M256d)  {
	storeuPd(mem_addr, [4]float64(a))
}

func storeuPd(mem_addr float64, a [4]float64) 


// MaskStoreuPs: Store packed single-precision (32-bit) floating-point elements
// from 'a' into memory using writemask 'k'.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				MEM[mem_addr+i+31:mem_addr+i] := a[i+31:i]
//			FI
//		ENDFOR
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_mask_storeu_ps'.
// Requires AVX512F.
func MaskStoreuPs(mem_addr uintptr, k Mmask8, a M256)  {
	maskStoreuPs(uintptr(mem_addr), uint8(k), [8]float32(a))
}

func maskStoreuPs(mem_addr uintptr, k uint8, a [8]float32) 


// StoreuPs: Store 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVUPS'. Intrinsic: '_mm256_storeu_ps'.
// Requires AVX.
func StoreuPs(mem_addr float32, a M256)  {
	storeuPs(mem_addr, [8]float32(a))
}

func storeuPs(mem_addr float32, a [8]float32) 


// StoreuSi256: Store 256-bits of integer data from 'a' into memory.
// 	'mem_addr' does not need to be aligned on any particular boundary. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVDQU'. Intrinsic: '_mm256_storeu_si256'.
// Requires AVX.
func StoreuSi256(mem_addr M256i, a M256i)  {
	storeuSi256([32]byte(mem_addr), [32]byte(a))
}

func storeuSi256(mem_addr [32]byte, a [32]byte) 


// Storeu2M128: Store the high and low 128-bit halves (each composed of 4
// packed single-precision (32-bit) floating-point elements) from 'a' into
// memory two different 128-bit locations.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		MEM[loaddr+127:loaddr] := a[127:0]
//		MEM[hiaddr+127:hiaddr] := a[255:128]
//
// Instruction: '...'. Intrinsic: '_mm256_storeu2_m128'.
// Requires AVX.
func Storeu2M128(hiaddr float32, loaddr float32, a M256)  {
	storeu2M128(hiaddr, loaddr, [8]float32(a))
}

func storeu2M128(hiaddr float32, loaddr float32, a [8]float32) 


// Storeu2M128d: Store the high and low 128-bit halves (each composed of 2
// packed double-precision (64-bit) floating-point elements) from 'a' into
// memory two different 128-bit locations.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		MEM[loaddr+127:loaddr] := a[127:0]
//		MEM[hiaddr+127:hiaddr] := a[255:128]
//
// Instruction: '...'. Intrinsic: '_mm256_storeu2_m128d'.
// Requires AVX.
func Storeu2M128d(hiaddr float64, loaddr float64, a M256d)  {
	storeu2M128d(hiaddr, loaddr, [4]float64(a))
}

func storeu2M128d(hiaddr float64, loaddr float64, a [4]float64) 


// Storeu2M128i: Store the high and low 128-bit halves (each composed of
// integer data) from 'a' into memory two different 128-bit locations.
// 	'hiaddr' and 'loaddr' do not need to be aligned on any particular boundary. 
//
//		MEM[loaddr+127:loaddr] := a[127:0]
//		MEM[hiaddr+127:hiaddr] := a[255:128]
//
// Instruction: '...'. Intrinsic: '_mm256_storeu2_m128i'.
// Requires AVX.
func Storeu2M128i(hiaddr M128i, loaddr M128i, a M256i)  {
	storeu2M128i([16]byte(hiaddr), [16]byte(loaddr), [32]byte(a))
}

func storeu2M128i(hiaddr [16]byte, loaddr [16]byte, a [32]byte) 


// StreamLoadSi256: Load 256-bits of integer data from memory into 'dst' using
// a non-temporal memory hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		dst[255:0] := MEM[mem_addr+255:mem_addr]
//		dst[MAX:256] := 0
//
// Instruction: 'VMOVNTDQA'. Intrinsic: '_mm256_stream_load_si256'.
// Requires AVX2.
func StreamLoadSi256(mem_addr M256iConst) M256i {
	return M256i(streamLoadSi256(mem_addr))
}

func streamLoadSi256(mem_addr M256iConst) [32]byte


// StreamPd: Store 256-bits (composed of 4 packed double-precision (64-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVNTPD'. Intrinsic: '_mm256_stream_pd'.
// Requires AVX.
func StreamPd(mem_addr float64, a M256d)  {
	streamPd(mem_addr, [4]float64(a))
}

func streamPd(mem_addr float64, a [4]float64) 


// StreamPs: Store 256-bits (composed of 8 packed single-precision (32-bit)
// floating-point elements) from 'a' into memory using a non-temporal memory
// hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVNTPS'. Intrinsic: '_mm256_stream_ps'.
// Requires AVX.
func StreamPs(mem_addr float32, a M256)  {
	streamPs(mem_addr, [8]float32(a))
}

func streamPs(mem_addr float32, a [8]float32) 


// StreamSi256: Store 256-bits of integer data from 'a' into memory using a
// non-temporal memory hint.
// 	'mem_addr' must be aligned on a 32-byte boundary or a general-protection
// exception may be generated. 
//
//		MEM[mem_addr+255:mem_addr] := a[255:0]
//
// Instruction: 'VMOVNTDQ'. Intrinsic: '_mm256_stream_si256'.
// Requires AVX.
func StreamSi256(mem_addr M256i, a M256i)  {
	streamSi256([32]byte(mem_addr), [32]byte(a))
}

func streamSi256(mem_addr [32]byte, a [32]byte) 


// MaskSubEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm256_mask_sub_epi16'.
// Requires AVX512BW.
func MaskSubEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskSubEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskSubEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzSubEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := a[i+15:i] - b[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm256_maskz_sub_epi16'.
// Requires AVX512BW.
func MaskzSubEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzSubEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzSubEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// SubEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit integers
// in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := a[i+15:i] - b[i+15:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBW'. Intrinsic: '_mm256_sub_epi16'.
// Requires AVX2.
func SubEpi16(a M256i, b M256i) M256i {
	return M256i(subEpi16([32]byte(a), [32]byte(b)))
}

func subEpi16(a [32]byte, b [32]byte) [32]byte


// MaskSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_mask_sub_epi32'.
// Requires AVX512F.
func MaskSubEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskSubEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskSubEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzSubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_maskz_sub_epi32'.
// Requires AVX512F.
func MaskzSubEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzSubEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzSubEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// SubEpi32: Subtract packed 32-bit integers in 'b' from packed 32-bit integers
// in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBD'. Intrinsic: '_mm256_sub_epi32'.
// Requires AVX2.
func SubEpi32(a M256i, b M256i) M256i {
	return M256i(subEpi32([32]byte(a), [32]byte(b)))
}

func subEpi32(a [32]byte, b [32]byte) [32]byte


// MaskSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_mask_sub_epi64'.
// Requires AVX512F.
func MaskSubEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskSubEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskSubEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzSubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_maskz_sub_epi64'.
// Requires AVX512F.
func MaskzSubEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzSubEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzSubEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// SubEpi64: Subtract packed 64-bit integers in 'b' from packed 64-bit integers
// in 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBQ'. Intrinsic: '_mm256_sub_epi64'.
// Requires AVX2.
func SubEpi64(a M256i, b M256i) M256i {
	return M256i(subEpi64([32]byte(a), [32]byte(b)))
}

func subEpi64(a [32]byte, b [32]byte) [32]byte


// MaskSubEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a', and store the results in 'dst' using writemask 'k'
// (elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm256_mask_sub_epi8'.
// Requires AVX512BW.
func MaskSubEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskSubEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskSubEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzSubEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := a[i+7:i] - b[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm256_maskz_sub_epi8'.
// Requires AVX512BW.
func MaskzSubEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzSubEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzSubEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// SubEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers in
// 'a', and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := a[i+7:i] - b[i+7:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBB'. Intrinsic: '_mm256_sub_epi8'.
// Requires AVX2.
func SubEpi8(a M256i, b M256i) M256i {
	return M256i(subEpi8([32]byte(a), [32]byte(b)))
}

func subEpi8(a [32]byte, b [32]byte) [32]byte


// MaskSubPd: Subtract packed double-precision (64-bit) floating-point elements
// in 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_mask_sub_pd'.
// Requires AVX512F.
func MaskSubPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskSubPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskSubPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzSubPd: Subtract packed double-precision (64-bit) floating-point
// elements in 'b' from packed double-precision (64-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] - b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_maskz_sub_pd'.
// Requires AVX512F.
func MaskzSubPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzSubPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzSubPd(k uint8, a [4]float64, b [4]float64) [4]float64


// SubPd: Subtract packed double-precision (64-bit) floating-point elements in
// 'b' from packed double-precision (64-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] - b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPD'. Intrinsic: '_mm256_sub_pd'.
// Requires AVX.
func SubPd(a M256d, b M256d) M256d {
	return M256d(subPd([4]float64(a), [4]float64(b)))
}

func subPd(a [4]float64, b [4]float64) [4]float64


// MaskSubPs: Subtract packed single-precision (32-bit) floating-point elements
// in 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst' using writemask 'k' (elements are copied from
// 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_mask_sub_ps'.
// Requires AVX512F.
func MaskSubPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskSubPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskSubPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzSubPs: Subtract packed single-precision (32-bit) floating-point
// elements in 'b' from packed single-precision (32-bit) floating-point
// elements in 'a', and store the results in 'dst' using zeromask 'k' (elements
// are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] - b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_maskz_sub_ps'.
// Requires AVX512F.
func MaskzSubPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzSubPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzSubPs(k uint8, a [8]float32, b [8]float32) [8]float32


// SubPs: Subtract packed single-precision (32-bit) floating-point elements in
// 'b' from packed single-precision (32-bit) floating-point elements in 'a',
// and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] - b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VSUBPS'. Intrinsic: '_mm256_sub_ps'.
// Requires AVX.
func SubPs(a M256, b M256) M256 {
	return M256(subPs([8]float32(a), [8]float32(b)))
}

func subPs(a [8]float32, b [8]float32) [8]float32


// MaskSubsEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm256_mask_subs_epi16'.
// Requires AVX512BW.
func MaskSubsEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskSubsEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskSubsEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzSubsEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm256_maskz_subs_epi16'.
// Requires AVX512BW.
func MaskzSubsEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzSubsEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzSubsEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// SubsEpi16: Subtract packed 16-bit integers in 'b' from packed 16-bit
// integers in 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_Int16(a[i+15:i] - b[i+15:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSW'. Intrinsic: '_mm256_subs_epi16'.
// Requires AVX2.
func SubsEpi16(a M256i, b M256i) M256i {
	return M256i(subsEpi16([32]byte(a), [32]byte(b)))
}

func subsEpi16(a [32]byte, b [32]byte) [32]byte


// MaskSubsEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm256_mask_subs_epi8'.
// Requires AVX512BW.
func MaskSubsEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskSubsEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskSubsEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzSubsEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit
// integers in 'a' using saturation, and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm256_maskz_subs_epi8'.
// Requires AVX512BW.
func MaskzSubsEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzSubsEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzSubsEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// SubsEpi8: Subtract packed 8-bit integers in 'b' from packed 8-bit integers
// in 'a' using saturation, and store the results in 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_Int8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBSB'. Intrinsic: '_mm256_subs_epi8'.
// Requires AVX2.
func SubsEpi8(a M256i, b M256i) M256i {
	return M256i(subsEpi8([32]byte(a), [32]byte(b)))
}

func subsEpi8(a [32]byte, b [32]byte) [32]byte


// MaskSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm256_mask_subs_epu16'.
// Requires AVX512BW.
func MaskSubsEpu16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskSubsEpu16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskSubsEpu16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzSubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])
//			ELSE
//				dst[i+15:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm256_maskz_subs_epu16'.
// Requires AVX512BW.
func MaskzSubsEpu16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzSubsEpu16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzSubsEpu16(k uint16, a [32]byte, b [32]byte) [32]byte


// SubsEpu16: Subtract packed unsigned 16-bit integers in 'b' from packed
// unsigned 16-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 15
//			i := j*16
//			dst[i+15:i] := Saturate_To_UnsignedInt16(a[i+15:i] - b[i+15:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSW'. Intrinsic: '_mm256_subs_epu16'.
// Requires AVX2.
func SubsEpu16(a M256i, b M256i) M256i {
	return M256i(subsEpu16([32]byte(a), [32]byte(b)))
}

func subsEpu16(a [32]byte, b [32]byte) [32]byte


// MaskSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using writemask 'k' (elements are copied from 'src' when the
// corresponding mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm256_mask_subs_epu8'.
// Requires AVX512BW.
func MaskSubsEpu8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskSubsEpu8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskSubsEpu8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzSubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst' using zeromask 'k' (elements are zeroed out when the corresponding
// mask bit is not set). 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])
//			ELSE
//				dst[i+7:i] := 0
//			FI	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm256_maskz_subs_epu8'.
// Requires AVX512BW.
func MaskzSubsEpu8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzSubsEpu8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzSubsEpu8(k uint32, a [32]byte, b [32]byte) [32]byte


// SubsEpu8: Subtract packed unsigned 8-bit integers in 'b' from packed
// unsigned 8-bit integers in 'a' using saturation, and store the results in
// 'dst'. 
//
//		FOR j := 0 to 31
//			i := j*8
//			dst[i+7:i] := Saturate_To_UnsignedInt8(a[i+7:i] - b[i+7:i])	
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPSUBUSB'. Intrinsic: '_mm256_subs_epu8'.
// Requires AVX2.
func SubsEpu8(a M256i, b M256i) M256i {
	return M256i(subsEpu8([32]byte(a), [32]byte(b)))
}

func subsEpu8(a [32]byte, b [32]byte) [32]byte


// SvmlCeilPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := CEIL(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_ceil_pd'.
// Requires AVX.
func SvmlCeilPd(a M256d) M256d {
	return M256d(svmlCeilPd([4]float64(a)))
}

func svmlCeilPd(a [4]float64) [4]float64


// SvmlCeilPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' up to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := CEIL(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_ceil_ps'.
// Requires AVX.
func SvmlCeilPs(a M256) M256 {
	return M256(svmlCeilPs([8]float32(a)))
}

func svmlCeilPs(a [8]float32) [8]float32


// SvmlFloorPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := FLOOR(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_floor_pd'.
// Requires AVX.
func SvmlFloorPd(a M256d) M256d {
	return M256d(svmlFloorPd([4]float64(a)))
}

func svmlFloorPd(a [4]float64) [4]float64


// SvmlFloorPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' down to an integer value, and store the results as packed
// single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := FLOOR(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_floor_ps'.
// Requires AVX.
func SvmlFloorPs(a M256) M256 {
	return M256(svmlFloorPs([8]float32(a)))
}

func svmlFloorPs(a [8]float32) [8]float32


// SvmlRoundPd: Round the packed double-precision (64-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed double-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := ROUND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_round_pd'.
// Requires AVX.
func SvmlRoundPd(a M256d) M256d {
	return M256d(svmlRoundPd([4]float64(a)))
}

func svmlRoundPd(a [4]float64) [4]float64


// SvmlRoundPs: Round the packed single-precision (32-bit) floating-point
// elements in 'a' to the nearest integer value, and store the results as
// packed single-precision floating-point elements in 'dst'. This intrinsic may
// generate the 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := ROUND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_round_ps'.
// Requires AVX.
func SvmlRoundPs(a M256) M256 {
	return M256(svmlRoundPs([8]float32(a)))
}

func svmlRoundPs(a [8]float32) [8]float32


// SvmlSqrtPd: Compute the square root of packed double-precision (64-bit)
// floating-point elements in 'a', and store the results in 'dst'. Note that
// this intrinsic is less efficient than '_mm_sqrt_pd'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := SQRT(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_sqrt_pd'.
// Requires AVX.
func SvmlSqrtPd(a M256d) M256d {
	return M256d(svmlSqrtPd([4]float64(a)))
}

func svmlSqrtPd(a [4]float64) [4]float64


// SvmlSqrtPs: Compute the square root of packed single-precision (32-bit)
// floating-point elements in 'a', and store the results in 'dst'. Note that
// this intrinsic is less efficient than '_mm_sqrt_ps'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := SQRT(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_svml_sqrt_ps'.
// Requires AVX.
func SvmlSqrtPs(a M256) M256 {
	return M256(svmlSqrtPs([8]float32(a)))
}

func svmlSqrtPs(a [8]float32) [8]float32


// TanPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TAN(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tan_pd'.
// Requires AVX.
func TanPd(a M256d) M256d {
	return M256d(tanPd([4]float64(a)))
}

func tanPd(a [4]float64) [4]float64


// TanPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TAN(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tan_ps'.
// Requires AVX.
func TanPs(a M256) M256 {
	return M256(tanPs([8]float32(a)))
}

func tanPs(a [8]float32) [8]float32


// TandPd: Compute the tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TAND(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tand_pd'.
// Requires AVX.
func TandPd(a M256d) M256d {
	return M256d(tandPd([4]float64(a)))
}

func tandPd(a [4]float64) [4]float64


// TandPs: Compute the tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in degrees, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TAND(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tand_ps'.
// Requires AVX.
func TandPs(a M256) M256 {
	return M256(tandPs([8]float32(a)))
}

func tandPs(a [8]float32) [8]float32


// TanhPd: Compute the hyperbolic tangent of packed double-precision (64-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TANH(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tanh_pd'.
// Requires AVX.
func TanhPd(a M256d) M256d {
	return M256d(tanhPd([4]float64(a)))
}

func tanhPd(a [4]float64) [4]float64


// TanhPs: Compute the hyperbolic tangent of packed single-precision (32-bit)
// floating-point elements in 'a' expressed in radians, and store the results
// in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TANH(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_tanh_ps'.
// Requires AVX.
func TanhPs(a M256) M256 {
	return M256(tanhPs([8]float32(a)))
}

func tanhPs(a [8]float32) [8]float32


// MaskTernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 32-bit granularity (32-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_mask_ternarylogic_epi32'.
// Requires AVX512F.
func MaskTernarylogicEpi32(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskTernarylogicEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskTernarylogicEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzTernarylogicEpi32: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 32-bit granularity (32-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				FOR h := 0 to 31
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_maskz_ternarylogic_epi32'.
// Requires AVX512F.
func MaskzTernarylogicEpi32(k Mmask8, a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(maskzTernarylogicEpi32(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func maskzTernarylogicEpi32(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// TernarylogicEpi32: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 32-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			FOR h := 0 to 31
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGD'. Intrinsic: '_mm256_ternarylogic_epi32'.
// Requires AVX512F.
func TernarylogicEpi32(a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(ternarylogicEpi32([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func ternarylogicEpi32(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// MaskTernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'src', 'a', and 'b' are used to form a 3 bit
// index into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using writemask 'k' at 64-bit granularity (64-bit
// elements are copied from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (src[i+h] << 2) OR (a[i+h] << 1) OR b[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_mask_ternarylogic_epi64'.
// Requires AVX512F.
func MaskTernarylogicEpi64(src M256i, k Mmask8, a M256i, b M256i, imm8 int) M256i {
	return M256i(maskTernarylogicEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b), imm8))
}

func maskTernarylogicEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte, imm8 int) [32]byte


// MaskzTernarylogicEpi64: Bitwise ternary logic that provides the capability
// to implement any three-operand binary function; the specific binary function
// is specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst' using zeromask 'k' at 64-bit granularity (64-bit
// elements are zeroed out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				FOR h := 0 to 63
//					index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//					dst[i+h] := imm8[index[2:0]]
//				ENDFOR
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_maskz_ternarylogic_epi64'.
// Requires AVX512F.
func MaskzTernarylogicEpi64(k Mmask8, a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(maskzTernarylogicEpi64(uint8(k), [32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func maskzTernarylogicEpi64(k uint8, a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// TernarylogicEpi64: Bitwise ternary logic that provides the capability to
// implement any three-operand binary function; the specific binary function is
// specified by value in 'imm8'. For each bit in each packed 64-bit integer,
// the corresponding bit from 'a', 'b', and 'c' are used to form a 3 bit index
// into 'imm8', and the value at that bit in 'imm8' is written to the
// corresponding bit in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			FOR h := 0 to 63
//				index[2:0] := (a[i+h] << 2) OR (b[i+h] << 1) OR c[i+h]
//				dst[i+h] := imm8[index[2:0]]
//			ENDFOR
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPTERNLOGQ'. Intrinsic: '_mm256_ternarylogic_epi64'.
// Requires AVX512F.
func TernarylogicEpi64(a M256i, b M256i, c M256i, imm8 int) M256i {
	return M256i(ternarylogicEpi64([32]byte(a), [32]byte(b), [32]byte(c), imm8))
}

func ternarylogicEpi64(a [32]byte, b [32]byte, c [32]byte, imm8 int) [32]byte


// MaskTestEpi16Mask: Compute the bitwise AND of packed 16-bit integers in 'a'
// and 'b', producing intermediate 16-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm256_mask_test_epi16_mask'.
// Requires AVX512BW.
func MaskTestEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskTestEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskTestEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// TestEpi16Mask: Compute the bitwise AND of packed 16-bit integers in 'a' and
// 'b', producing intermediate 16-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTMW'. Intrinsic: '_mm256_test_epi16_mask'.
// Requires AVX512BW.
func TestEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(testEpi16Mask([32]byte(a), [32]byte(b)))
}

func testEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskTestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_mask_test_epi32_mask'.
// Requires AVX512F.
func MaskTestEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// TestEpi32Mask: Compute the bitwise AND of packed 32-bit integers in 'a' and
// 'b', producing intermediate 32-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTMD'. Intrinsic: '_mm256_test_epi32_mask'.
// Requires AVX512F.
func TestEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(testEpi32Mask([32]byte(a), [32]byte(b)))
}

func testEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskTestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_mask_test_epi64_mask'.
// Requires AVX512F.
func MaskTestEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// TestEpi64Mask: Compute the bitwise AND of packed 64-bit integers in 'a' and
// 'b', producing intermediate 64-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTMQ'. Intrinsic: '_mm256_test_epi64_mask'.
// Requires AVX512F.
func TestEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(testEpi64Mask([32]byte(a), [32]byte(b)))
}

func testEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskTestEpi8Mask: Compute the bitwise AND of packed 8-bit integers in 'a'
// and 'b', producing intermediate 8-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// non-zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm256_mask_test_epi8_mask'.
// Requires AVX512BW.
func MaskTestEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskTestEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskTestEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// TestEpi8Mask: Compute the bitwise AND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is non-zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) != 0) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTMB'. Intrinsic: '_mm256_test_epi8_mask'.
// Requires AVX512BW.
func TestEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(testEpi8Mask([32]byte(a), [32]byte(b)))
}

func testEpi8Mask(a [32]byte, b [32]byte) uint32


// TestcPd: Compute the bitwise AND of 256 bits (representing double-precision
// (64-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 64-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 64-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm256_testc_pd'.
// Requires AVX.
func TestcPd(a M256d, b M256d) int {
	return int(testcPd([4]float64(a), [4]float64(b)))
}

func testcPd(a [4]float64, b [4]float64) int


// TestcPs: Compute the bitwise AND of 256 bits (representing single-precision
// (32-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 32-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 32-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm256_testc_ps'.
// Requires AVX.
func TestcPs(a M256, b M256) int {
	return int(testcPs([8]float32(a), [8]float32(b)))
}

func testcPs(a [8]float32, b [8]float32) int


// TestcSi256: Compute the bitwise AND of 256 bits (representing integer data)
// in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set 'ZF'
// to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if the
// result is zero, otherwise set 'CF' to 0. Return the 'CF' value. 
//
//		IF (a[255:0] AND b[255:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[255:0] AND NOT b[255:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN CF
//
// Instruction: 'VPTEST'. Intrinsic: '_mm256_testc_si256'.
// Requires AVX.
func TestcSi256(a M256i, b M256i) int {
	return int(testcSi256([32]byte(a), [32]byte(b)))
}

func testcSi256(a [32]byte, b [32]byte) int


// MaskTestnEpi16Mask: Compute the bitwise NAND of packed 16-bit integers in
// 'a' and 'b', producing intermediate 16-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			IF k1[j]
//				k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm256_mask_testn_epi16_mask'.
// Requires AVX512BW.
func MaskTestnEpi16Mask(k1 Mmask16, a M256i, b M256i) Mmask16 {
	return Mmask16(maskTestnEpi16Mask(uint16(k1), [32]byte(a), [32]byte(b)))
}

func maskTestnEpi16Mask(k1 uint16, a [32]byte, b [32]byte) uint16


// TestnEpi16Mask: Compute the bitwise NAND of packed 16-bit integers in 'a'
// and 'b', producing intermediate 16-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 15
//			i := j*16
//			k[j] := ((a[i+15:i] AND b[i+15:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:16] := 0
//
// Instruction: 'VPTESTNMW'. Intrinsic: '_mm256_testn_epi16_mask'.
// Requires AVX512BW.
func TestnEpi16Mask(a M256i, b M256i) Mmask16 {
	return Mmask16(testnEpi16Mask([32]byte(a), [32]byte(b)))
}

func testnEpi16Mask(a [32]byte, b [32]byte) uint16


// MaskTestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in
// 'a' and 'b', producing intermediate 32-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k1[j]
//				k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_mask_testn_epi32_mask'.
// Requires AVX512F.
func MaskTestnEpi32Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestnEpi32Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestnEpi32Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// TestnEpi32Mask: Compute the bitwise NAND of packed 32-bit integers in 'a'
// and 'b', producing intermediate 32-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 7
//			i := j*32
//			k[j] := ((a[i+31:i] AND b[i+31:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:8] := 0
//
// Instruction: 'VPTESTNMD'. Intrinsic: '_mm256_testn_epi32_mask'.
// Requires AVX512F.
func TestnEpi32Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(testnEpi32Mask([32]byte(a), [32]byte(b)))
}

func testnEpi32Mask(a [32]byte, b [32]byte) uint8


// MaskTestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in
// 'a' and 'b', producing intermediate 64-bit values, and set the corresponding
// bit in result mask 'k' (subject to writemask 'k') if the intermediate value
// is zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k1[j]
//				k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_mask_testn_epi64_mask'.
// Requires AVX512F.
func MaskTestnEpi64Mask(k1 Mmask8, a M256i, b M256i) Mmask8 {
	return Mmask8(maskTestnEpi64Mask(uint8(k1), [32]byte(a), [32]byte(b)))
}

func maskTestnEpi64Mask(k1 uint8, a [32]byte, b [32]byte) uint8


// TestnEpi64Mask: Compute the bitwise NAND of packed 64-bit integers in 'a'
// and 'b', producing intermediate 64-bit values, and set the corresponding bit
// in result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 3
//			i := j*64
//			k[j] := ((a[i+63:i] AND b[i+63:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:4] := 0
//
// Instruction: 'VPTESTNMQ'. Intrinsic: '_mm256_testn_epi64_mask'.
// Requires AVX512F.
func TestnEpi64Mask(a M256i, b M256i) Mmask8 {
	return Mmask8(testnEpi64Mask([32]byte(a), [32]byte(b)))
}

func testnEpi64Mask(a [32]byte, b [32]byte) uint8


// MaskTestnEpi8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a'
// and 'b', producing intermediate 8-bit values, and set the corresponding bit
// in result mask 'k' (subject to writemask 'k') if the intermediate value is
// zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			IF k1[j]
//				k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//			ELSE
//				k[j] := 0
//			FI
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm256_mask_testn_epi8_mask'.
// Requires AVX512BW.
func MaskTestnEpi8Mask(k1 Mmask32, a M256i, b M256i) Mmask32 {
	return Mmask32(maskTestnEpi8Mask(uint32(k1), [32]byte(a), [32]byte(b)))
}

func maskTestnEpi8Mask(k1 uint32, a [32]byte, b [32]byte) uint32


// TestnEpi8Mask: Compute the bitwise NAND of packed 8-bit integers in 'a' and
// 'b', producing intermediate 8-bit values, and set the corresponding bit in
// result mask 'k' if the intermediate value is zero. 
//
//		FOR j := 0 to 31
//			i := j*8
//			k[j] := ((a[i+7:i] AND b[i+7:i]) == 0) ? 1 : 0
//		ENDFOR
//		k[MAX:32] := 0
//
// Instruction: 'VPTESTNMB'. Intrinsic: '_mm256_testn_epi8_mask'.
// Requires AVX512BW.
func TestnEpi8Mask(a M256i, b M256i) Mmask32 {
	return Mmask32(testnEpi8Mask([32]byte(a), [32]byte(b)))
}

func testnEpi8Mask(a [32]byte, b [32]byte) uint32


// TestnzcPd: Compute the bitwise AND of 256 bits (representing
// double-precision (64-bit) floating-point elements) in 'a' and 'b', producing
// an intermediate 256-bit value, and set 'ZF' to 1 if the sign bit of each
// 64-bit element in the intermediate value is zero, otherwise set 'ZF' to 0.
// Compute the bitwise AND NOT of 'a' and 'b', producing an intermediate value,
// and set 'CF' to 1 if the sign bit of each 64-bit element in the intermediate
// value is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and 'CF'
// values are zero, otherwise return 0. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm256_testnzc_pd'.
// Requires AVX.
func TestnzcPd(a M256d, b M256d) int {
	return int(testnzcPd([4]float64(a), [4]float64(b)))
}

func testnzcPd(a [4]float64, b [4]float64) int


// TestnzcPs: Compute the bitwise AND of 256 bits (representing
// single-precision (32-bit) floating-point elements) in 'a' and 'b', producing
// an intermediate 256-bit value, and set 'ZF' to 1 if the sign bit of each
// 32-bit element in the intermediate value is zero, otherwise set 'ZF' to 0.
// Compute the bitwise AND NOT of 'a' and 'b', producing an intermediate value,
// and set 'CF' to 1 if the sign bit of each 32-bit element in the intermediate
// value is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and 'CF'
// values are zero, otherwise return 0. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255]  == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255]  == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm256_testnzc_ps'.
// Requires AVX.
func TestnzcPs(a M256, b M256) int {
	return int(testnzcPs([8]float32(a), [8]float32(b)))
}

func testnzcPs(a [8]float32, b [8]float32) int


// TestnzcSi256: Compute the bitwise AND of 256 bits (representing integer
// data) in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set
// 'ZF' to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if
// the result is zero, otherwise set 'CF' to 0. Return 1 if both the 'ZF' and
// 'CF' values are zero, otherwise return 0. 
//
//		IF (a[255:0] AND b[255:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[255:0] AND NOT b[255:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		IF (ZF == 0 && CF == 0)
//			RETURN 1
//		ELSE
//			RETURN 0
//		FI
//
// Instruction: 'VPTEST'. Intrinsic: '_mm256_testnzc_si256'.
// Requires AVX.
func TestnzcSi256(a M256i, b M256i) int {
	return int(testnzcSi256([32]byte(a), [32]byte(b)))
}

func testnzcSi256(a [32]byte, b [32]byte) int


// TestzPd: Compute the bitwise AND of 256 bits (representing double-precision
// (64-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 64-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 64-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[63] == tmp[127] == tmp[191] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VTESTPD'. Intrinsic: '_mm256_testz_pd'.
// Requires AVX.
func TestzPd(a M256d, b M256d) int {
	return int(testzPd([4]float64(a), [4]float64(b)))
}

func testzPd(a [4]float64, b [4]float64) int


// TestzPs: Compute the bitwise AND of 256 bits (representing single-precision
// (32-bit) floating-point elements) in 'a' and 'b', producing an intermediate
// 256-bit value, and set 'ZF' to 1 if the sign bit of each 32-bit element in
// the intermediate value is zero, otherwise set 'ZF' to 0. Compute the bitwise
// AND NOT of 'a' and 'b', producing an intermediate value, and set 'CF' to 1
// if the sign bit of each 32-bit element in the intermediate value is zero,
// otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		tmp[255:0] := a[255:0] AND b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		tmp[255:0] := a[255:0] AND NOT b[255:0]
//		IF (tmp[31] == tmp[63] == tmp[95] == tmp[127] == tmp[159] == tmp[191] == tmp[223] == tmp[255] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VTESTPS'. Intrinsic: '_mm256_testz_ps'.
// Requires AVX.
func TestzPs(a M256, b M256) int {
	return int(testzPs([8]float32(a), [8]float32(b)))
}

func testzPs(a [8]float32, b [8]float32) int


// TestzSi256: Compute the bitwise AND of 256 bits (representing integer data)
// in 'a' and 'b', and set 'ZF' to 1 if the result is zero, otherwise set 'ZF'
// to 0. Compute the bitwise AND NOT of 'a' and 'b', and set 'CF' to 1 if the
// result is zero, otherwise set 'CF' to 0. Return the 'ZF' value. 
//
//		IF (a[255:0] AND b[255:0] == 0)
//			ZF := 1
//		ELSE
//			ZF := 0
//		FI
//		IF (a[255:0] AND NOT b[255:0] == 0)
//			CF := 1
//		ELSE
//			CF := 0
//		FI
//		RETURN ZF
//
// Instruction: 'VPTEST'. Intrinsic: '_mm256_testz_si256'.
// Requires AVX.
func TestzSi256(a M256i, b M256i) int {
	return int(testzSi256([32]byte(a), [32]byte(b)))
}

func testzSi256(a [32]byte, b [32]byte) int


// TruncPd: Truncate the packed double-precision (64-bit) floating-point
// elements in 'a', and store the results as packed double-precision
// floating-point elements in 'dst'. This intrinsic may generate the
// 'roundpd'/'vroundpd' instruction. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := TRUNCATE(a[i+63:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_trunc_pd'.
// Requires AVX.
func TruncPd(a M256d) M256d {
	return M256d(truncPd([4]float64(a)))
}

func truncPd(a [4]float64) [4]float64


// TruncPs: Truncate the packed single-precision (32-bit) floating-point
// elements in 'a', and store the results as packed single-precision
// floating-point elements in 'dst'. This intrinsic may generate the
// 'roundps'/'vroundps' instruction. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := TRUNCATE(a[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_trunc_ps'.
// Requires AVX.
func TruncPs(a M256) M256 {
	return M256(truncPs([8]float32(a)))
}

func truncPs(a [8]float32) [8]float32


// UdivEpi32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the truncated results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_udiv_epi32'.
// Requires AVX.
func UdivEpi32(a M256i, b M256i) M256i {
	return M256i(udivEpi32([32]byte(a), [32]byte(b)))
}

func udivEpi32(a [32]byte, b [32]byte) [32]byte


// UdivremEpi32: Divide packed unsigned 32-bit integers in 'a' by packed
// elements in 'b', store the truncated results in 'dst', and store the
// remainders as packed unsigned 32-bit integers into memory at 'mem_addr'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := TRUNCATE(a[i+31:i] / b[i+31:i])
//			MEM[mem_addr+i+31:mem_addr+i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_udivrem_epi32'.
// Requires AVX.
func UdivremEpi32(mem_addr M256i, a M256i, b M256i) M256i {
	return M256i(udivremEpi32([32]byte(mem_addr), [32]byte(a), [32]byte(b)))
}

func udivremEpi32(mem_addr [32]byte, a [32]byte, b [32]byte) [32]byte


// UndefinedPd: Return vector of type __m256d with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_undefined_pd'.
// Requires AVX.
func UndefinedPd() M256d {
	return M256d(undefinedPd())
}

func undefinedPd() [4]float64


// UndefinedPs: Return vector of type __m256 with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_undefined_ps'.
// Requires AVX.
func UndefinedPs() M256 {
	return M256(undefinedPs())
}

func undefinedPs() [8]float32


// UndefinedSi256: Return vector of type __m256i with undefined elements. 
//
//		
//
// Instruction: ''. Intrinsic: '_mm256_undefined_si256'.
// Requires AVX.
func UndefinedSi256() M256i {
	return M256i(undefinedSi256())
}

func undefinedSi256() [32]byte


// MaskUnpackhiEpi16: Unpack and interleave 16-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm256_mask_unpackhi_epi16'.
// Requires AVX512BW.
func MaskUnpackhiEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskUnpackhiEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhiEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhiEpi16: Unpack and interleave 16-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm256_maskz_unpackhi_epi16'.
// Requires AVX512BW.
func MaskzUnpackhiEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhiEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhiEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// UnpackhiEpi16: Unpack and interleave 16-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[79:64]
//			dst[31:16] := src2[79:64] 
//			dst[47:32] := src1[95:80] 
//			dst[63:48] := src2[95:80] 
//			dst[79:64] := src1[111:96] 
//			dst[95:80] := src2[111:96] 
//			dst[111:96] := src1[127:112] 
//			dst[127:112] := src2[127:112] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_HIGH_WORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_WORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHWD'. Intrinsic: '_mm256_unpackhi_epi16'.
// Requires AVX2.
func UnpackhiEpi16(a M256i, b M256i) M256i {
	return M256i(unpackhiEpi16([32]byte(a), [32]byte(b)))
}

func unpackhiEpi16(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_mask_unpackhi_epi32'.
// Requires AVX512F.
func MaskUnpackhiEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackhiEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhiEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhiEpi32: Unpack and interleave 32-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi32'.
// Requires AVX512F.
func MaskzUnpackhiEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhiEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhiEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// UnpackhiEpi32: Unpack and interleave 32-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHDQ'. Intrinsic: '_mm256_unpackhi_epi32'.
// Requires AVX2.
func UnpackhiEpi32(a M256i, b M256i) M256i {
	return M256i(unpackhiEpi32([32]byte(a), [32]byte(b)))
}

func unpackhiEpi32(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_mask_unpackhi_epi64'.
// Requires AVX512F.
func MaskUnpackhiEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackhiEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhiEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhiEpi64: Unpack and interleave 64-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_maskz_unpackhi_epi64'.
// Requires AVX512F.
func MaskzUnpackhiEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhiEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhiEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// UnpackhiEpi64: Unpack and interleave 64-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHQDQ'. Intrinsic: '_mm256_unpackhi_epi64'.
// Requires AVX2.
func UnpackhiEpi64(a M256i, b M256i) M256i {
	return M256i(unpackhiEpi64([32]byte(a), [32]byte(b)))
}

func unpackhiEpi64(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhiEpi8: Unpack and interleave 8-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm256_mask_unpackhi_epi8'.
// Requires AVX512BW.
func MaskUnpackhiEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskUnpackhiEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackhiEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackhiEpi8: Unpack and interleave 8-bit integers from the high half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm256_maskz_unpackhi_epi8'.
// Requires AVX512BW.
func MaskzUnpackhiEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzUnpackhiEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackhiEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// UnpackhiEpi8: Unpack and interleave 8-bit integers from the high half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_HIGH_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[71:64] 
//			dst[15:8] := src2[71:64] 
//			dst[23:16] := src1[79:72] 
//			dst[31:24] := src2[79:72] 
//			dst[39:32] := src1[87:80] 
//			dst[47:40] := src2[87:80] 
//			dst[55:48] := src1[95:88] 
//			dst[63:56] := src2[95:88] 
//			dst[71:64] := src1[103:96] 
//			dst[79:72] := src2[103:96] 
//			dst[87:80] := src1[111:104] 
//			dst[95:88] := src2[111:104] 
//			dst[103:96] := src1[119:112] 
//			dst[111:104] := src2[119:112] 
//			dst[119:112] := src1[127:120] 
//			dst[127:120] := src2[127:120] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_BYTES(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_BYTES(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKHBW'. Intrinsic: '_mm256_unpackhi_epi8'.
// Requires AVX2.
func UnpackhiEpi8(a M256i, b M256i) M256i {
	return M256i(unpackhiEpi8([32]byte(a), [32]byte(b)))
}

func unpackhiEpi8(a [32]byte, b [32]byte) [32]byte


// MaskUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_mask_unpackhi_pd'.
// Requires AVX512F.
func MaskUnpackhiPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskUnpackhiPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskUnpackhiPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzUnpackhiPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_maskz_unpackhi_pd'.
// Requires AVX512F.
func MaskzUnpackhiPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzUnpackhiPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzUnpackhiPd(k uint8, a [4]float64, b [4]float64) [4]float64


// UnpackhiPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[127:64] 
//			dst[127:64] := src2[127:64] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPD'. Intrinsic: '_mm256_unpackhi_pd'.
// Requires AVX.
func UnpackhiPd(a M256d, b M256d) M256d {
	return M256d(unpackhiPd([4]float64(a), [4]float64(b)))
}

func unpackhiPd(a [4]float64, b [4]float64) [4]float64


// MaskUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_mask_unpackhi_ps'.
// Requires AVX512F.
func MaskUnpackhiPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskUnpackhiPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskUnpackhiPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzUnpackhiPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the high half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_maskz_unpackhi_ps'.
// Requires AVX512F.
func MaskzUnpackhiPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzUnpackhiPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzUnpackhiPs(k uint8, a [8]float32, b [8]float32) [8]float32


// UnpackhiPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the high half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_HIGH_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[95:64] 
//			dst[63:32] := src2[95:64] 
//			dst[95:64] := src1[127:96] 
//			dst[127:96] := src2[127:96] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_HIGH_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_HIGH_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKHPS'. Intrinsic: '_mm256_unpackhi_ps'.
// Requires AVX.
func UnpackhiPs(a M256, b M256) M256 {
	return M256(unpackhiPs([8]float32(a), [8]float32(b)))
}

func unpackhiPs(a [8]float32, b [8]float32) [8]float32


// MaskUnpackloEpi16: Unpack and interleave 16-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := src[i+15:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm256_mask_unpacklo_epi16'.
// Requires AVX512BW.
func MaskUnpackloEpi16(src M256i, k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskUnpackloEpi16([32]byte(src), uint16(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackloEpi16(src [32]byte, k uint16, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackloEpi16: Unpack and interleave 16-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 15
//			i := j*16
//			IF k[j]
//				dst[i+15:i] := tmp_dst[i+15:i]
//			ELSE
//				dst[i+15:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm256_maskz_unpacklo_epi16'.
// Requires AVX512BW.
func MaskzUnpackloEpi16(k Mmask16, a M256i, b M256i) M256i {
	return M256i(maskzUnpackloEpi16(uint16(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackloEpi16(k uint16, a [32]byte, b [32]byte) [32]byte


// UnpackloEpi16: Unpack and interleave 16-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_WORDS(src1[127:0], src2[127:0]){
//			dst[15:0] := src1[15:0] 
//			dst[31:16] := src2[15:0] 
//			dst[47:32] := src1[31:16] 
//			dst[63:48] := src2[31:16] 
//			dst[79:64] := src1[47:32] 
//			dst[95:80] := src2[47:32] 
//			dst[111:96] := src1[63:48] 
//			dst[127:112] := src2[63:48] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_WORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_WORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLWD'. Intrinsic: '_mm256_unpacklo_epi16'.
// Requires AVX2.
func UnpackloEpi16(a M256i, b M256i) M256i {
	return M256i(unpackloEpi16([32]byte(a), [32]byte(b)))
}

func unpackloEpi16(a [32]byte, b [32]byte) [32]byte


// MaskUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_mask_unpacklo_epi32'.
// Requires AVX512F.
func MaskUnpackloEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackloEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackloEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackloEpi32: Unpack and interleave 32-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi32'.
// Requires AVX512F.
func MaskzUnpackloEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackloEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackloEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// UnpackloEpi32: Unpack and interleave 32-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLDQ'. Intrinsic: '_mm256_unpacklo_epi32'.
// Requires AVX2.
func UnpackloEpi32(a M256i, b M256i) M256i {
	return M256i(unpackloEpi32([32]byte(a), [32]byte(b)))
}

func unpackloEpi32(a [32]byte, b [32]byte) [32]byte


// MaskUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_mask_unpacklo_epi64'.
// Requires AVX512F.
func MaskUnpackloEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskUnpackloEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackloEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackloEpi64: Unpack and interleave 64-bit integers from the low half
// of each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_maskz_unpacklo_epi64'.
// Requires AVX512F.
func MaskzUnpackloEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzUnpackloEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackloEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// UnpackloEpi64: Unpack and interleave 64-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLQDQ'. Intrinsic: '_mm256_unpacklo_epi64'.
// Requires AVX2.
func UnpackloEpi64(a M256i, b M256i) M256i {
	return M256i(unpackloEpi64([32]byte(a), [32]byte(b)))
}

func unpackloEpi64(a [32]byte, b [32]byte) [32]byte


// MaskUnpackloEpi8: Unpack and interleave 8-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := src[i+7:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm256_mask_unpacklo_epi8'.
// Requires AVX512BW.
func MaskUnpackloEpi8(src M256i, k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskUnpackloEpi8([32]byte(src), uint32(k), [32]byte(a), [32]byte(b)))
}

func maskUnpackloEpi8(src [32]byte, k uint32, a [32]byte, b [32]byte) [32]byte


// MaskzUnpackloEpi8: Unpack and interleave 8-bit integers from the low half of
// each 128-bit lane in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		
//		FOR j := 0 to 31
//			i := j*8
//			IF k[j]
//				dst[i+7:i] := tmp_dst[i+7:i]
//			ELSE
//				dst[i+7:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm256_maskz_unpacklo_epi8'.
// Requires AVX512BW.
func MaskzUnpackloEpi8(k Mmask32, a M256i, b M256i) M256i {
	return M256i(maskzUnpackloEpi8(uint32(k), [32]byte(a), [32]byte(b)))
}

func maskzUnpackloEpi8(k uint32, a [32]byte, b [32]byte) [32]byte


// UnpackloEpi8: Unpack and interleave 8-bit integers from the low half of each
// 128-bit lane in 'a' and 'b', and store the results in 'dst'. 
//
//		INTERLEAVE_BYTES(src1[127:0], src2[127:0]){
//			dst[7:0] := src1[7:0] 
//			dst[15:8] := src2[7:0] 
//			dst[23:16] := src1[15:8] 
//			dst[31:24] := src2[15:8] 
//			dst[39:32] := src1[23:16] 
//			dst[47:40] := src2[23:16] 
//			dst[55:48] := src1[31:24] 
//			dst[63:56] := src2[31:24] 
//			dst[71:64] := src1[39:32]
//			dst[79:72] := src2[39:32] 
//			dst[87:80] := src1[47:40] 
//			dst[95:88] := src2[47:40] 
//			dst[103:96] := src1[55:48] 
//			dst[111:104] := src2[55:48] 
//			dst[119:112] := src1[63:56] 
//			dst[127:120] := src2[63:56] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_BYTES(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_BYTES(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VPUNPCKLBW'. Intrinsic: '_mm256_unpacklo_epi8'.
// Requires AVX2.
func UnpackloEpi8(a M256i, b M256i) M256i {
	return M256i(unpackloEpi8([32]byte(a), [32]byte(b)))
}

func unpackloEpi8(a [32]byte, b [32]byte) [32]byte


// MaskUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_mask_unpacklo_pd'.
// Requires AVX512F.
func MaskUnpackloPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskUnpackloPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskUnpackloPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzUnpackloPd: Unpack and interleave double-precision (64-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		tmp_dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := tmp_dst[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_maskz_unpacklo_pd'.
// Requires AVX512F.
func MaskzUnpackloPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzUnpackloPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzUnpackloPd(k uint8, a [4]float64, b [4]float64) [4]float64


// UnpackloPd: Unpack and interleave double-precision (64-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_QWORDS(src1[127:0], src2[127:0]){
//			dst[63:0] := src1[63:0] 
//			dst[127:64] := src2[63:0] 
//			RETURN dst[127:0]
//		}
//		
//		dst[127:0] := INTERLEAVE_QWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_QWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPD'. Intrinsic: '_mm256_unpacklo_pd'.
// Requires AVX.
func UnpackloPd(a M256d, b M256d) M256d {
	return M256d(unpackloPd([4]float64(a), [4]float64(b)))
}

func unpackloPd(a [4]float64, b [4]float64) [4]float64


// MaskUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_mask_unpacklo_ps'.
// Requires AVX512F.
func MaskUnpackloPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskUnpackloPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskUnpackloPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzUnpackloPs: Unpack and interleave single-precision (32-bit)
// floating-point elements from the low half of each 128-bit lane in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		tmp_dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		tmp_dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := tmp_dst[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_maskz_unpacklo_ps'.
// Requires AVX512F.
func MaskzUnpackloPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzUnpackloPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzUnpackloPs(k uint8, a [8]float32, b [8]float32) [8]float32


// UnpackloPs: Unpack and interleave single-precision (32-bit) floating-point
// elements from the low half of each 128-bit lane in 'a' and 'b', and store
// the results in 'dst'. 
//
//		INTERLEAVE_DWORDS(src1[127:0], src2[127:0]){
//			dst[31:0] := src1[31:0] 
//			dst[63:32] := src2[31:0] 
//			dst[95:64] := src1[63:32] 
//			dst[127:96] := src2[63:32] 
//			RETURN dst[127:0]
//		}	
//		
//		dst[127:0] := INTERLEAVE_DWORDS(a[127:0], b[127:0])
//		dst[255:128] := INTERLEAVE_DWORDS(a[255:128], b[255:128])
//		dst[MAX:256] := 0
//
// Instruction: 'VUNPCKLPS'. Intrinsic: '_mm256_unpacklo_ps'.
// Requires AVX.
func UnpackloPs(a M256, b M256) M256 {
	return M256(unpackloPs([8]float32(a), [8]float32(b)))
}

func unpackloPs(a [8]float32, b [8]float32) [8]float32


// UremEpi32: Divide packed unsigned 32-bit integers in 'a' by packed elements
// in 'b', and store the remainders as packed unsigned 32-bit integers in
// 'dst'. 
//
//		FOR j := 0 to 7
//			i := 32*j
//			dst[i+31:i] := REMAINDER(a[i+31:i] / b[i+31:i])
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: '...'. Intrinsic: '_mm256_urem_epi32'.
// Requires AVX.
func UremEpi32(a M256i, b M256i) M256i {
	return M256i(uremEpi32([32]byte(a), [32]byte(b)))
}

func uremEpi32(a [32]byte, b [32]byte) [32]byte


// MaskXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_mask_xor_epi32'.
// Requires AVX512F.
func MaskXorEpi32(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskXorEpi32([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskXorEpi32(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXorEpi32: Compute the bitwise XOR of packed 32-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORD'. Intrinsic: '_mm256_maskz_xor_epi32'.
// Requires AVX512F.
func MaskzXorEpi32(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzXorEpi32(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzXorEpi32(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using writemask 'k' (elements are copied
// from 'src' when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_mask_xor_epi64'.
// Requires AVX512F.
func MaskXorEpi64(src M256i, k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskXorEpi64([32]byte(src), uint8(k), [32]byte(a), [32]byte(b)))
}

func maskXorEpi64(src [32]byte, k uint8, a [32]byte, b [32]byte) [32]byte


// MaskzXorEpi64: Compute the bitwise XOR of packed 64-bit integers in 'a' and
// 'b', and store the results in 'dst' using zeromask 'k' (elements are zeroed
// out when the corresponding mask bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VPXORQ'. Intrinsic: '_mm256_maskz_xor_epi64'.
// Requires AVX512F.
func MaskzXorEpi64(k Mmask8, a M256i, b M256i) M256i {
	return M256i(maskzXorEpi64(uint8(k), [32]byte(a), [32]byte(b)))
}

func maskzXorEpi64(k uint8, a [32]byte, b [32]byte) [32]byte


// MaskXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := src[i+63:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_mask_xor_pd'.
// Requires AVX512DQ.
func MaskXorPd(src M256d, k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskXorPd([4]float64(src), uint8(k), [4]float64(a), [4]float64(b)))
}

func maskXorPd(src [4]float64, k uint8, a [4]float64, b [4]float64) [4]float64


// MaskzXorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 3
//			i := j*64
//			IF k[j]
//				dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//			ELSE
//				dst[i+63:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_maskz_xor_pd'.
// Requires AVX512DQ.
func MaskzXorPd(k Mmask8, a M256d, b M256d) M256d {
	return M256d(maskzXorPd(uint8(k), [4]float64(a), [4]float64(b)))
}

func maskzXorPd(k uint8, a [4]float64, b [4]float64) [4]float64


// XorPd: Compute the bitwise XOR of packed double-precision (64-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 3
//			i := j*64
//			dst[i+63:i] := a[i+63:i] XOR b[i+63:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPD'. Intrinsic: '_mm256_xor_pd'.
// Requires AVX.
func XorPd(a M256d, b M256d) M256d {
	return M256d(xorPd([4]float64(a), [4]float64(b)))
}

func xorPd(a [4]float64, b [4]float64) [4]float64


// MaskXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// writemask 'k' (elements are copied from 'src' when the corresponding mask
// bit is not set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := src[i+31:i]
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_mask_xor_ps'.
// Requires AVX512DQ.
func MaskXorPs(src M256, k Mmask8, a M256, b M256) M256 {
	return M256(maskXorPs([8]float32(src), uint8(k), [8]float32(a), [8]float32(b)))
}

func maskXorPs(src [8]float32, k uint8, a [8]float32, b [8]float32) [8]float32


// MaskzXorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst' using
// zeromask 'k' (elements are zeroed out when the corresponding mask bit is not
// set). 
//
//		FOR j := 0 to 7
//			i := j*32
//			IF k[j]
//				dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//			ELSE
//				dst[i+31:i] := 0
//			FI
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_maskz_xor_ps'.
// Requires AVX512DQ.
func MaskzXorPs(k Mmask8, a M256, b M256) M256 {
	return M256(maskzXorPs(uint8(k), [8]float32(a), [8]float32(b)))
}

func maskzXorPs(k uint8, a [8]float32, b [8]float32) [8]float32


// XorPs: Compute the bitwise XOR of packed single-precision (32-bit)
// floating-point elements in 'a' and 'b', and store the results in 'dst'. 
//
//		FOR j := 0 to 7
//			i := j*32
//			dst[i+31:i] := a[i+31:i] XOR b[i+31:i]
//		ENDFOR
//		dst[MAX:256] := 0
//
// Instruction: 'VXORPS'. Intrinsic: '_mm256_xor_ps'.
// Requires AVX.
func XorPs(a M256, b M256) M256 {
	return M256(xorPs([8]float32(a), [8]float32(b)))
}

func xorPs(a [8]float32, b [8]float32) [8]float32


// XorSi256: Compute the bitwise XOR of 256 bits (representing integer data) in
// 'a' and 'b', and store the result in 'dst'. 
//
//		dst[255:0] := (a[255:0] XOR b[255:0])
//		dst[MAX:256] := 0
//
// Instruction: 'VPXOR'. Intrinsic: '_mm256_xor_si256'.
// Requires AVX2.
func XorSi256(a M256i, b M256i) M256i {
	return M256i(xorSi256([32]byte(a), [32]byte(b)))
}

func xorSi256(a [32]byte, b [32]byte) [32]byte


// Zeroall: Zero the contents of all XMM or YMM registers. 
//
//		YMM0[MAX:0] := 0
//		YMM1[MAX:0] := 0
//		YMM2[MAX:0] := 0
//		YMM3[MAX:0] := 0
//		YMM4[MAX:0] := 0
//		YMM5[MAX:0] := 0
//		YMM6[MAX:0] := 0
//		YMM7[MAX:0] := 0
//		IF 64-bit mode
//			YMM8[MAX:0] := 0
//			YMM9[MAX:0] := 0
//			YMM10[MAX:0] := 0
//			YMM11[MAX:0] := 0
//			YMM12[MAX:0] := 0
//			YMM13[MAX:0] := 0
//			YMM14[MAX:0] := 0
//			YMM15[MAX:0] := 0
//		FI
//
// Instruction: 'VZEROALL'. Intrinsic: '_mm256_zeroall'.
// Requires AVX.
func Zeroall()  {
	zeroall()
}

func zeroall() 


// Zeroupper: Zero the upper 128 bits of all YMM registers; the lower 128-bits
// of the registers are unmodified. 
//
//		YMM0[MAX:128] := 0
//		YMM1[MAX:128] := 0
//		YMM2[MAX:128] := 0
//		YMM3[MAX:128] := 0
//		YMM4[MAX:128] := 0
//		YMM5[MAX:128] := 0
//		YMM6[MAX:128] := 0
//		YMM7[MAX:128] := 0
//		IF 64-bit mode
//			YMM8[MAX:128] := 0
//			YMM9[MAX:128] := 0
//			YMM10[MAX:128] := 0
//			YMM11[MAX:128] := 0
//			YMM12[MAX:128] := 0
//			YMM13[MAX:128] := 0
//			YMM14[MAX:128] := 0
//			YMM15[MAX:128] := 0
//		FI
//
// Instruction: 'VZEROUPPER'. Intrinsic: '_mm256_zeroupper'.
// Requires AVX.
func Zeroupper()  {
	zeroupper()
}

func zeroupper() 

